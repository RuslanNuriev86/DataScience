{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Air_quality_model_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Tuning"
      ],
      "metadata": {
        "id": "TY9JahOXiWrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we'll find optimal model adjustments"
      ],
      "metadata": {
        "id": "48qM9HllilgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUyD9fWyg6Rk",
        "outputId": "fb5ddd6f-4b75-4b92-aacd-bf2a218c2a16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.3 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.3\n"
          ]
        }
      ],
      "source": [
        "#install and import libraries\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn import linear_model\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import os\n",
        "import glob\n",
        "from catboost import CatBoostRegressor, Pool"
      ],
      "metadata": {
        "id": "kZCfbYVjhNuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4859d839-0a5d-4f48-bfc3-4d1a71aa38d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzDpai2mhlaz",
        "outputId": "da09e989-c254-4aa8-b397-da886892def9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the stations's data\n",
        "shab = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Moscow_contest/shab.csv', decimal=',')\n",
        "shab = shab.set_index('data_time')\n",
        "shab.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "mpIOepLdhpzw",
        "outputId": "4dc05fb5-c62d-4062-d68f-291a804e9233"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e9d07dec-2665-4883-b751-4c7ecfbd25b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>CO</th>\n",
              "      <th>NO2</th>\n",
              "      <th>NO</th>\n",
              "      <th>PM10</th>\n",
              "      <th>-T-</th>\n",
              "      <th>| V |</th>\n",
              "      <th>_V_</th>\n",
              "      <th>Давление</th>\n",
              "      <th>Влажность</th>\n",
              "      <th>Осадки</th>\n",
              "      <th>0</th>\n",
              "      <th>50</th>\n",
              "      <th>100</th>\n",
              "      <th>150</th>\n",
              "      <th>200</th>\n",
              "      <th>250</th>\n",
              "      <th>300</th>\n",
              "      <th>350</th>\n",
              "      <th>400</th>\n",
              "      <th>450</th>\n",
              "      <th>500</th>\n",
              "      <th>550</th>\n",
              "      <th>600</th>\n",
              "      <th>OutsideTemperature</th>\n",
              "      <th>Quality</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_time</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>01/01/2020 00:00:00</th>\n",
              "      <td>0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>732.6</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.36</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.04</td>\n",
              "      <td>-0.57</td>\n",
              "      <td>-0.89</td>\n",
              "      <td>-1.18</td>\n",
              "      <td>-1.41</td>\n",
              "      <td>-1.58</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>1.79</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01/01/2020 00:20:00</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.009</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>732.7</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.81</td>\n",
              "      <td>2.71</td>\n",
              "      <td>2.82</td>\n",
              "      <td>2.38</td>\n",
              "      <td>1.69</td>\n",
              "      <td>0.77</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.76</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>-1.55</td>\n",
              "      <td>-1.66</td>\n",
              "      <td>-1.77</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01/01/2020 00:40:00</th>\n",
              "      <td>2</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>732.8</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>2.69</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.47</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>-1.23</td>\n",
              "      <td>-1.44</td>\n",
              "      <td>-1.56</td>\n",
              "      <td>-1.69</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01/01/2020 01:00:00</th>\n",
              "      <td>3</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>732.9</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.81</td>\n",
              "      <td>2.57</td>\n",
              "      <td>2.63</td>\n",
              "      <td>2.24</td>\n",
              "      <td>1.67</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-0.33</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>-0.92</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01/01/2020 01:20:00</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>732.9</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>2.44</td>\n",
              "      <td>2.49</td>\n",
              "      <td>2.15</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.77</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-1.07</td>\n",
              "      <td>-1.26</td>\n",
              "      <td>-1.38</td>\n",
              "      <td>-1.50</td>\n",
              "      <td>1.71</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9d07dec-2665-4883-b751-4c7ecfbd25b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e9d07dec-2665-4883-b751-4c7ecfbd25b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e9d07dec-2665-4883-b751-4c7ecfbd25b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     Unnamed: 0    CO  ... OutsideTemperature Quality\n",
              "data_time                              ...                           \n",
              "01/01/2020 00:00:00           0  0.21  ...               1.79     0.0\n",
              "01/01/2020 00:20:00           1   0.2  ...               1.85     0.0\n",
              "01/01/2020 00:40:00           2  0.19  ...               1.85     0.0\n",
              "01/01/2020 01:00:00           3  0.19  ...               1.81     0.0\n",
              "01/01/2020 01:20:00           4   0.2  ...               1.71     0.0\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert object data to numeric\n",
        "def to_numeric(data):\n",
        "  data = data.copy()\n",
        "  s = (data.dtypes == 'object')\n",
        "  object_cols = s[s].index\n",
        "  for col in object_cols:\n",
        "    data[col] = data[col].astype('float')\n",
        "  return data"
      ],
      "metadata": {
        "id": "n1Zk5Nibh1uj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run data preprocessing\n",
        "def preprocessing(data):\n",
        "  data = data.copy()\n",
        "  garbage_cols = [col for col in data.columns if ('Unnamed' in col)]\n",
        "  data = data.drop(garbage_cols, axis=1)\n",
        "  data.index = pd.to_datetime(data.index)\n",
        "  data = data.sort_index()\n",
        "  data = data.resample('1H').sum()\n",
        "  return data"
      ],
      "metadata": {
        "id": "VFLBORGiiEOu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select target columns\n",
        "def get_target(data):\n",
        "  target_cols = set(data.columns).intersection(set(['CO', 'NO2', 'NO', 'PM10', 'PM2.5', 'PM25']))\n",
        "  return list(target_cols)"
      ],
      "metadata": {
        "id": "q5pC-IWSiHYQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the dataset\n",
        "data_clear = to_numeric(shab)\n",
        "data_clear = preprocessing(data_clear)\n",
        "data_clear.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTjNZXeXiKKC",
        "outputId": "f6478691-fa83-4113-e67c-3b9ec9a58ec7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 8785 entries, 2020-01-01 00:00:00 to 2021-01-01 00:00:00\n",
            "Freq: H\n",
            "Data columns (total 25 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   CO                  8785 non-null   float64\n",
            " 1   NO2                 8785 non-null   float64\n",
            " 2   NO                  8785 non-null   float64\n",
            " 3   PM10                8785 non-null   float64\n",
            " 4   -T-                 8785 non-null   float64\n",
            " 5   | V |               8785 non-null   float64\n",
            " 6   _V_                 8785 non-null   float64\n",
            " 7   Давление            8785 non-null   float64\n",
            " 8   Влажность           8785 non-null   float64\n",
            " 9   Осадки              8785 non-null   float64\n",
            " 10  0                   8785 non-null   float64\n",
            " 11  50                  8785 non-null   float64\n",
            " 12  100                 8785 non-null   float64\n",
            " 13  150                 8785 non-null   float64\n",
            " 14  200                 8785 non-null   float64\n",
            " 15  250                 8785 non-null   float64\n",
            " 16  300                 8785 non-null   float64\n",
            " 17  350                 8785 non-null   float64\n",
            " 18  400                 8785 non-null   float64\n",
            " 19  450                 8785 non-null   float64\n",
            " 20  500                 8785 non-null   float64\n",
            " 21  550                 8785 non-null   float64\n",
            " 22  600                 8785 non-null   float64\n",
            " 23  OutsideTemperature  8785 non-null   float64\n",
            " 24  Quality             8785 non-null   float64\n",
            "dtypes: float64(25)\n",
            "memory usage: 1.7 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#targets autocorrelation function\n",
        "for col in get_target(data_clear):\n",
        "  plot_acf(x=data_clear[col], lags=365, title='Autocorrelation for {}'.format(col))\n",
        "  plt.xlabel(\"Days\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bFfjBoBZiPIk",
        "outputId": "c7ada13f-f3d4-48e7-8211-42c40e5336fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwcdZn48c/TPVfuayYhkBsCAiohRsCbHwgiuwueiK7nsov7+8nuiq67Kv4Q2Z/X7goeuKuugBIUREQEQUAuJUAgkzCBHOSayRxJ5spkjszVXVXP74+qnvRRPdMzmat7nvfrFeiurq5+unr6qW8/3299S1QVY4wx+S8y0QEYY4wZHZbQjTGmQFhCN8aYAmEJ3RhjCoQldGOMKRCW0I0xpkBYQjcFT0Q+KSIbjuP5fxCRT4xmTMF2p4nIgyLSISK/Hu3tm6nHErpBRJ4WkSMiUjrM56mInDJWcU0EEblBRO5MXqaq71bVn4/By30AWAQsUNUPHu/GROT84DP5r7TlG0Tkk0n3l4jIL0TksIh0i8iLIvKXSY8vFJG7RORgcLB5VkTOPd74zNizhD7FicgK4G2AApdNaDA5EJGiXJblieXAblV1hvvEQd5zN/Cx4HMNe958YAMQA84EyoGbgV+KyAeC1WYCm4A3APOBnwMPicjM4cZpxpcldPNxYCPwMyClrBC03P826f5A6UJE/hws3ioiR0XkQ8HyvxORvSLSJiIPiMiJSc8/U0T+GDzWJCJfDpaXish3gxbhweB2afDY+SLSICL/KiKNwO1BK/peEblTRDqBT4rIHBG5VUQOicgBEfl/IhINe8Mi8j0RqReRThHZLCJvC5ZfAnwZ+FDwnram7wcRiYjIV0SkVkSaReQOEZkTPLYiaCF/QkTqRKRVRK7LEsPXgOuTXuuqHLd9lYjUAU9m+Tzbg8/yq1kevxY4Clylqo2q2quqdwFfB74jIqKq1ap6k6oeUlVXVX8ClACnZdmmmSQsoZuPA78I/r1LRBbl8iRVfXtw8yxVnamqvxKRC4BvAlcAi4Fa4G4AEZkFPA48ApwInAI8EWzjOuA8YA1wFnAO8JWklzsBv6W4HLg6WHY5cC8wN4j9Z4ATbPds4GLgbwm3KXit+cAvgV+LSJmqPgJ8A/hV8J7OCnnuJ4N//wtYhd+avSVtnbfiJ78LgetF5PT0jajqV9Ne69Yct/0O4HTgXVneG/jJ+f0iEpaALwJ+o6pe2vJ7gGXAqelPEJE1+Al97yCvaSYBS+hTmIi8FT9J3qOqm4F9wEeOY5N/DdymqltUtR/4EvCm4Of/XwKNqvodVe1T1S5VfSHpeTeqarOqtgBfAz6WtF0P+Kqq9qtqb7DseVW9P0hMs4FLgc+qareqNuOXEa4MC1JV71TVw6rqqOp3gFJyb33+NXBT0Io9GrzHK9NKIF8LWr5bga34B6nR2vYNwXvsDd8EqGoj8CPgxpCHy4FDIcsPJT0+QERmA+uD99SR4/swE8QS+tT2CeAxVW0N7v+StLLLMJ2I3yoHIEhKh4GTgKX4B4whnxfcPjHpfouq9qU9pz7p9nKgGDgkIu0i0g78GFgY9mIi8s8isjPo8GsH5pCWyAYRFmsRfudmQmPS7R78lvZobbue3Hwb/xdX+sGkFf/XU7rFSY8D/igc4EFgo6p+M8fXNRPIEvoUFXxZrwDeISKNQX36WuCspCTQDUxPetoJQ2z2IH5yTbzGDGABcAA/Ea3K5Xn4P/0PJt0PmxI0eVk90A+Uq+rc4N9sVT0z/UlBvfxf8N/7PFWdC3QAMshrDRWrAzQN8bxc5LLtnKZHVdXDwHeBf0t76HHgfSKS/t2/An8/7ga/XwO4H2gAPp1j/GaCWUKfut4DuMAZ+PXkNfi12Wfw6+oAVfhf/uniD0+8Km0bTaQm6buAT4nImiAhfAN4QVX3A78HFovIZ4NO0FlJQ+HuAr4iIhUiUo7fWZgydHAwqnoIeAy/U2920Ll4soi8I2T1WfhJsgUoEpHr8Us2ye9pRUjCS36P14rIymDUR6IOPuyRKuOw7ZuAN+N/rgk34/8iuVVEThCRMhH5MH4/xhdUVUWkGL9/ohf4REi93UxSltCnrk8At6tqXTDaoTGovd4C/HVQt70Zf3hbE/7QtV+kbeMG4OdBmeMKVX0c+L/Ab/BrsicT1LFVtQu/Q+6v8EsSe/A7/wD+H1AJvAy8AmwJlg3Hx/E77nYAR/ATUlhp4VH8jtnd+CWNPlLLGIkTfA6LyJaQ59+GX1P+M1ATPP8fhhlrNqO6bVXtBP4dv/M3sewwfqdtGf6+Ogx8DviYqv4qWO3N+H0eFwPtwSico4nRQGbyErvAhTHGFAZroRtjTIGwhG6MMQXCEroxxhQIS+jGGFMgJmxSo/Lycl2xYsVEvbwxxuSlzZs3t6pqRdhjE5bQV6xYQWVl5US9vDHG5CURqc32mJVcjDGmQFhCN8aYAmEJ3RhjCoQldGOMKRCW0I0xpkAMmdBF5LbgcljbsjwuIvJ98S879rKIrB39MH2upzyxs4nvP7GHJ3Y24Xo2D40xxiTkMmzxZ/gz8N2R5fF3A6uDf+cC/x38f1S5nvKxW1+gqr6d3pjLtJIoa5bOZf1V5xKNyNAbMMaYAjdkC11V/wy0DbLK5cAd6tsIzBWRsGlLj8vTu5qpqm+nJ+aiQE/Mpaq+nad3NY/2SxljTF4ajRr6SaTOJ90QLMsgIleLSKWIVLa0tAzrRbYf7KQ35qYs64257DjYOcxwjTGmMI1rp6iq/kRV16nquoqK0DNXszrzxNlMK4mmLJtWEuWME2dneYYxxkwto5HQD+BfADhhSbBsVJ1/2kLWLJ2LuDFQj+lBDf3800KvA2yMMVPOaCT0B4CPB6NdzgM6gms8jqpoRFh/1blU7HmQuQ3P8oMPn20dosYYk2TIUS4ichdwPlAuIg3AV4FiAFX9EfAwcCmwF+gBPjVWwUYjwvT2aqa3V3Ph6YvG6mWMMSYvDZnQVfXDQzyuwGdGLSJjjDEjYmeKGmNMgbCEbowxBcISujHGFAhL6MYYUyDyLqErQs/cVTZBlzHGpJmwa4qOhOspTad/kP6Zi7n5j7ttgi5jjEmSVy30p3c10z9zMRotsQm6jDEmTV4l9O0HO9FI6o8Km6DLGGN8eZXQzzxxNuI5Kctsgi5jjPHlVUI//7SFlB49hMb6bIIuY4xJk1edotGIsGjnr9neBktedy7f+cq1nH/aQusQNcYY8iyhAwiKU1fF3PlqE3QZY0ySvCq5GGOMyc4SujHGFAhL6MYYUyAsoRtjTIGwhG6MMQXCEroxxhQIS+jGGFMgLKEbY0yBsIRujDEFwhK6McYUiPxM6GJXLTLGmHR5N5eLIsz4iy/SsvhUu2qRMcYkybsWeu/clRQtPNmuWmSMMWnyLqHHZiyCotKUZXbVImOMycOEXtLdBE5/yjK7apExxuSY0EXkEhHZJSJ7ReSLIY8vE5GnROQlEXlZRC4d/VB909prcJr32VWLjDEmzZCdoiISBX4IXAQ0AJtE5AFV3ZG02leAe1T1v0XkDOBhYMUYxIugdD/0LYqWrrGrFhljTJJcRrmcA+xV1WoAEbkbuBxITugKJGoec4CDoxlkBlWcupfsqkXGGJMkl4R+ElCfdL8BODdtnRuAx0TkH4AZwDtHJTpjjDE5G61O0Q8DP1PVJcClwHoRydi2iFwtIpUiUtnS0jJKL22MMQZyS+gHgKVJ95cEy5JdBdwDoKrPA2VAefqGVPUnqrpOVddVVFSMLGJjjDGhcknom4DVIrJSREqAK4EH0tapAy4EEJHT8RO6NcGNMWYcDZnQVdUBrgEeBXbij2bZLiI3ishlwWqfB/5ORLYCdwGfVFWbYMUYY8ZRTnO5qOrD+EMRk5ddn3R7B/CW0Q3NGGPMcOTdmaLGGGPC5W9Ctyl0jTEmRd5NnwuA2BS6xhiTLi9b6EVL19gUusYYkyYvE3q0fLlNoWuMMWnyMqG7rbU2ha4xxqTJy4Tu1FfZFLrGGJMmPztF1abQNcaYdPmZ0MGm0DXGmDR5WXIxxhiTyRK6McYUCEvoxhhTICyhG2NMgbCEbowxBcISujHGFIj8Tug246IxxgzI33HoNuOiMcakyNsWus24aIwxqfI2oduMi8YYkypvE7rNuGiMManyNqHbjIvGGJMqfztFbcZFY4xJkb8JHWzGRWOMSZK3JRdjjDGpLKEbY0yBsIRujDEFwhK6McYUCEvoxhhTIHJK6CJyiYjsEpG9IvLFLOtcISI7RGS7iPxydMPMGhhFy86m/aQ32eRcxpgpb8hhiyISBX4IXAQ0AJtE5AFV3ZG0zmrgS8BbVPWIiIz92T3B5FxFC0+mvbiUf7jrJZucyxgzpeXSQj8H2Kuq1aoaA+4GLk9b5++AH6rqEQBVHfMZshKTc0nJNJCITc5ljJnycknoJwH1SfcbgmXJTgVOFZFnRWSjiFwStiERuVpEKkWksqWlZWQRB2xyLmOMSTVanaJFwGrgfODDwP+IyNz0lVT1J6q6TlXXVVRUHNcL2uRcxhiTKpeEfgBYmnR/SbAsWQPwgKrGVbUG2I2f4MdMYnIuL9aLejY5lzHG5DKXyyZgtYisxE/kVwIfSVvnfvyW+e0iUo5fgqkezUAzBJNzORWnMWPJa7jth9+2ybmMMVPakC10VXWAa4BHgZ3APaq6XURuFJHLgtUeBQ6LyA7gKeALqnp4rIJOCo6+6kr6t9zPhacvsmRujJnScpptUVUfBh5OW3Z90m0FPhf8M8YYMwHsTFFjjCkQ+Z/QJULRsrP5/hN77GxRY8yUlt8XuBCh/AM3ULr4VG7+426mBSNd7GxRY8xUlNct9KKlayhZfCpSMg0FO1vUGDOl5XVCj5YvR+xsUWOMAfI8obuttaidLWqMMUCeJ3SnvorYod14sV5QO1vUGDO15XenqCqt995A2cq1nPLG8/nOV661s0WNMVNWfid0APXoq65k7ollXHj6oomOxhhjJkxel1yMMcYcUxgJXSL0zF1lJxcZY6a0/C+5SITyD9xAy5LT7eQiY8yUlvct9LKVaylZfCoaLbGTi4wxU1reJ/Tihavs5CJjjKEAEnq8udpOLjLGGAogoffVbCF2aDca67OTi4wxU1reJ3TUo/XeG+h+/IfMaN3JxWcs4lNvWTHRURljzLjL/1EugdLXv4ue+av5XdVBHtvRZCNdjDFTTv630PFHuhQtPNlGuhhjprSCSOjFC1eBjXQxxkxxBZHQ483VYCNdjDFTXEEk9L6aLTjN+wZGuhRHhWXzp/O21RUTHZoxxoybgkjoqEf3w9/G7WwCVRxXqWvr4ZO3v2jzuhhjpozCSOhA0ZKziM5eCJGodYwaY6akgkno0fLl1jFqjJnSCiahu621GR2jJUURXnPCrAmKyBhjxlfBJHSnvsrvGPVcUL9uHnc9bn9uv9XRjTFTQk4JXUQuEZFdIrJXRL44yHrvFxEVkXWjF2KOVOl/5RHwXBD/7FBPsTq6MWbKGDKhi0gU+CHwbuAM4MMickbIerOAfwJeGO0gcxVdsBwiqbMZ9Fgd3RgzReTSQj8H2Kuq1aoaA+4GLg9Z79+AbwN9oxjfsLitdUBqeSUiWB3dGDMl5JLQTwLqk+43BMsGiMhaYKmqPjTYhkTkahGpFJHKlpaWYQc7JMlSK7f5uYwxU8Bxd4qKSAS4Cfj8UOuq6k9UdZ2qrquoGP2zOKMLlpOevT3FSi7GmCkhl4R+AFiadH9JsCxhFvBa4GkR2Q+cBzwwER2jYUMXAR7Z1mgjXYwxBS+XhL4JWC0iK0WkBLgSeCDxoKp2qGq5qq5Q1RXARuAyVa0ck4gH4dRX4XY2o5qavGvbemykizGm4A2Z0FXVAa4BHgV2Aveo6nYRuVFELhvrAIdFlXjNJtI7RntiLtsOdExMTMYYM05yumKRqj4MPJy27Pos655//GGNnNuyH433IyXTUpY/sq2Ray5YbVcwMsYUrII5UzTBqa/CaT9kZRdjzJRTcAkdVXr3bMTKLsaYqabwEjoQb9qHxm20izFmainIhN5Xs8XKLsaYKacgEzrqWdnFGDPlFGZCxy+7MEjZJeZ43PTYLj7yPxu56bFdxBxvAqI0xpjRk9OwxXzUV7MFt7OZ6IJliBwbqljb1sNjrxzi8795mZ6YC8Bz+w7z8+f3s+m6iygpKthjnDGmwBVu9lIv60lG/3rfKwPJPKGj1+H7T+w+rpd0PeWJnU18/4k9PLGzyTpgjTHjqmBb6OCfZES8H9JOMursd0LXv+3ZGv7xwlNH1EqPOR6X37KBvS1HcVxlWkmUNUvnsv6qc+1kJmPMuCjcFjrZ53bJpifmJ+XhtqxdT7nslg3sbOwi7iqK/0tgc+0RntzZNILIjTFm+Ao6oaOK29YwrKfsbTk67KGNT+9qZl/L0Yzl/Y7Hv/1+h5VejDHjorATOpBeQx+K4+qw50/ffrCTuBv+Ok1d/Tb23RgzLgo+ocf3bYRBSi7p5ZiSosiwL1l35omzmVYcvitjjmcX2DDGjIuCT+hOXRVuW31oHV1VUSeGqjeQ9OOux+3P7R9WmeRtqyv8jlTVjNeZVhLljBNnH9+bMMaYHBR8QkeVo/d9hXhLDaqen8RVUc8j3lJD20M3getAMFbdU6iqbx9WmeSZPS3+iUkix8a8qxKNCMvmT+dtq0f/cnvGGJOu8BM6gOfRvP7zHP7tN4jteZbY7mfpfvQmmtd/nuLyZRBJHb3ZG3OHVSZ55UAHffHMM01dT6lr6+GTt79oHaPGmDFX0OPQU6hHX/Umipp3piyLN1f71yFNGqs+nDq66ymPbGvM7HoNWuo9MXegxX/h6YuO800YY0x2U6OFPoi+mi04zftQzx1RHf3pXc3UHu5OWZZeRx9ui98YY0Ziyid01KP/lUfAc0dUR99+sDO03JLMOkaNMePBEjoQXbB8xHX0M0+czbSSaOpC9VAnBupRHLWOUWPM+LCEDrittX4dPUmudfS3ra5g+fzp4LmoenixXpyDO3HbD4Eqjmsdo8aY8WEJneDC0iOoo7ue8snbX2T/4W4QQV0Xp/0Q/dseJTp7IUSiA/O6DHcopDHGDJcldADVEdXRn97VTFV9O71xDyRCpKiYormLKV51LhSVpqxrHaPGmLFmCT0QVkfvGSIJbz/YSW/avOqSSORpJRzrGDXGjDVL6AG3tY70ibwiwqB19NMXz8qYO12dfuJ7X/BLOLE+6xg1xowbS+gJkqVWnuXaFK6n3LahhrjrDczhop5L7NBunPqX6H7427idTdYxaowZN5bQA9EFy0nP3p6SteTy9K5mtjZ04CkDc7io63B084OgStGSs6xj1BgzriyhB8KGLgI8sq0xtFX9yoGOzPp5tJjihSsAiJYvt45RY8y4yimhi8glIrJLRPaKyBdDHv+ciOwQkZdF5AkRWT76oY6tbJerq27tzriMXLb5W9TpJ95c468TcoAoK45Yx6gxZswMmdBFJAr8EHg3cAbwYRE5I221l4B1qvp64F7g30c70DGnSrxmE+kdo2GXkXvy1Saq0y45p6o47Yfoq9kCgNOwFfVc/wARHCRKiiLWMWqMGTO5tNDPAfaqarWqxoC7gcuTV1DVp1S1J7i7EVgyumGOD7dlP+rEM5Y3tPdy8x934XqK6yk3PriDWPol51Tp3f08qD+vS9GSs5BI1J8fPRjbHneVZ/a0jPn7MMZMTblMn3sSUJ90vwE4d5D1rwL+EPaAiFwNXA2wbNmyHEMcP059FW53GzLnhGMXqsDvHL3lqX389JlqlsybTsOR3swne44/FW9gsBq6TaNrjBkLo9opKiIfBdYB/xH2uKr+RFXXqeq6iopJWHpQpeOp2/wrGIXoc5S9Ld2ZtXNVvO4jA+UWOL75YYwxZiRySegHgKVJ95cEy1KIyDuB64DLVDVzuEie6KuuxDnaGnoN0sHE9jw7UG6B1Plh9DiuV2qMMbnKJaFvAlaLyEoRKQGuBB5IXkFEzgZ+jJ/M83ugtXp+K90Lb6Vn5aUOYRyYH8Z1Bso3I7leqTHG5GrIhK6qDnAN8CiwE7hHVbeLyI0iclmw2n8AM4Ffi0iViDyQZXN5oa+6EufQrpTW9aDcuF9iSRNdsByixSnLemIu2w50jFaoxhgzIKdriqrqw8DDacuuT7r9zlGOa2KpR/dD36Jo6RuYcfE1aHSQ3aQeTuNunPqqjIfc1lrU6UeSrlcK/slK11ywmmgky7wCxhgzAlPnItHDpYpTV0nHbX+DvPYvmXbKG4lEo2ifP/7cdT3co4fR2kqcuqqBsebJnPoqnPZDFFesTBk1kzhZ6aIzTxi3t2OMKXyW0IfieXQ990u6nvslM2fOHFh89Kif2JOXZVCld89GiitWkDxPTOJkpQtOX2StdGPMqLG5XMZYvGlf6MlKBzv6MqYUMMaY42EJfYz11WzB7W7L6Fx1POXL971CzPGyPNMYY4bHEvpYG2QYZEt3jAv+8ylL6saYUWE19HHQV12Jd7SNyOyFKZ2jAA3tfVx2ywYe+se3pdTTXU95cmcTv3/5IE2d/SyaU8Zfvn4xF7xm5HV311Oe3tXM9oOdnHnibM4/baHV8I0pIJbQx4N69D63nhkXXwvRaMbDu5u6Uka9uJ7y0Z9uZGN1W8o0Aw9uPch5qxaw/qpzh52IY47H5bdsYG/LURxXmVYSZc3SuSPaljFmaBPRgLKEPk6cuiq8vi4i0+dktNI9hS/f9wrvOG0hJUURnt7VzJa69ow5YzyFF2vahj3k0fWUy27ZwKuNXQPLemIum2uP2PBJY8ZAogG1p7kLx4PiqHByxUweuOatGdchHk1WQx8vqvQ+cyvp860ntHTHuPyWDbie8sqBDvqz1NUdTzPmZx/K07ua2Zc2fzuEz/U+UVxP+eP2Rj5790t89u6X+OOO8CtFGTPZJRpQOxv9ZA7+1NmvNnZxwX8+PaZ9ZtZCH0dO7Us4B3ZQdOIZA9chTba72S+9ON7gH3hTVz9P72rOeRreVw50EE+fv32E2xoLYSWmB46jvJQPEn0kD71yCIC/OM7+ETN5PPlqE3uaMxtQ4F9b4YL/fJon//n8MWmpWwt9PKnS/dC36Nx4T8rMjAmuBzc+uJ37t2RMZpmi3/GGNR+MN0hLd7jbGgtPvtqU0V+QXF4qNIkD2Kfv3Mz9VQe5v+ogV9+xmb/4/jM24inPJS6AM9ivy4b23oFf46PNEvp4U6Xr+V8Rb60NnfjrQEcfjZ1Dzz6c7eLVoYZo9A1rW2Pg9y8fCi1EjaS8lA+efLWJTfuPkPy2FHi1sWvMvuhmfDy9q5nmrqG/v3uau8Zk1lVL6BNBPZrv/AJeT0dGUvfUT2Qpq6tmrFfb1pPTH4TrKY9tH7yVm+u2xoLrKc/uac36eKIkNBm4nvLEzia+/8QentjZNKLEm2jBpX/GCYmymxl/o9GPs/1gZ0b/V1jDzfHg5Yb244o3jNXQJ4rn0PvnnzLj4n/KmGI3c10XIqnDHRPT8A5V+852Qevk+v1EXhrv6V3NdPZlTo2QkCgJjSS20axTj9aohaFacK6HzfMzAWKOx2U/eIZdTUcHfi3+ruogp50wa1if8emLZxERSD8OpH/nAH6z5QD/eOGpo/o5W0KfQE5dVdYTjhJU1V9n2ixIm4b34W2N/N3bVxGNSNhkj7ie8rWwC1qnKSmKsHzBdI72Ozlfqcn1lD/vauGR7Ydo7oqxcFYpF595Am8/tWLoP9Ckl6jcf2TI+H7/8iE+fM6yjO0O9qyY4/HxW19gb0v3wLIHth5k7bJ5/OAjZ+f2JQpewPWUj6VtKzFq4dLvPcPP/+acnL+UL1S3ZR3BlNDY2c+9m+t5yynlOW0zNPQJqNoM9po66Kd1/NvPeRshy1xP+fT6SvYlfb6JdV9t7OKS7/6Z//7oG7J8xqlbbGjryYxTdeBC8cmaOvtGfUCCJfQxoJr6B6yQ8tMtcct1Pfr3PMu0N7x3sK3R/+qfKD75HIoWLEtJ/Ptbu7l9w37WLp8X+swttUdo7OgLDXCgxSBCzPX48Z+qWTCjlEgOicnzlK8/vIMdh7pSlj+8rZFl86bx9fe8jqIcWjSepzz0ysGQ8FJbM3VtPfy6siHr+wzb7pd++zJ1bakX8/YUXqo/wr2VDaxbMT+nbQFU7m9LSebJqluPct+W3Lc3q6wIkdTklP5+Y67Hppo2ls2fkXOM4L/vLXVH2Fh9GIDzVi1g7bJ5OX2mU1nl/jZqWsM/X4D9h7t5bHvjkJ+x5yn/9afq8EOXeiCpv7IdV0f9l7El9IAO/MfnpR1msyblkGXJzx2yUZF+6bqQx53W/SBQtGAp6dPw7j/cnTXR7T/cHVqP93o7iJTOgKKSYBnsbTlKVX17Tkmzqr6d3U3hw7LqjvRy3e+28c33vm7IRLKl7ggH21OTbtgvhH7Ho6b1aM4Jvaq+nQPtIQcy/JLG+o21w0p0iQQZxtPhby+XxuqL+4/w3rOX5LxNx/H48v2vUH/k2P58dt/hYR1gpyLPU+7YWJtRIklZR+GO5/cPfMaJA+fz+1pp646BCPNnlFAxs4S27pBymufgdbcTmV2RcuCeVhLljBNnj+r7mXKfsqqfcD3129CJRO15qctUk/6NYTxOy35ww69fqqrED9cTr6vCaanN+M0pwLL507Nue9n86RRH08oUTox4w3aIpB7LY8HBIRc1rZkHimQNR3rYUndk0G04jsdPN9SQUX1QDZ3I7MX9RwYdfpke32CdWYe7+4eMLyWkIR4fzvZqD/eEbi/9QNbU2UdVfW6dZp6nXPe71GSekDjA5rrvppqq+naOdMeGXK/1aIwtdUdwHI8v3vcy3/njbp6rbuPVpqO82tjFc/sO87uth3DT/p5VFa/7CO13f554Sw3qxkGV6cHUG+eftnBU309BJ3RPFdfz/yUStac6kKgng3hdFU77wYwvtKridDTRdOcXghpc5nMT7yeM5yl/2HYIx9WBEot6LrGDrxLbtxGc1JZENCKDHhxStj3EzvMUfrqhGifb2a6Ox+d+vZWO3szOUHXjeEePHFeCG+rErEQrPZck53lKfdvgB7pct+d5yov7Q1r7IQf04VJsupgAABhzSURBVBxgt9QdoSEkmSfkcoAN43nKltoj3LelgS21uR9Qx4vnKZX727jlyT3c8uQeKve3DTvGsMZJ2KgyBZ7d08znfr019MCZPUiH7g3rwXVpWv95Wn/3LWbUPsMPPnz2mJw0V1All0TrW5PuT3qqdN57HbPedyPFC5b6o1lcF6f9II13fC5orZZStGA5YVn91g3VqCov1rRxpCfOvBklvGnVAlSVPc1Bj70IAniOQ9fmB4k07STevI/iE09HJAIiuKo8sq0xp9JBLn+CHb1OaOnFcTw+d08VLSGtIlXFPdpGfM+zTHvj+0gvL+VSdvE8ZcPe7MMgE470xHIqMVXVt9OUdl5A2IiFXLZXVd+e0aeRaMGld3oXR4UVC4auoedaMhhuWchxPK773TYajvTgqX/AP2luGV+/fHKUb0arxDTUwT/Zi/vbGc5pX4kBDfG6l4IFHn3VlcwsL+bC0789jC3lrmASel6fjOF5NK3/PGUr1zLjpNNwWmuJ11WllB6c1v3gxiFSmvLUzj6X7z6xN2XZc/sOE41Ixj6RaDElC1fgNO6g7+VHKD7hVCjyO2pyraN7nrKpti1jeViSazjSQ1V9O2uWzmVL3RGe29vC5rr27KNaPJf2p26ltLQU4v0Zo3oSdWUga+dfVX077T0hLf+0+OKuDhwgPE+pqm9n/+FuViyYwZqlcwcSX01rd2i8g20vm9Btqd/pXXbWu0HVP4SJUByN8PqT5mTdVkKuJYNcD2BwrIST3KnsekpdW+79I2NpqBLT5369lZs+eNaQSd1xPJ58Nds5Dkp602W45/CqG7TOx7FlWRAJPY9T+THB0TvSuDP04XhdFW5PO9FBhjgmCzvAqRsn1lxDBPwWf1odPdEK9lR5fl9rSos/OWGGtTJDvwAKe5s6uHtT3ZA/U/3+gjr6ajYTmT4dp7M5Y1RPU2cflbVt3Lu5IWvLLNtP6DAv7j/CX73uRP7vg9sHWqIR8UeiXPWWlZy9dF54icSJ+yumnT8wWEdm1nKLE4NIBIlEU95rv+NR1dA+5MiKbB3fQMYBZ7AO9GSDlXAS5ZvhjBJKGOzAORxDlZhajvbz5ftf4Vvve33K9j1Pqaxt4w+vHKKls4+Ofje8L8iN4/X3hM6Mmi6sIZNY7na1Hmudj5O8TuiJNDIlqNLz7HpmvuuzSHT4H1uinNFXs4Xp06bhtNYStvc2Vh/m3rS5ZJ5LS5hhrUwv1kOkdEbGH/dvtzbmFJvX3x30F3igSrz6xdBRPbc+W0Nnb2bNue5IL9feU8W0ksz55nHieD2ZowwOtffwz/duTSn/eOqXi256fA/lM4ppT6vzqypu+yFUyDjgNHb0DrSC05OX63mhI3rcjib/PRel/vJyPM2pTLJs/vSMYZD+EDlJ2XdDdaAPvP8hSjgjGtXD6JVwcikxAdQf6R048CQS+U+f2UdX/+Dt7EQJrPvZ9cy8+J+Qouwn/WnQGaeQ8XevrkP7U7cSGee6b14n9MnWSTPW4rUv4Ryuo7hiZU6t9BRBOWNgUrAsT2/IMtxv8IQZI167ldJT3zy8mBJU6X7ix6klpsSonrQ4w5J5Qmt3DNL6ERMJOFaziWlvfH/KY3GP0Fr+se2FnMGqSqz6xWAY6bKUh2KuUt3SheN53BYceBS/MS8iGSMgEtvKVk5LjJ4ZsjUcOmyG1H2X459LLiWc4ZRv4FgneMvRY30RiRJOruWR4cSX8Py+VtYsmZtRax+U59K9YT3xupdwj7Yic04Y5LumdP3he5S98XKKK1YwsJPVI3ZgB301m5k+bVqW546Nie/dGKG86PAcbao03fkFnI6mnM/o9J+maLyXvprNA8uydbIOprU7lvHFSLQyY/uezzr8ctC4PI/4wR3E67akPjhaJdqBBDw6G1Q3HpwXEP7VeXp3Czc/voeOIJmD36rNVgJzWvcPlNPSP9NcRs+ED4OUjPerCrU5jJrJpWSV6C/IRaLenZzMk7Uc7R/WsMpsJaaw78Pm2jY+c9eWnJO5quK2HfDLJKq0P3UbOsiQYre1nnhdpT965bffoH/3Bvp3b6DrkZtpufcGwmZUHWt520IfauhcwfIcGm/7DIs/dhPR8tSSREJYi6LvlcdS/sCytQqHLUiY8dqXcNoPZpQhMlfXgee5Pe30/Pk24rUvZRyhR3LACX29IGkWVaw4/m0lRuHUVWXd3uGwVv0Q2xoop4X8xG/u6udf7t3KzLIiJDiBJdGnAWQfBikMnDiWkMvJSrl+r3I98Wmwk7wShlOXz7XEBNDvQn+OjYzEMOHO31w3sPG+6kpiB3ZQuvS1qQdwVZy2+mPrqkdf9SYijTtSY5oAeZnQp2gqP8Zz6Lj3yxQvO5uSU86Fsjm43W24+7cw/Zz3E00rBWh/N72bf5uybLidrNkMtFiD4Zezr/xPiuYsyr5NVeIHX6Vj02/pq67M+pM01wNOtk6pxGMDSRP8DsjikW8vUbaKqPonhOWwvayStgV+OS3bT/wDHX2QNGV9ok/j/WcvCa/LZ6nxJ8byD1YqyVpcSNsvuWwL/Bb1UCPQhl2XD9mc19OZUyfmwCZS514g3rqfpju/wPSypM9TPVruvYE5r3kLJaecS2TGfOKdLfTsetZP3pOwUZlTQheRS4DvAVHgp6r6rbTHS4E7gDcAh4EPqer+0Q01yeTbj+NPlXjtFuK1W+jp8X9KT58+g47azRQvW0vJay8iUjYTp76K3sr7IH28bQ6drGGjJdIfT0mYnkfjbZ9h0Uf/I7WmOPAEj/jBnXQ9+E36ugf/yZ7LAUdVwfPQSCR8naSkmfP21EPJ3F7yKJzp06Yd1wFRVfFivak11uAn/oK/+pdBO+IS6o708t0n92R+FVJq/JmdyoMNrcw2JBXXgWjmiKhcRs2EdcSOdBw/ZC8xxRu2UXLKeUMOGEh0ZHpuDK+zCbe1nti+jXTsfDZoVacdoNUb+J4BKd+1yWjIhC4iUeCHwEVAA7BJRB5Q1aTfF1wFHFHVU0TkSuDbwIfGImAzBFXitZvp2PlnYPA/vME6WVUVr7MFKZ0OIaNXgIxWpr/MoWn951NaNZ7n4R49jLu/MrS8ku19DHbAScTX/fydzLr4syE149QEPFhZI/n9dD3yA2Ze9PdQXJbykHO47tgonBziS44jbN8dfenhjJ/lfdWVOXTEJW079D04/q8bQOP9SJax/GEt4UFPfJq1IGVyqVxHzSTWTY81bBz/UAeIwYZ+xvZuJDLvxEEHDPh9SX0cfuhm+qo3pf46nKASyWiToTrXRORNwA2q+q7g/pcAVPWbSes8GqzzvIgUAY1AhQ6y8fnLT9eLvnzbsAOu2lrF0a5jrTs3mNwqmjRfuC0b3rKSRacgaQnMi/fitexHymYSnb80NOFrvI9Y094xjS9SsYJI8bSU11dV1InhNu/z11+wDClNPXAlYsvY3oKlREpnAmRuM96H21LjL5i5gMi0WYjn4XW3oX1Hc44veZv+SJ20xzyX/oM7Q99/pGwWxeXL/DN4h0lVwY3hNO0bNLaT5pYxe1rmQa2lq5/Wo6kjSAYmc5uWWc44aU4ps6en1ulz3SZk/vLLFldCV288YxRW8ufmei4lJ5yKFJWE/r2iHs6hXRP+/Zs1aw5rz16T9X0O5Z6/f/NmVV0X9lguJZeTgPqk+w3AudnWUVVHRDqABUDKOdgicjVwNcDMxSfnFHy6NWetwfGUntjwRlSY7FSVrj6Hrj5/n84uK2Jm2Sxk+SJUldrDPfTGU1swZcVRVpxQTmT56E4ulM7zPPYf7k2ZQ9x/7flElpUPEX9mbKpKZ2+cps5+kofTD/5+Fg8aX01rT+jZpGXFUZbPL+Nov8uRnjiewszSKOUzS4gsCR/imW1/50JEOKl8DrOX+l/PbMm0vq0HnH40SDDqxvF6Ovy+j+KylGQYEWHO/HI6+5yM7TQc7qR/224gezLzGwRLUg5Q2Q5+tft24/R0ZN1eWLIWESrmzaZiuf+es30exdEIq8pnED0pPXWNv+Lo2A0uHNdOUVX9CfATgHXr1umvPv2mEW3nSHeMVxu7hl7RjIrkebZFhHNXzh/XebZH6wzDsdpm8nSq7b0O86YXH9dc5IkTYW7dUE1n3xDTKydZNn96ymn5W2qPcPPju1OG+Q0kw+KylB6O6LTZ/pw/acl26fzpfPANS/jeE3sytqMSoWvT7/zyRVJpb6DOPGMms/7qS0QTZxKLIALlM0pDhzFKUSkt91wP6mVsr+zkcyi/7F8y4iuOCp96y6qUUs1E/70OpXxmCasXzRrx8+/5++yP5ZLQDwBLk+4vCZaFrdMQlFzm4HeOmgIQiQjrVswf0eneo/X6a5fPy/lElvHe5mjvn0hEOGflAtYtn59y0HEcj5uf3BP6nIqZpXz98temJK01S+cyb0YJLWmXvAutMUs0dCz3OSvmsXbZvNDtECli7v+6isak8xuSFS9bQ/HCk5Gk1nZUhLecsoDfv3wo4wARmTmfspVvoK96U8brzLvof2dMVQEwb3oJa5bOTV19gv9eJ1IuCX0TsFpEVuIn7iuBj6St8wDwCeB54APAk4PVz4/XKJ0jYsykln7Q8TzljBNmsbOxK6WTMdsMg5GI8PHzlnPz47uHPFUeMhN9cVRYWT5zYDvffWJ3ytmuIkJ09kJmv+lDOFsfzDyXoGJFxrBO11OKIjLoAaJdIsw44+1EZswDgXkLTwmti0cj8LHzlk+alvdkMGRCD2ri1wCP4g9bvE1Vt4vIjUClqj4A3AqsF5G9QBt+0h8zM0uLKIqKP9e3MVNEJCJc9xdnDKucsHbZPJbMm5ZxOb5cJLd+1y6bx/wZpZmt/UiE2W+6Am/d5bgtNXjdbXjb/0RfzRaKV51D+tDVkqIIK8tnsnzBjNADRNHcEyh/z5cyYgn7VbFgRunACVbGl1MNXVUfBh5OW3Z90u0+4IOjG1p2RdEIi+eUUT+CP1Jj8tlwywmRiPD1y1/Hl377ctZ5ekKfJ6mt38Fa6SBEisuInHg6AKWnvNmfLXPm/IxEvGh22cBBIuwAcWybw4vP+PJ2LpfFc6ZRUmQfpjFDKSqK8O33n8W171zNaxbNpHxGCSfNKWP+9OxDBJfMm57R+k200sNI0KEqIkgkQtGcRRnTC4Nfk49EZOAAkT7gI9eTtMLiM3l66j/4028unT+dfc25XabLmKks0dF6zsoFA8scx+PTd1bSkzZEcum8aRkdrIlthLXSw4Ql5kRNPiFbGWcos8uKQuMzedxCB1g4q4yFs49zciljpqiiogg//ug63rvmRJbNn8YZi2fxuXeu5lvve33W6WzXLpvHaSMccpc+IiVbK30wpUURfvChsyfFZfAmo7xtoSecXDGTkmiEA+29k3GuHGMmtaKiCFe8cRlXvHHZ0CsTdMxeegYv7j/MD57cm9PoGche804cIHYcyjyvRIDVFTPoiXtEIv667z97iSXzQeR9Qgf/5IfymaUc7Ojl8NFYfl9f1JhJLhIRzltVzrpl87nvpQYqa9vwPKWxM4abpVWVreadOEAkrjnb0N6HiCXvkRpyLpexsm7dOq2srBz17Xqe0tEbp6M3TlefQ0/MybkVYYwZuU01h7np8cwTnypmlHDTFWssOQeO90xRETmuuVzySiQ4aWHeDH/SIFWlN+7SG3Ppczz64i79cY9+x6Xf8axMY8woecPy+Zy5eDa7mjpxPL/MsmRu+ElPZmwUXEJPJyJMLyliekn4FKz9jkd/3KPPcYk5HjHXw3EVx/PwPHBVcT3FU8Xz1Fr7xmQRiQhfvvT0UZ93x+Su4BP6YESEsuIoZcVR5jD0RQUguKhDkNi94LbjBf93PfqDg0LM8Yi7HnHXf8zq+mYqGIt5d0zupnRCHwkRoSg68ln5PFWU4FKE6EDJJ3E/cRsI1ju2PknrJK8XJvRC8Hrs2QOvd5zHmeTtZT6W28ZTrgZG4qK/xxdX5nbTLyw8wm0OPH/oDeTyEsc2MzrvOf11R7OkmOvnebyvPfLP5vjebOjfcJZNDvX3lPx3oqR+ztEx/MViCX0cRSJCZNQuZ2+MMamsp8IYYwqEJXRjjCkQltCNMaZAWEI3xpgCYQndGGMKhCV0Y4wpEJbQjTGmQFhCN8aYAmEJ3RhjCsSETZ8rIi1A7QifXg60jmI4Y8XiHD35ECPkR5z5ECNYnNksV9WKsAcmLKEfDxGpzDYf8GRicY6efIgR8iPOfIgRLM6RsJKLMcYUCEvoxhhTIPI1of9kogPIkcU5evIhRsiPOPMhRrA4hy0va+jGGGMy5WsL3RhjTBpL6MYYUyDyLqGLyCUisktE9orIFyc6ngQR2S8ir4hIlYhUBsvmi8gfRWRP8P9xv9CiiNwmIs0isi1pWWhc4vt+sG9fFpG1ExznDSJyINinVSJyadJjXwri3CUi7xqnGJeKyFMiskNEtovIPwXLJ9X+HCTOSbM/RaRMRF4Uka1BjF8Llq8UkReCWH4lIiXB8tLg/t7g8RVjHeMQcf5MRGqS9uWaYPmEfYeAxPUb8+MfEAX2AauAEmArcMZExxXEth8oT1v278AXg9tfBL49AXG9HVgLbBsqLuBS4A+AAOcBL0xwnDcA/xyy7hnBZ18KrAz+JqLjEONiYG1wexawO4hlUu3PQeKcNPsz2Cczg9vFwAvBProHuDJY/iPgfwe3/w/wo+D2lcCvxmlfZovzZ8AHQtafsO+QquZdC/0cYK+qVqtqDLgbuHyCYxrM5cDPg9s/B94z3gGo6p+BtrTF2eK6HLhDfRuBuSKyeALjzOZy4G5V7VfVGmAv/t/GmFLVQ6q6JbjdBewETmKS7c9B4sxm3PdnsE+OBneLg38KXADcGyxP35eJfXwvcKGIjPkFegeJM5sJ+w5B/pVcTgLqk+43MPgf6nhS4DER2SwiVwfLFqnqoeB2I7BoYkLLkC2uybh/rwl+ut6WVLKa8DiDn/xn47fYJu3+TIsTJtH+FJGoiFQBzcAf8X8ZtKuqExLHQIzB4x3AgrGOMSxOVU3sy68H+/JmESlNjzMwrp95viX0yeytqroWeDfwGRF5e/KD6v8em3RjRCdrXIH/Bk4G1gCHgO9MbDg+EZkJ/Ab4rKp2Jj82mfZnSJyTan+qqquqa4Al+L8IXjOR8WSTHqeIvBb4En68bwTmA/86gSEOyLeEfgBYmnR/SbBswqnqgeD/zcBv8f9AmxI/t4L/N09chCmyxTWp9q+qNgVfJg/4H46VASYsThEpxk+Sv1DV+4LFk25/hsU5GfdnEFc78BTwJvwSRVFIHAMxBo/PAQ6PV4xpcV4SlLVUVfuB25kk+zLfEvomYHXQE16C3znywATHhIjMEJFZidvAxcA2/Ng+Eaz2CeB3ExNhhmxxPQB8POipPw/oSColjLu02uN78fcp+HFeGYx8WAmsBl4ch3gEuBXYqao3JT00qfZntjgn0/4UkQoRmRvcngZchF/rfwr4QLBa+r5M7OMPAE8Gv4bGVJY4X006gAt+nT95X07cd2g8e2BH4x9+L/Ju/HrbdRMdTxDTKvxRAluB7Ym48Gt8TwB7gMeB+RMQ2134P6/j+PW8q7LFhd8z/8Ng374CrJvgONcHcbyM/0VZnLT+dUGcu4B3j1OMb8Uvp7wMVAX/Lp1s+3OQOCfN/gReD7wUxLINuD5Yvgr/YLIX+DVQGiwvC+7vDR5fNU77MlucTwb7chtwJ8dGwkzYd0hV7dR/Y4wpFPlWcjHGGJOFJXRjjCkQltCNMaZAWEI3xpgCYQndGGMKRNHQqxiT/0TExR9GVgw4wB3AzeqfZGNMQbCEbqaKXvVP30ZEFgK/BGYDX53QqIwZRVZyMVOO+tMzXI0/UZWIyAoReUZEtgT/3gwgIneIyMAMmSLyCxG5XETODObIrgomZ1o9Ue/FmGR2YpGZEkTkqKrOTFvWDpwGdAGeqvYFyfkuVV0nIu8ArlXV94jIHPwzLlcDNwMbVfUXwRQUUVXtHd93ZEwmK7kY49fVbwmuOuMCpwKo6p9E5L9EpAJ4P/AbVXVE5HngOhFZAtynqnsmLHJjkljJxUxJIrIKP3k3A9cCTcBZwDr8q2El3AF8FPgUcBuAqv4SuAzoBR4WkQvGL3JjsrMWuplyghb3j4BbVFWDckqDqnoi8gn8Sx0m/Ax/MqhGVd0RPH8VUK2q3xeRZfgTOD05rm/CmBCW0M1UMS246kxi2OJ6IDG17H8BvxGRjwOPAN2JJ6lqk4jsBO5P2tYVwMdEJI5/haJvjEP8xgzJOkWNGYSITMcfv75WVTsmOh5jBmM1dGOyEJF34l904QeWzE0+sBa6McYUCGuhG2NMgbCEbowxBcISujHGFAhL6MYYUyAsoRtjTIH4/6USaUK/mnATAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract components of date, moving average, moving standard deviation for each indicators, sesonality and trend for one of them (CO)\n",
        "def make_features(data, max_lag, moving_average_size):\n",
        "  data_new = data.copy()\n",
        "  data_new['year'] = data_new.index.year\n",
        "  data_new['month'] = data_new.index.month\n",
        "  data_new['day'] = data_new.index.day\n",
        "  data_new['dayofweek'] = data_new.index.dayofweek\n",
        "\n",
        "  for col in get_target(data):\n",
        "    data_new['moving_average_{}'.format(col)] = data_new[col].shift().rolling(moving_average_size).mean()\n",
        "    data_new['std_{}'.format(col)] = data_new[col].shift().rolling(moving_average_size).std()\n",
        "\n",
        "    for lag in range(1, max_lag + 1):\n",
        "        data_new['lag_{}'.format([col, lag])] = data_new[col].shift(lag)\n",
        "  \n",
        "  decomposed = seasonal_decompose(data_new['CO'])\n",
        "  data_new['seasonality'] = decomposed.seasonal.shift()\n",
        "  data_new['trend'] = decomposed.trend.shift()\n",
        "  \n",
        "  return data_new"
      ],
      "metadata": {
        "id": "0Itcx0TBjEYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find optimal moving_average_size\n",
        "best_mape = 2\n",
        "best_smape = 2\n",
        "\n",
        "for moving_average_size in np.arange(10, 200, 10):\n",
        "  data_clear = make_features(data_clear, 10, moving_average_size)\n",
        "  \n",
        "  data_clear = data_clear.dropna()\n",
        "\n",
        "  target = data_clear[['CO', 'NO2', 'NO', 'PM10']]\n",
        "  features = data_clear.drop(['CO','NO2', 'NO', 'PM10'], axis=1)\n",
        "\n",
        "  model = linear_model.Lasso(alpha=0.2, normalize=True)\n",
        "\n",
        "  def smape(A, F):\n",
        "    return 1/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
        "\n",
        "  def mape(A, F):\n",
        "    return (abs((A-F) / A)).median()\n",
        "\n",
        "  # do CV\n",
        "  tscv = TimeSeriesSplit(n_splits=3)\n",
        "  data_clear_drop_index = data_clear.reset_index().drop('data_time', axis=1)\n",
        "  features_drop = features.reset_index().drop('data_time', axis=1)\n",
        "  target_drop = target.reset_index().drop('data_time', axis=1)\n",
        "  scores_mape = []\n",
        "  scores_smape = []\n",
        "\n",
        "  for train_index, test_index in tscv.split(data_clear_drop_index):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = features_drop.loc[train_index], features_drop.loc[test_index]\n",
        "    y_train, y_test = target_drop.loc[train_index], target_drop.loc[test_index]\n",
        "\n",
        "    model = model.fit(X_train, y_train)\n",
        "    predictions = pd.DataFrame(model.predict(X_test), columns = ['CO', 'NO2', 'NO', 'PM10'], index = y_test.index)\n",
        "    \n",
        "\n",
        "    score_mape = mape(y_test, predictions)\n",
        "    score_smape = smape(y_test, predictions)\n",
        "    scores_mape.append(score_mape)\n",
        "    scores_smape.append(score_smape)\n",
        "\n",
        "  final_score_mape = pd.DataFrame(scores_mape).mean(axis=0)\n",
        "  final_score_smape = pd.DataFrame(scores_smape).mean(axis=0)\n",
        "\n",
        "  if final_score_mape[1] < best_mape:\n",
        "    best_mape = final_score_mape[1]\n",
        "    best_ma = moving_average_size\n",
        "\n",
        "  if final_score_smape[1] < best_smape:\n",
        "    best_smape = final_score_smape[1]  \n",
        "\n",
        "print('moving_average_size', best_ma)\n",
        "print('MAPE', final_score_mape)\n",
        "print()\n",
        "print('SMAPE', final_score_smape)\n",
        "print('------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEbV9Vy5lNIJ",
        "outputId": "1a3473ff-0f79-4164-bf6c-13d67ae3a449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2182 2183 2184] TEST: [2185 2186 2187 ... 4366 4367 4368]\n",
            "TRAIN: [   0    1    2 ... 4366 4367 4368] TEST: [4369 4370 4371 ... 6550 6551 6552]\n",
            "TRAIN: [   0    1    2 ... 6550 6551 6552] TEST: [6553 6554 6555 ... 8734 8735 8736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2175 2176 2177] TEST: [2178 2179 2180 ... 4351 4352 4353]\n",
            "TRAIN: [   0    1    2 ... 4351 4352 4353] TEST: [4354 4355 4356 ... 6527 6528 6529]\n",
            "TRAIN: [   0    1    2 ... 6527 6528 6529] TEST: [6530 6531 6532 ... 8703 8704 8705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2164 2165 2166] TEST: [2167 2168 2169 ... 4330 4331 4332]\n",
            "TRAIN: [   0    1    2 ... 4330 4331 4332] TEST: [4333 4334 4335 ... 6496 6497 6498]\n",
            "TRAIN: [   0    1    2 ... 6496 6497 6498] TEST: [6499 6500 6501 ... 8662 8663 8664]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2152 2153 2154] TEST: [2155 2156 2157 ... 4305 4306 4307]\n",
            "TRAIN: [   0    1    2 ... 4305 4306 4307] TEST: [4308 4309 4310 ... 6458 6459 6460]\n",
            "TRAIN: [   0    1    2 ... 6458 6459 6460] TEST: [6461 6462 6463 ... 8611 8612 8613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2136 2137 2138] TEST: [2139 2140 2141 ... 4274 4275 4276]\n",
            "TRAIN: [   0    1    2 ... 4274 4275 4276] TEST: [4277 4278 4279 ... 6412 6413 6414]\n",
            "TRAIN: [   0    1    2 ... 6412 6413 6414] TEST: [6415 6416 6417 ... 8550 8551 8552]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2119 2120 2121] TEST: [2122 2123 2124 ... 4239 4240 4241]\n",
            "TRAIN: [   0    1    2 ... 4239 4240 4241] TEST: [4242 4243 4244 ... 6359 6360 6361]\n",
            "TRAIN: [   0    1    2 ... 6359 6360 6361] TEST: [6362 6363 6364 ... 8479 8480 8481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2098 2099 2100] TEST: [2101 2102 2103 ... 4198 4199 4200]\n",
            "TRAIN: [   0    1    2 ... 4198 4199 4200] TEST: [4201 4202 4203 ... 6298 6299 6300]\n",
            "TRAIN: [   0    1    2 ... 6298 6299 6300] TEST: [6301 6302 6303 ... 8398 8399 8400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2076 2077 2078] TEST: [2079 2080 2081 ... 4153 4154 4155]\n",
            "TRAIN: [   0    1    2 ... 4153 4154 4155] TEST: [4156 4157 4158 ... 6230 6231 6232]\n",
            "TRAIN: [   0    1    2 ... 6230 6231 6232] TEST: [6233 6234 6235 ... 8307 8308 8309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2050 2051 2052] TEST: [2053 2054 2055 ... 4102 4103 4104]\n",
            "TRAIN: [   0    1    2 ... 4102 4103 4104] TEST: [4105 4106 4107 ... 6154 6155 6156]\n",
            "TRAIN: [   0    1    2 ... 6154 6155 6156] TEST: [6157 6158 6159 ... 8206 8207 8208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2023 2024 2025] TEST: [2026 2027 2028 ... 4047 4048 4049]\n",
            "TRAIN: [   0    1    2 ... 4047 4048 4049] TEST: [4050 4051 4052 ... 6071 6072 6073]\n",
            "TRAIN: [   0    1    2 ... 6071 6072 6073] TEST: [6074 6075 6076 ... 8095 8096 8097]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1992 1993 1994] TEST: [1995 1996 1997 ... 3986 3987 3988]\n",
            "TRAIN: [   0    1    2 ... 3986 3987 3988] TEST: [3989 3990 3991 ... 5980 5981 5982]\n",
            "TRAIN: [   0    1    2 ... 5980 5981 5982] TEST: [5983 5984 5985 ... 7974 7975 7976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1960 1961 1962] TEST: [1963 1964 1965 ... 3921 3922 3923]\n",
            "TRAIN: [   0    1    2 ... 3921 3922 3923] TEST: [3924 3925 3926 ... 5882 5883 5884]\n",
            "TRAIN: [   0    1    2 ... 5882 5883 5884] TEST: [5885 5886 5887 ... 7843 7844 7845]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1924 1925 1926] TEST: [1927 1928 1929 ... 3850 3851 3852]\n",
            "TRAIN: [   0    1    2 ... 3850 3851 3852] TEST: [3853 3854 3855 ... 5776 5777 5778]\n",
            "TRAIN: [   0    1    2 ... 5776 5777 5778] TEST: [5779 5780 5781 ... 7702 7703 7704]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1887 1888 1889] TEST: [1890 1891 1892 ... 3775 3776 3777]\n",
            "TRAIN: [   0    1    2 ... 3775 3776 3777] TEST: [3778 3779 3780 ... 5663 5664 5665]\n",
            "TRAIN: [   0    1    2 ... 5663 5664 5665] TEST: [5666 5667 5668 ... 7551 7552 7553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1846 1847 1848] TEST: [1849 1850 1851 ... 3694 3695 3696]\n",
            "TRAIN: [   0    1    2 ... 3694 3695 3696] TEST: [3697 3698 3699 ... 5542 5543 5544]\n",
            "TRAIN: [   0    1    2 ... 5542 5543 5544] TEST: [5545 5546 5547 ... 7390 7391 7392]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1804 1805 1806] TEST: [1807 1808 1809 ... 3609 3610 3611]\n",
            "TRAIN: [   0    1    2 ... 3609 3610 3611] TEST: [3612 3613 3614 ... 5414 5415 5416]\n",
            "TRAIN: [   0    1    2 ... 5414 5415 5416] TEST: [5417 5418 5419 ... 7219 7220 7221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1758 1759 1760] TEST: [1761 1762 1763 ... 3518 3519 3520]\n",
            "TRAIN: [   0    1    2 ... 3518 3519 3520] TEST: [3521 3522 3523 ... 5278 5279 5280]\n",
            "TRAIN: [   0    1    2 ... 5278 5279 5280] TEST: [5281 5282 5283 ... 7038 7039 7040]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1711 1712 1713] TEST: [1714 1715 1716 ... 3423 3424 3425]\n",
            "TRAIN: [   0    1    2 ... 3423 3424 3425] TEST: [3426 3427 3428 ... 5135 5136 5137]\n",
            "TRAIN: [   0    1    2 ... 5135 5136 5137] TEST: [5138 5139 5140 ... 6847 6848 6849]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1660 1661 1662] TEST: [1663 1664 1665 ... 3322 3323 3324]\n",
            "TRAIN: [   0    1    2 ... 3322 3323 3324] TEST: [3325 3326 3327 ... 4984 4985 4986]\n",
            "TRAIN: [   0    1    2 ... 4984 4985 4986] TEST: [4987 4988 4989 ... 6646 6647 6648]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moving_average_size 180\n",
            "MAPE CO      0.372960\n",
            "NO2     0.563542\n",
            "NO      1.867616\n",
            "PM10    0.582109\n",
            "dtype: float64\n",
            "\n",
            "SMAPE CO      0.516176\n",
            "NO2     0.651327\n",
            "NO      1.181406\n",
            "PM10    0.668502\n",
            "dtype: float64\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find optimal max_lag\n",
        "\n",
        "best_mape = 2\n",
        "best_smape = 2\n",
        "\n",
        "for max_lag in np.arange(0, 360, 10):\n",
        "  data_clear = make_features(data_clear, max_lag, 80)\n",
        "  \n",
        "  data_clear = data_clear.dropna()\n",
        "\n",
        "  target = data_clear[['CO', 'NO2', 'NO', 'PM10']]\n",
        "  features = data_clear.drop(['CO','NO2', 'NO', 'PM10'], axis=1)\n",
        "\n",
        "  model = linear_model.Lasso(alpha=0.2, normalize=True)\n",
        "\n",
        "  def smape(A, F):\n",
        "    return 1/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
        "\n",
        "  def mape(A, F):\n",
        "    return (abs((A-F) / A)).median()\n",
        "\n",
        "  # do CV\n",
        "  tscv = TimeSeriesSplit(n_splits=3)\n",
        "  data_clear_drop_index = data_clear.reset_index().drop('data_time', axis=1)\n",
        "  features_drop = features.reset_index().drop('data_time', axis=1)\n",
        "  target_drop = target.reset_index().drop('data_time', axis=1)\n",
        "  scores_mape = []\n",
        "  scores_smape = []\n",
        "\n",
        "\n",
        "  for train_index, test_index in tscv.split(data_clear_drop_index):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = features_drop.loc[train_index], features_drop.loc[test_index]\n",
        "    y_train, y_test = target_drop.loc[train_index], target_drop.loc[test_index]\n",
        "\n",
        "    model = model.fit(X_train, y_train)\n",
        "    predictions = pd.DataFrame(model.predict(X_test), columns = ['CO', 'NO2', 'NO', 'PM10'], index = y_test.index)\n",
        "    \n",
        "\n",
        "    score_mape = mape(y_test, predictions)\n",
        "    score_smape = smape(y_test, predictions)\n",
        "    scores_mape.append(score_mape)\n",
        "    scores_smape.append(score_smape)\n",
        "\n",
        "  final_score_mape = pd.DataFrame(scores_mape).mean(axis=0)\n",
        "  final_score_smape = pd.DataFrame(scores_smape).mean(axis=0)\n",
        "\n",
        "  if final_score_mape[1] < best_mape:\n",
        "    best_mape = final_score_mape[1]\n",
        "    best_max_lag = max_lag\n",
        "\n",
        "  if final_score_smape[1] < best_smape:\n",
        "    best_smape = final_score_smape[1]  \n",
        "\n",
        "print('max_lag', best_max_lag)\n",
        "print('MAPE', final_score_mape)\n",
        "print()\n",
        "print('SMAPE', final_score_smape)\n",
        "print('------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7PJzNFbmDnF",
        "outputId": "15e239b8-9ad6-483e-dd36-8e6dead05e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2172 2173 2174] TEST: [2175 2176 2177 ... 4345 4346 4347]\n",
            "TRAIN: [   0    1    2 ... 4345 4346 4347] TEST: [4348 4349 4350 ... 6518 6519 6520]\n",
            "TRAIN: [   0    1    2 ... 6518 6519 6520] TEST: [6521 6522 6523 ... 8691 8692 8693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2150 2151 2152] TEST: [2153 2154 2155 ... 4300 4301 4302]\n",
            "TRAIN: [   0    1    2 ... 4300 4301 4302] TEST: [4303 4304 4305 ... 6450 6451 6452]\n",
            "TRAIN: [   0    1    2 ... 6450 6451 6452] TEST: [6453 6454 6455 ... 8600 8601 8602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2125 2126 2127] TEST: [2128 2129 2130 ... 4253 4254 4255]\n",
            "TRAIN: [   0    1    2 ... 4253 4254 4255] TEST: [4256 4257 4258 ... 6381 6382 6383]\n",
            "TRAIN: [   0    1    2 ... 6381 6382 6383] TEST: [6384 6385 6386 ... 8509 8510 8511]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2103 2104 2105] TEST: [2106 2107 2108 ... 4208 4209 4210]\n",
            "TRAIN: [   0    1    2 ... 4208 4209 4210] TEST: [4211 4212 4213 ... 6313 6314 6315]\n",
            "TRAIN: [   0    1    2 ... 6313 6314 6315] TEST: [6316 6317 6318 ... 8418 8419 8420]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2081 2082 2083] TEST: [2084 2085 2086 ... 4163 4164 4165]\n",
            "TRAIN: [   0    1    2 ... 4163 4164 4165] TEST: [4166 4167 4168 ... 6245 6246 6247]\n",
            "TRAIN: [   0    1    2 ... 6245 6246 6247] TEST: [6248 6249 6250 ... 8327 8328 8329]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2059 2060 2061] TEST: [2062 2063 2064 ... 4118 4119 4120]\n",
            "TRAIN: [   0    1    2 ... 4118 4119 4120] TEST: [4121 4122 4123 ... 6177 6178 6179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 6177 6178 6179] TEST: [6180 6181 6182 ... 8236 8237 8238]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2034 2035 2036] TEST: [2037 2038 2039 ... 4071 4072 4073]\n",
            "TRAIN: [   0    1    2 ... 4071 4072 4073] TEST: [4074 4075 4076 ... 6108 6109 6110]\n",
            "TRAIN: [   0    1    2 ... 6108 6109 6110] TEST: [6111 6112 6113 ... 8145 8146 8147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2012 2013 2014] TEST: [2015 2016 2017 ... 4026 4027 4028]\n",
            "TRAIN: [   0    1    2 ... 4026 4027 4028] TEST: [4029 4030 4031 ... 6040 6041 6042]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 6040 6041 6042] TEST: [6043 6044 6045 ... 8054 8055 8056]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1990 1991 1992] TEST: [1993 1994 1995 ... 3981 3982 3983]\n",
            "TRAIN: [   0    1    2 ... 3981 3982 3983] TEST: [3984 3985 3986 ... 5972 5973 5974]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5972 5973 5974] TEST: [5975 5976 5977 ... 7963 7964 7965]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1964 1965 1966] TEST: [1967 1968 1969 ... 3930 3931 3932]\n",
            "TRAIN: [   0    1    2 ... 3930 3931 3932] TEST: [3933 3934 3935 ... 5896 5897 5898]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5896 5897 5898] TEST: [5899 5900 5901 ... 7862 7863 7864]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1937 1938 1939] TEST: [1940 1941 1942 ... 3875 3876 3877]\n",
            "TRAIN: [   0    1    2 ... 3875 3876 3877] TEST: [3878 3879 3880 ... 5813 5814 5815]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5813 5814 5815] TEST: [5816 5817 5818 ... 7751 7752 7753]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1906 1907 1908] TEST: [1909 1910 1911 ... 3814 3815 3816]\n",
            "TRAIN: [   0    1    2 ... 3814 3815 3816] TEST: [3817 3818 3819 ... 5722 5723 5724]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5722 5723 5724] TEST: [5725 5726 5727 ... 7630 7631 7632]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1874 1875 1876] TEST: [1877 1878 1879 ... 3749 3750 3751]\n",
            "TRAIN: [   0    1    2 ... 3749 3750 3751] TEST: [3752 3753 3754 ... 5624 5625 5626]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5624 5625 5626] TEST: [5627 5628 5629 ... 7499 7500 7501]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1838 1839 1840] TEST: [1841 1842 1843 ... 3678 3679 3680]\n",
            "TRAIN: [   0    1    2 ... 3678 3679 3680] TEST: [3681 3682 3683 ... 5518 5519 5520]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5518 5519 5520] TEST: [5521 5522 5523 ... 7358 7359 7360]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1801 1802 1803] TEST: [1804 1805 1806 ... 3603 3604 3605]\n",
            "TRAIN: [   0    1    2 ... 3603 3604 3605] TEST: [3606 3607 3608 ... 5405 5406 5407]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5405 5406 5407] TEST: [5408 5409 5410 ... 7207 7208 7209]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1760 1761 1762] TEST: [1763 1764 1765 ... 3522 3523 3524]\n",
            "TRAIN: [   0    1    2 ... 3522 3523 3524] TEST: [3525 3526 3527 ... 5284 5285 5286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5284 5285 5286] TEST: [5287 5288 5289 ... 7046 7047 7048]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1718 1719 1720] TEST: [1721 1722 1723 ... 3437 3438 3439]\n",
            "TRAIN: [   0    1    2 ... 3437 3438 3439] TEST: [3440 3441 3442 ... 5156 5157 5158]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5156 5157 5158] TEST: [5159 5160 5161 ... 6875 6876 6877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1672 1673 1674] TEST: [1675 1676 1677 ... 3346 3347 3348]\n",
            "TRAIN: [   0    1    2 ... 3346 3347 3348] TEST: [3349 3350 3351 ... 5020 5021 5022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 5020 5021 5022] TEST: [5023 5024 5025 ... 6694 6695 6696]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1625 1626 1627] TEST: [1628 1629 1630 ... 3251 3252 3253]\n",
            "TRAIN: [   0    1    2 ... 3251 3252 3253] TEST: [3254 3255 3256 ... 4877 4878 4879]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4877 4878 4879] TEST: [4880 4881 4882 ... 6503 6504 6505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1574 1575 1576] TEST: [1577 1578 1579 ... 3150 3151 3152]\n",
            "TRAIN: [   0    1    2 ... 3150 3151 3152] TEST: [3153 3154 3155 ... 4726 4727 4728]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4726 4727 4728] TEST: [4729 4730 4731 ... 6302 6303 6304]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1522 1523 1524] TEST: [1525 1526 1527 ... 3045 3046 3047]\n",
            "TRAIN: [   0    1    2 ... 3045 3046 3047] TEST: [3048 3049 3050 ... 4568 4569 4570]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4568 4569 4570] TEST: [4571 4572 4573 ... 6091 6092 6093]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1466 1467 1468] TEST: [1469 1470 1471 ... 2934 2935 2936]\n",
            "TRAIN: [   0    1    2 ... 2934 2935 2936] TEST: [2937 2938 2939 ... 4402 4403 4404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4402 4403 4404] TEST: [4405 4406 4407 ... 5870 5871 5872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1409 1410 1411] TEST: [1412 1413 1414 ... 2819 2820 2821]\n",
            "TRAIN: [   0    1    2 ... 2819 2820 2821] TEST: [2822 2823 2824 ... 4229 4230 4231]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4229 4230 4231] TEST: [4232 4233 4234 ... 5639 5640 5641]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1348 1349 1350] TEST: [1351 1352 1353 ... 2698 2699 2700]\n",
            "TRAIN: [   0    1    2 ... 2698 2699 2700] TEST: [2701 2702 2703 ... 4048 4049 4050]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 4048 4049 4050] TEST: [4051 4052 4053 ... 5398 5399 5400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1286 1287 1288] TEST: [1289 1290 1291 ... 2573 2574 2575]\n",
            "TRAIN: [   0    1    2 ... 2573 2574 2575] TEST: [2576 2577 2578 ... 3860 3861 3862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 3860 3861 3862] TEST: [3863 3864 3865 ... 5147 5148 5149]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1220 1221 1222] TEST: [1223 1224 1225 ... 2442 2443 2444]\n",
            "TRAIN: [   0    1    2 ... 2442 2443 2444] TEST: [2445 2446 2447 ... 3664 3665 3666]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 3664 3665 3666] TEST: [3667 3668 3669 ... 4886 4887 4888]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1153 1154 1155] TEST: [1156 1157 1158 ... 2307 2308 2309]\n",
            "TRAIN: [   0    1    2 ... 2307 2308 2309] TEST: [2310 2311 2312 ... 3461 3462 3463]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 3461 3462 3463] TEST: [3464 3465 3466 ... 4615 4616 4617]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1082 1083 1084] TEST: [1085 1086 1087 ... 2166 2167 2168]\n",
            "TRAIN: [   0    1    2 ... 2166 2167 2168] TEST: [2169 2170 2171 ... 3250 3251 3252]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 3250 3251 3252] TEST: [3253 3254 3255 ... 4334 4335 4336]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1010 1011 1012] TEST: [1013 1014 1015 ... 2021 2022 2023]\n",
            "TRAIN: [   0    1    2 ... 2021 2022 2023] TEST: [2024 2025 2026 ... 3032 3033 3034]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 3032 3033 3034] TEST: [3035 3036 3037 ... 4043 4044 4045]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
            " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
            " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
            " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
            " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
            " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
            " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
            " 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n",
            " 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899\n",
            " 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917\n",
            " 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935\n",
            " 936] TEST: [ 937  938  939  940  941  942  943  944  945  946  947  948  949  950\n",
            "  951  952  953  954  955  956  957  958  959  960  961  962  963  964\n",
            "  965  966  967  968  969  970  971  972  973  974  975  976  977  978\n",
            "  979  980  981  982  983  984  985  986  987  988  989  990  991  992\n",
            "  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006\n",
            " 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020\n",
            " 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034\n",
            " 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048\n",
            " 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062\n",
            " 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076\n",
            " 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090\n",
            " 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104\n",
            " 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118\n",
            " 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132\n",
            " 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146\n",
            " 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160\n",
            " 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174\n",
            " 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188\n",
            " 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202\n",
            " 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216\n",
            " 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230\n",
            " 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244\n",
            " 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258\n",
            " 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272\n",
            " 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286\n",
            " 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300\n",
            " 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314\n",
            " 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328\n",
            " 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342\n",
            " 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356\n",
            " 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370\n",
            " 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384\n",
            " 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398\n",
            " 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412\n",
            " 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426\n",
            " 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440\n",
            " 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454\n",
            " 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468\n",
            " 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482\n",
            " 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496\n",
            " 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510\n",
            " 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524\n",
            " 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538\n",
            " 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552\n",
            " 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566\n",
            " 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580\n",
            " 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594\n",
            " 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608\n",
            " 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622\n",
            " 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636\n",
            " 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650\n",
            " 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664\n",
            " 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678\n",
            " 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692\n",
            " 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706\n",
            " 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720\n",
            " 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734\n",
            " 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748\n",
            " 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762\n",
            " 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776\n",
            " 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790\n",
            " 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804\n",
            " 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818\n",
            " 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832\n",
            " 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846\n",
            " 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860\n",
            " 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872]\n",
            "TRAIN: [   0    1    2 ... 1870 1871 1872] TEST: [1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886\n",
            " 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900\n",
            " 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914\n",
            " 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928\n",
            " 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942\n",
            " 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956\n",
            " 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
            " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
            " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
            " 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012\n",
            " 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026\n",
            " 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040\n",
            " 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054\n",
            " 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068\n",
            " 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082\n",
            " 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096\n",
            " 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110\n",
            " 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124\n",
            " 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138\n",
            " 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152\n",
            " 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166\n",
            " 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180\n",
            " 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194\n",
            " 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208\n",
            " 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222\n",
            " 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236\n",
            " 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250\n",
            " 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264\n",
            " 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278\n",
            " 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292\n",
            " 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306\n",
            " 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320\n",
            " 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334\n",
            " 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348\n",
            " 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362\n",
            " 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376\n",
            " 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390\n",
            " 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404\n",
            " 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418\n",
            " 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432\n",
            " 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446\n",
            " 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460\n",
            " 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474\n",
            " 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488\n",
            " 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502\n",
            " 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516\n",
            " 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530\n",
            " 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544\n",
            " 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558\n",
            " 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572\n",
            " 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586\n",
            " 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600\n",
            " 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614\n",
            " 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628\n",
            " 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642\n",
            " 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656\n",
            " 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670\n",
            " 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684\n",
            " 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698\n",
            " 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712\n",
            " 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726\n",
            " 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740\n",
            " 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754\n",
            " 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768\n",
            " 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782\n",
            " 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796\n",
            " 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2806 2807 2808] TEST: [2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822\n",
            " 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836\n",
            " 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850\n",
            " 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864\n",
            " 2865 2866 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878\n",
            " 2879 2880 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892\n",
            " 2893 2894 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906\n",
            " 2907 2908 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920\n",
            " 2921 2922 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934\n",
            " 2935 2936 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948\n",
            " 2949 2950 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962\n",
            " 2963 2964 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976\n",
            " 2977 2978 2979 2980 2981 2982 2983 2984 2985 2986 2987 2988 2989 2990\n",
            " 2991 2992 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004\n",
            " 3005 3006 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018\n",
            " 3019 3020 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032\n",
            " 3033 3034 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046\n",
            " 3047 3048 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060\n",
            " 3061 3062 3063 3064 3065 3066 3067 3068 3069 3070 3071 3072 3073 3074\n",
            " 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088\n",
            " 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102\n",
            " 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114 3115 3116\n",
            " 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128 3129 3130\n",
            " 3131 3132 3133 3134 3135 3136 3137 3138 3139 3140 3141 3142 3143 3144\n",
            " 3145 3146 3147 3148 3149 3150 3151 3152 3153 3154 3155 3156 3157 3158\n",
            " 3159 3160 3161 3162 3163 3164 3165 3166 3167 3168 3169 3170 3171 3172\n",
            " 3173 3174 3175 3176 3177 3178 3179 3180 3181 3182 3183 3184 3185 3186\n",
            " 3187 3188 3189 3190 3191 3192 3193 3194 3195 3196 3197 3198 3199 3200\n",
            " 3201 3202 3203 3204 3205 3206 3207 3208 3209 3210 3211 3212 3213 3214\n",
            " 3215 3216 3217 3218 3219 3220 3221 3222 3223 3224 3225 3226 3227 3228\n",
            " 3229 3230 3231 3232 3233 3234 3235 3236 3237 3238 3239 3240 3241 3242\n",
            " 3243 3244 3245 3246 3247 3248 3249 3250 3251 3252 3253 3254 3255 3256\n",
            " 3257 3258 3259 3260 3261 3262 3263 3264 3265 3266 3267 3268 3269 3270\n",
            " 3271 3272 3273 3274 3275 3276 3277 3278 3279 3280 3281 3282 3283 3284\n",
            " 3285 3286 3287 3288 3289 3290 3291 3292 3293 3294 3295 3296 3297 3298\n",
            " 3299 3300 3301 3302 3303 3304 3305 3306 3307 3308 3309 3310 3311 3312\n",
            " 3313 3314 3315 3316 3317 3318 3319 3320 3321 3322 3323 3324 3325 3326\n",
            " 3327 3328 3329 3330 3331 3332 3333 3334 3335 3336 3337 3338 3339 3340\n",
            " 3341 3342 3343 3344 3345 3346 3347 3348 3349 3350 3351 3352 3353 3354\n",
            " 3355 3356 3357 3358 3359 3360 3361 3362 3363 3364 3365 3366 3367 3368\n",
            " 3369 3370 3371 3372 3373 3374 3375 3376 3377 3378 3379 3380 3381 3382\n",
            " 3383 3384 3385 3386 3387 3388 3389 3390 3391 3392 3393 3394 3395 3396\n",
            " 3397 3398 3399 3400 3401 3402 3403 3404 3405 3406 3407 3408 3409 3410\n",
            " 3411 3412 3413 3414 3415 3416 3417 3418 3419 3420 3421 3422 3423 3424\n",
            " 3425 3426 3427 3428 3429 3430 3431 3432 3433 3434 3435 3436 3437 3438\n",
            " 3439 3440 3441 3442 3443 3444 3445 3446 3447 3448 3449 3450 3451 3452\n",
            " 3453 3454 3455 3456 3457 3458 3459 3460 3461 3462 3463 3464 3465 3466\n",
            " 3467 3468 3469 3470 3471 3472 3473 3474 3475 3476 3477 3478 3479 3480\n",
            " 3481 3482 3483 3484 3485 3486 3487 3488 3489 3490 3491 3492 3493 3494\n",
            " 3495 3496 3497 3498 3499 3500 3501 3502 3503 3504 3505 3506 3507 3508\n",
            " 3509 3510 3511 3512 3513 3514 3515 3516 3517 3518 3519 3520 3521 3522\n",
            " 3523 3524 3525 3526 3527 3528 3529 3530 3531 3532 3533 3534 3535 3536\n",
            " 3537 3538 3539 3540 3541 3542 3543 3544 3545 3546 3547 3548 3549 3550\n",
            " 3551 3552 3553 3554 3555 3556 3557 3558 3559 3560 3561 3562 3563 3564\n",
            " 3565 3566 3567 3568 3569 3570 3571 3572 3573 3574 3575 3576 3577 3578\n",
            " 3579 3580 3581 3582 3583 3584 3585 3586 3587 3588 3589 3590 3591 3592\n",
            " 3593 3594 3595 3596 3597 3598 3599 3600 3601 3602 3603 3604 3605 3606\n",
            " 3607 3608 3609 3610 3611 3612 3613 3614 3615 3616 3617 3618 3619 3620\n",
            " 3621 3622 3623 3624 3625 3626 3627 3628 3629 3630 3631 3632 3633 3634\n",
            " 3635 3636 3637 3638 3639 3640 3641 3642 3643 3644 3645 3646 3647 3648\n",
            " 3649 3650 3651 3652 3653 3654 3655 3656 3657 3658 3659 3660 3661 3662\n",
            " 3663 3664 3665 3666 3667 3668 3669 3670 3671 3672 3673 3674 3675 3676\n",
            " 3677 3678 3679 3680 3681 3682 3683 3684 3685 3686 3687 3688 3689 3690\n",
            " 3691 3692 3693 3694 3695 3696 3697 3698 3699 3700 3701 3702 3703 3704\n",
            " 3705 3706 3707 3708 3709 3710 3711 3712 3713 3714 3715 3716 3717 3718\n",
            " 3719 3720 3721 3722 3723 3724 3725 3726 3727 3728 3729 3730 3731 3732\n",
            " 3733 3734 3735 3736 3737 3738 3739 3740 3741 3742 3743 3744]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
            " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
            " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
            " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
            " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
            " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
            " 846 847 848 849 850 851 852 853 854 855 856 857 858 859] TEST: [ 860  861  862  863  864  865  866  867  868  869  870  871  872  873\n",
            "  874  875  876  877  878  879  880  881  882  883  884  885  886  887\n",
            "  888  889  890  891  892  893  894  895  896  897  898  899  900  901\n",
            "  902  903  904  905  906  907  908  909  910  911  912  913  914  915\n",
            "  916  917  918  919  920  921  922  923  924  925  926  927  928  929\n",
            "  930  931  932  933  934  935  936  937  938  939  940  941  942  943\n",
            "  944  945  946  947  948  949  950  951  952  953  954  955  956  957\n",
            "  958  959  960  961  962  963  964  965  966  967  968  969  970  971\n",
            "  972  973  974  975  976  977  978  979  980  981  982  983  984  985\n",
            "  986  987  988  989  990  991  992  993  994  995  996  997  998  999\n",
            " 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013\n",
            " 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027\n",
            " 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041\n",
            " 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055\n",
            " 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069\n",
            " 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083\n",
            " 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097\n",
            " 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111\n",
            " 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125\n",
            " 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139\n",
            " 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153\n",
            " 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167\n",
            " 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181\n",
            " 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195\n",
            " 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209\n",
            " 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223\n",
            " 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237\n",
            " 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251\n",
            " 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265\n",
            " 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279\n",
            " 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293\n",
            " 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307\n",
            " 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321\n",
            " 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335\n",
            " 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349\n",
            " 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363\n",
            " 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377\n",
            " 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391\n",
            " 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405\n",
            " 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419\n",
            " 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433\n",
            " 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447\n",
            " 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461\n",
            " 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475\n",
            " 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489\n",
            " 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503\n",
            " 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517\n",
            " 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531\n",
            " 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545\n",
            " 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559\n",
            " 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573\n",
            " 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587\n",
            " 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601\n",
            " 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615\n",
            " 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629\n",
            " 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643\n",
            " 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657\n",
            " 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671\n",
            " 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685\n",
            " 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699\n",
            " 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713\n",
            " 1714 1715 1716 1717]\n",
            "TRAIN: [   0    1    2 ... 1715 1716 1717] TEST: [1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731\n",
            " 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745\n",
            " 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759\n",
            " 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773\n",
            " 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787\n",
            " 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801\n",
            " 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815\n",
            " 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829\n",
            " 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843\n",
            " 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857\n",
            " 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871\n",
            " 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885\n",
            " 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899\n",
            " 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913\n",
            " 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927\n",
            " 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941\n",
            " 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955\n",
            " 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969\n",
            " 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983\n",
            " 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n",
            " 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011\n",
            " 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025\n",
            " 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039\n",
            " 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053\n",
            " 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067\n",
            " 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081\n",
            " 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095\n",
            " 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109\n",
            " 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123\n",
            " 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137\n",
            " 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151\n",
            " 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165\n",
            " 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179\n",
            " 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193\n",
            " 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207\n",
            " 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221\n",
            " 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235\n",
            " 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249\n",
            " 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263\n",
            " 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277\n",
            " 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291\n",
            " 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305\n",
            " 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319\n",
            " 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333\n",
            " 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347\n",
            " 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361\n",
            " 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375\n",
            " 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389\n",
            " 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403\n",
            " 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417\n",
            " 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431\n",
            " 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445\n",
            " 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459\n",
            " 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473\n",
            " 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487\n",
            " 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501\n",
            " 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515\n",
            " 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529\n",
            " 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543\n",
            " 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557\n",
            " 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571\n",
            " 2572 2573 2574 2575]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2573 2574 2575] TEST: [2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589\n",
            " 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603\n",
            " 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617\n",
            " 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631\n",
            " 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645\n",
            " 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659\n",
            " 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673\n",
            " 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687\n",
            " 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701\n",
            " 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715\n",
            " 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729\n",
            " 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743\n",
            " 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757\n",
            " 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771\n",
            " 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785\n",
            " 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799\n",
            " 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813\n",
            " 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827\n",
            " 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841\n",
            " 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855\n",
            " 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866 2867 2868 2869\n",
            " 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880 2881 2882 2883\n",
            " 2884 2885 2886 2887 2888 2889 2890 2891 2892 2893 2894 2895 2896 2897\n",
            " 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908 2909 2910 2911\n",
            " 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922 2923 2924 2925\n",
            " 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936 2937 2938 2939\n",
            " 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950 2951 2952 2953\n",
            " 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964 2965 2966 2967\n",
            " 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978 2979 2980 2981\n",
            " 2982 2983 2984 2985 2986 2987 2988 2989 2990 2991 2992 2993 2994 2995\n",
            " 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006 3007 3008 3009\n",
            " 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020 3021 3022 3023\n",
            " 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034 3035 3036 3037\n",
            " 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048 3049 3050 3051\n",
            " 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062 3063 3064 3065\n",
            " 3066 3067 3068 3069 3070 3071 3072 3073 3074 3075 3076 3077 3078 3079\n",
            " 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090 3091 3092 3093\n",
            " 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104 3105 3106 3107\n",
            " 3108 3109 3110 3111 3112 3113 3114 3115 3116 3117 3118 3119 3120 3121\n",
            " 3122 3123 3124 3125 3126 3127 3128 3129 3130 3131 3132 3133 3134 3135\n",
            " 3136 3137 3138 3139 3140 3141 3142 3143 3144 3145 3146 3147 3148 3149\n",
            " 3150 3151 3152 3153 3154 3155 3156 3157 3158 3159 3160 3161 3162 3163\n",
            " 3164 3165 3166 3167 3168 3169 3170 3171 3172 3173 3174 3175 3176 3177\n",
            " 3178 3179 3180 3181 3182 3183 3184 3185 3186 3187 3188 3189 3190 3191\n",
            " 3192 3193 3194 3195 3196 3197 3198 3199 3200 3201 3202 3203 3204 3205\n",
            " 3206 3207 3208 3209 3210 3211 3212 3213 3214 3215 3216 3217 3218 3219\n",
            " 3220 3221 3222 3223 3224 3225 3226 3227 3228 3229 3230 3231 3232 3233\n",
            " 3234 3235 3236 3237 3238 3239 3240 3241 3242 3243 3244 3245 3246 3247\n",
            " 3248 3249 3250 3251 3252 3253 3254 3255 3256 3257 3258 3259 3260 3261\n",
            " 3262 3263 3264 3265 3266 3267 3268 3269 3270 3271 3272 3273 3274 3275\n",
            " 3276 3277 3278 3279 3280 3281 3282 3283 3284 3285 3286 3287 3288 3289\n",
            " 3290 3291 3292 3293 3294 3295 3296 3297 3298 3299 3300 3301 3302 3303\n",
            " 3304 3305 3306 3307 3308 3309 3310 3311 3312 3313 3314 3315 3316 3317\n",
            " 3318 3319 3320 3321 3322 3323 3324 3325 3326 3327 3328 3329 3330 3331\n",
            " 3332 3333 3334 3335 3336 3337 3338 3339 3340 3341 3342 3343 3344 3345\n",
            " 3346 3347 3348 3349 3350 3351 3352 3353 3354 3355 3356 3357 3358 3359\n",
            " 3360 3361 3362 3363 3364 3365 3366 3367 3368 3369 3370 3371 3372 3373\n",
            " 3374 3375 3376 3377 3378 3379 3380 3381 3382 3383 3384 3385 3386 3387\n",
            " 3388 3389 3390 3391 3392 3393 3394 3395 3396 3397 3398 3399 3400 3401\n",
            " 3402 3403 3404 3405 3406 3407 3408 3409 3410 3411 3412 3413 3414 3415\n",
            " 3416 3417 3418 3419 3420 3421 3422 3423 3424 3425 3426 3427 3428 3429\n",
            " 3430 3431 3432 3433]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
            " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
            " 774 775 776 777 778] TEST: [ 779  780  781  782  783  784  785  786  787  788  789  790  791  792\n",
            "  793  794  795  796  797  798  799  800  801  802  803  804  805  806\n",
            "  807  808  809  810  811  812  813  814  815  816  817  818  819  820\n",
            "  821  822  823  824  825  826  827  828  829  830  831  832  833  834\n",
            "  835  836  837  838  839  840  841  842  843  844  845  846  847  848\n",
            "  849  850  851  852  853  854  855  856  857  858  859  860  861  862\n",
            "  863  864  865  866  867  868  869  870  871  872  873  874  875  876\n",
            "  877  878  879  880  881  882  883  884  885  886  887  888  889  890\n",
            "  891  892  893  894  895  896  897  898  899  900  901  902  903  904\n",
            "  905  906  907  908  909  910  911  912  913  914  915  916  917  918\n",
            "  919  920  921  922  923  924  925  926  927  928  929  930  931  932\n",
            "  933  934  935  936  937  938  939  940  941  942  943  944  945  946\n",
            "  947  948  949  950  951  952  953  954  955  956  957  958  959  960\n",
            "  961  962  963  964  965  966  967  968  969  970  971  972  973  974\n",
            "  975  976  977  978  979  980  981  982  983  984  985  986  987  988\n",
            "  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002\n",
            " 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016\n",
            " 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030\n",
            " 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044\n",
            " 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058\n",
            " 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072\n",
            " 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086\n",
            " 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100\n",
            " 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114\n",
            " 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128\n",
            " 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142\n",
            " 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156\n",
            " 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170\n",
            " 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184\n",
            " 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198\n",
            " 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212\n",
            " 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226\n",
            " 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240\n",
            " 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254\n",
            " 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268\n",
            " 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282\n",
            " 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296\n",
            " 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310\n",
            " 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324\n",
            " 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338\n",
            " 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352\n",
            " 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366\n",
            " 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380\n",
            " 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394\n",
            " 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408\n",
            " 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422\n",
            " 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436\n",
            " 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450\n",
            " 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464\n",
            " 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478\n",
            " 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492\n",
            " 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506\n",
            " 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520\n",
            " 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534\n",
            " 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548\n",
            " 1549 1550 1551 1552 1553 1554 1555 1556]\n",
            "TRAIN: [   0    1    2 ... 1554 1555 1556] TEST: [1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570\n",
            " 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584\n",
            " 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598\n",
            " 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612\n",
            " 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626\n",
            " 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640\n",
            " 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654\n",
            " 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668\n",
            " 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682\n",
            " 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696\n",
            " 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710\n",
            " 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724\n",
            " 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738\n",
            " 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752\n",
            " 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766\n",
            " 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780\n",
            " 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794\n",
            " 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808\n",
            " 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822\n",
            " 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836\n",
            " 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850\n",
            " 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864\n",
            " 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878\n",
            " 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892\n",
            " 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906\n",
            " 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920\n",
            " 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934\n",
            " 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948\n",
            " 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962\n",
            " 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976\n",
            " 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990\n",
            " 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004\n",
            " 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018\n",
            " 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032\n",
            " 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046\n",
            " 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060\n",
            " 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074\n",
            " 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088\n",
            " 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102\n",
            " 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116\n",
            " 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130\n",
            " 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144\n",
            " 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158\n",
            " 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172\n",
            " 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186\n",
            " 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200\n",
            " 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214\n",
            " 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228\n",
            " 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242\n",
            " 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256\n",
            " 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270\n",
            " 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284\n",
            " 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298\n",
            " 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312\n",
            " 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326\n",
            " 2327 2328 2329 2330 2331 2332 2333 2334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2332 2333 2334] TEST: [2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348\n",
            " 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362\n",
            " 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376\n",
            " 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390\n",
            " 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404\n",
            " 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418\n",
            " 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432\n",
            " 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446\n",
            " 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460\n",
            " 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474\n",
            " 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488\n",
            " 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502\n",
            " 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516\n",
            " 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530\n",
            " 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544\n",
            " 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558\n",
            " 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572\n",
            " 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586\n",
            " 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600\n",
            " 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614\n",
            " 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628\n",
            " 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642\n",
            " 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656\n",
            " 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670\n",
            " 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684\n",
            " 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698\n",
            " 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712\n",
            " 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726\n",
            " 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740\n",
            " 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754\n",
            " 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768\n",
            " 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782\n",
            " 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796\n",
            " 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810\n",
            " 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824\n",
            " 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838\n",
            " 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852\n",
            " 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 2866\n",
            " 2867 2868 2869 2870 2871 2872 2873 2874 2875 2876 2877 2878 2879 2880\n",
            " 2881 2882 2883 2884 2885 2886 2887 2888 2889 2890 2891 2892 2893 2894\n",
            " 2895 2896 2897 2898 2899 2900 2901 2902 2903 2904 2905 2906 2907 2908\n",
            " 2909 2910 2911 2912 2913 2914 2915 2916 2917 2918 2919 2920 2921 2922\n",
            " 2923 2924 2925 2926 2927 2928 2929 2930 2931 2932 2933 2934 2935 2936\n",
            " 2937 2938 2939 2940 2941 2942 2943 2944 2945 2946 2947 2948 2949 2950\n",
            " 2951 2952 2953 2954 2955 2956 2957 2958 2959 2960 2961 2962 2963 2964\n",
            " 2965 2966 2967 2968 2969 2970 2971 2972 2973 2974 2975 2976 2977 2978\n",
            " 2979 2980 2981 2982 2983 2984 2985 2986 2987 2988 2989 2990 2991 2992\n",
            " 2993 2994 2995 2996 2997 2998 2999 3000 3001 3002 3003 3004 3005 3006\n",
            " 3007 3008 3009 3010 3011 3012 3013 3014 3015 3016 3017 3018 3019 3020\n",
            " 3021 3022 3023 3024 3025 3026 3027 3028 3029 3030 3031 3032 3033 3034\n",
            " 3035 3036 3037 3038 3039 3040 3041 3042 3043 3044 3045 3046 3047 3048\n",
            " 3049 3050 3051 3052 3053 3054 3055 3056 3057 3058 3059 3060 3061 3062\n",
            " 3063 3064 3065 3066 3067 3068 3069 3070 3071 3072 3073 3074 3075 3076\n",
            " 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086 3087 3088 3089 3090\n",
            " 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100 3101 3102 3103 3104\n",
            " 3105 3106 3107 3108 3109 3110 3111 3112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696] TEST: [ 697  698  699  700  701  702  703  704  705  706  707  708  709  710\n",
            "  711  712  713  714  715  716  717  718  719  720  721  722  723  724\n",
            "  725  726  727  728  729  730  731  732  733  734  735  736  737  738\n",
            "  739  740  741  742  743  744  745  746  747  748  749  750  751  752\n",
            "  753  754  755  756  757  758  759  760  761  762  763  764  765  766\n",
            "  767  768  769  770  771  772  773  774  775  776  777  778  779  780\n",
            "  781  782  783  784  785  786  787  788  789  790  791  792  793  794\n",
            "  795  796  797  798  799  800  801  802  803  804  805  806  807  808\n",
            "  809  810  811  812  813  814  815  816  817  818  819  820  821  822\n",
            "  823  824  825  826  827  828  829  830  831  832  833  834  835  836\n",
            "  837  838  839  840  841  842  843  844  845  846  847  848  849  850\n",
            "  851  852  853  854  855  856  857  858  859  860  861  862  863  864\n",
            "  865  866  867  868  869  870  871  872  873  874  875  876  877  878\n",
            "  879  880  881  882  883  884  885  886  887  888  889  890  891  892\n",
            "  893  894  895  896  897  898  899  900  901  902  903  904  905  906\n",
            "  907  908  909  910  911  912  913  914  915  916  917  918  919  920\n",
            "  921  922  923  924  925  926  927  928  929  930  931  932  933  934\n",
            "  935  936  937  938  939  940  941  942  943  944  945  946  947  948\n",
            "  949  950  951  952  953  954  955  956  957  958  959  960  961  962\n",
            "  963  964  965  966  967  968  969  970  971  972  973  974  975  976\n",
            "  977  978  979  980  981  982  983  984  985  986  987  988  989  990\n",
            "  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004\n",
            " 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018\n",
            " 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032\n",
            " 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046\n",
            " 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060\n",
            " 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074\n",
            " 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088\n",
            " 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102\n",
            " 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116\n",
            " 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130\n",
            " 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144\n",
            " 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158\n",
            " 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172\n",
            " 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186\n",
            " 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200\n",
            " 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214\n",
            " 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228\n",
            " 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242\n",
            " 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256\n",
            " 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270\n",
            " 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284\n",
            " 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298\n",
            " 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312\n",
            " 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326\n",
            " 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340\n",
            " 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354\n",
            " 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368\n",
            " 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382\n",
            " 1383 1384 1385 1386 1387 1388 1389 1390 1391]\n",
            "TRAIN: [   0    1    2 ... 1389 1390 1391] TEST: [1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405\n",
            " 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419\n",
            " 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433\n",
            " 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447\n",
            " 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461\n",
            " 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475\n",
            " 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489\n",
            " 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503\n",
            " 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517\n",
            " 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531\n",
            " 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545\n",
            " 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559\n",
            " 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573\n",
            " 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587\n",
            " 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601\n",
            " 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615\n",
            " 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629\n",
            " 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643\n",
            " 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657\n",
            " 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671\n",
            " 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685\n",
            " 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699\n",
            " 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713\n",
            " 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727\n",
            " 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741\n",
            " 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755\n",
            " 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769\n",
            " 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783\n",
            " 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797\n",
            " 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811\n",
            " 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825\n",
            " 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839\n",
            " 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853\n",
            " 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867\n",
            " 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881\n",
            " 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895\n",
            " 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909\n",
            " 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923\n",
            " 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937\n",
            " 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951\n",
            " 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965\n",
            " 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979\n",
            " 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993\n",
            " 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007\n",
            " 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021\n",
            " 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035\n",
            " 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049\n",
            " 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063\n",
            " 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077\n",
            " 2078 2079 2080 2081 2082 2083 2084 2085 2086]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 2084 2085 2086] TEST: [2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100\n",
            " 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114\n",
            " 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128\n",
            " 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142\n",
            " 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156\n",
            " 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170\n",
            " 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184\n",
            " 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198\n",
            " 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212\n",
            " 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226\n",
            " 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240\n",
            " 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254\n",
            " 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268\n",
            " 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282\n",
            " 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296\n",
            " 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310\n",
            " 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324\n",
            " 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338\n",
            " 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352\n",
            " 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366\n",
            " 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380\n",
            " 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394\n",
            " 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408\n",
            " 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422\n",
            " 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436\n",
            " 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450\n",
            " 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464\n",
            " 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478\n",
            " 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492\n",
            " 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506\n",
            " 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520\n",
            " 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534\n",
            " 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548\n",
            " 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562\n",
            " 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576\n",
            " 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590\n",
            " 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604\n",
            " 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618\n",
            " 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632\n",
            " 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646\n",
            " 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660\n",
            " 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674\n",
            " 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688\n",
            " 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702\n",
            " 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716\n",
            " 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730\n",
            " 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744\n",
            " 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758\n",
            " 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772\n",
            " 2773 2774 2775 2776 2777 2778 2779 2780 2781]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610] TEST: [ 611  612  613  614  615  616  617  618  619  620  621  622  623  624\n",
            "  625  626  627  628  629  630  631  632  633  634  635  636  637  638\n",
            "  639  640  641  642  643  644  645  646  647  648  649  650  651  652\n",
            "  653  654  655  656  657  658  659  660  661  662  663  664  665  666\n",
            "  667  668  669  670  671  672  673  674  675  676  677  678  679  680\n",
            "  681  682  683  684  685  686  687  688  689  690  691  692  693  694\n",
            "  695  696  697  698  699  700  701  702  703  704  705  706  707  708\n",
            "  709  710  711  712  713  714  715  716  717  718  719  720  721  722\n",
            "  723  724  725  726  727  728  729  730  731  732  733  734  735  736\n",
            "  737  738  739  740  741  742  743  744  745  746  747  748  749  750\n",
            "  751  752  753  754  755  756  757  758  759  760  761  762  763  764\n",
            "  765  766  767  768  769  770  771  772  773  774  775  776  777  778\n",
            "  779  780  781  782  783  784  785  786  787  788  789  790  791  792\n",
            "  793  794  795  796  797  798  799  800  801  802  803  804  805  806\n",
            "  807  808  809  810  811  812  813  814  815  816  817  818  819  820\n",
            "  821  822  823  824  825  826  827  828  829  830  831  832  833  834\n",
            "  835  836  837  838  839  840  841  842  843  844  845  846  847  848\n",
            "  849  850  851  852  853  854  855  856  857  858  859  860  861  862\n",
            "  863  864  865  866  867  868  869  870  871  872  873  874  875  876\n",
            "  877  878  879  880  881  882  883  884  885  886  887  888  889  890\n",
            "  891  892  893  894  895  896  897  898  899  900  901  902  903  904\n",
            "  905  906  907  908  909  910  911  912  913  914  915  916  917  918\n",
            "  919  920  921  922  923  924  925  926  927  928  929  930  931  932\n",
            "  933  934  935  936  937  938  939  940  941  942  943  944  945  946\n",
            "  947  948  949  950  951  952  953  954  955  956  957  958  959  960\n",
            "  961  962  963  964  965  966  967  968  969  970  971  972  973  974\n",
            "  975  976  977  978  979  980  981  982  983  984  985  986  987  988\n",
            "  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002\n",
            " 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016\n",
            " 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030\n",
            " 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044\n",
            " 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058\n",
            " 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072\n",
            " 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086\n",
            " 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100\n",
            " 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114\n",
            " 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128\n",
            " 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142\n",
            " 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156\n",
            " 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170\n",
            " 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184\n",
            " 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198\n",
            " 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212\n",
            " 1213 1214 1215 1216 1217 1218 1219 1220]\n",
            "TRAIN: [   0    1    2 ... 1218 1219 1220] TEST: [1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234\n",
            " 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248\n",
            " 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262\n",
            " 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276\n",
            " 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290\n",
            " 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304\n",
            " 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318\n",
            " 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332\n",
            " 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346\n",
            " 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360\n",
            " 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374\n",
            " 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388\n",
            " 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402\n",
            " 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416\n",
            " 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430\n",
            " 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444\n",
            " 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458\n",
            " 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472\n",
            " 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486\n",
            " 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500\n",
            " 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514\n",
            " 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528\n",
            " 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542\n",
            " 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556\n",
            " 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570\n",
            " 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584\n",
            " 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598\n",
            " 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612\n",
            " 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626\n",
            " 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640\n",
            " 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654\n",
            " 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668\n",
            " 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682\n",
            " 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696\n",
            " 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710\n",
            " 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724\n",
            " 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738\n",
            " 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752\n",
            " 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766\n",
            " 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780\n",
            " 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794\n",
            " 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808\n",
            " 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822\n",
            " 1823 1824 1825 1826 1827 1828 1829 1830]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1828 1829 1830] TEST: [1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844\n",
            " 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858\n",
            " 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872\n",
            " 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886\n",
            " 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900\n",
            " 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914\n",
            " 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928\n",
            " 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942\n",
            " 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956\n",
            " 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
            " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
            " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
            " 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012\n",
            " 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026\n",
            " 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040\n",
            " 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054\n",
            " 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068\n",
            " 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082\n",
            " 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096\n",
            " 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110\n",
            " 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124\n",
            " 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138\n",
            " 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152\n",
            " 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166\n",
            " 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180\n",
            " 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194\n",
            " 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208\n",
            " 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222\n",
            " 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236\n",
            " 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250\n",
            " 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264\n",
            " 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278\n",
            " 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292\n",
            " 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306\n",
            " 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320\n",
            " 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334\n",
            " 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348\n",
            " 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362\n",
            " 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376\n",
            " 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390\n",
            " 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404\n",
            " 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418\n",
            " 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432\n",
            " 2433 2434 2435 2436 2437 2438 2439 2440]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523] TEST: [ 524  525  526  527  528  529  530  531  532  533  534  535  536  537\n",
            "  538  539  540  541  542  543  544  545  546  547  548  549  550  551\n",
            "  552  553  554  555  556  557  558  559  560  561  562  563  564  565\n",
            "  566  567  568  569  570  571  572  573  574  575  576  577  578  579\n",
            "  580  581  582  583  584  585  586  587  588  589  590  591  592  593\n",
            "  594  595  596  597  598  599  600  601  602  603  604  605  606  607\n",
            "  608  609  610  611  612  613  614  615  616  617  618  619  620  621\n",
            "  622  623  624  625  626  627  628  629  630  631  632  633  634  635\n",
            "  636  637  638  639  640  641  642  643  644  645  646  647  648  649\n",
            "  650  651  652  653  654  655  656  657  658  659  660  661  662  663\n",
            "  664  665  666  667  668  669  670  671  672  673  674  675  676  677\n",
            "  678  679  680  681  682  683  684  685  686  687  688  689  690  691\n",
            "  692  693  694  695  696  697  698  699  700  701  702  703  704  705\n",
            "  706  707  708  709  710  711  712  713  714  715  716  717  718  719\n",
            "  720  721  722  723  724  725  726  727  728  729  730  731  732  733\n",
            "  734  735  736  737  738  739  740  741  742  743  744  745  746  747\n",
            "  748  749  750  751  752  753  754  755  756  757  758  759  760  761\n",
            "  762  763  764  765  766  767  768  769  770  771  772  773  774  775\n",
            "  776  777  778  779  780  781  782  783  784  785  786  787  788  789\n",
            "  790  791  792  793  794  795  796  797  798  799  800  801  802  803\n",
            "  804  805  806  807  808  809  810  811  812  813  814  815  816  817\n",
            "  818  819  820  821  822  823  824  825  826  827  828  829  830  831\n",
            "  832  833  834  835  836  837  838  839  840  841  842  843  844  845\n",
            "  846  847  848  849  850  851  852  853  854  855  856  857  858  859\n",
            "  860  861  862  863  864  865  866  867  868  869  870  871  872  873\n",
            "  874  875  876  877  878  879  880  881  882  883  884  885  886  887\n",
            "  888  889  890  891  892  893  894  895  896  897  898  899  900  901\n",
            "  902  903  904  905  906  907  908  909  910  911  912  913  914  915\n",
            "  916  917  918  919  920  921  922  923  924  925  926  927  928  929\n",
            "  930  931  932  933  934  935  936  937  938  939  940  941  942  943\n",
            "  944  945  946  947  948  949  950  951  952  953  954  955  956  957\n",
            "  958  959  960  961  962  963  964  965  966  967  968  969  970  971\n",
            "  972  973  974  975  976  977  978  979  980  981  982  983  984  985\n",
            "  986  987  988  989  990  991  992  993  994  995  996  997  998  999\n",
            " 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013\n",
            " 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027\n",
            " 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041\n",
            " 1042 1043 1044 1045]\n",
            "TRAIN: [   0    1    2 ... 1043 1044 1045] TEST: [1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059\n",
            " 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073\n",
            " 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087\n",
            " 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101\n",
            " 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115\n",
            " 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129\n",
            " 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143\n",
            " 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157\n",
            " 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171\n",
            " 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185\n",
            " 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199\n",
            " 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213\n",
            " 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227\n",
            " 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241\n",
            " 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255\n",
            " 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269\n",
            " 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283\n",
            " 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297\n",
            " 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311\n",
            " 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325\n",
            " 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339\n",
            " 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353\n",
            " 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367\n",
            " 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381\n",
            " 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395\n",
            " 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409\n",
            " 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423\n",
            " 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437\n",
            " 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451\n",
            " 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465\n",
            " 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479\n",
            " 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493\n",
            " 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507\n",
            " 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521\n",
            " 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535\n",
            " 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549\n",
            " 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563\n",
            " 1564 1565 1566 1567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1565 1566 1567] TEST: [1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581\n",
            " 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595\n",
            " 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609\n",
            " 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623\n",
            " 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637\n",
            " 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651\n",
            " 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665\n",
            " 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679\n",
            " 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693\n",
            " 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707\n",
            " 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721\n",
            " 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735\n",
            " 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749\n",
            " 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763\n",
            " 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777\n",
            " 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791\n",
            " 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805\n",
            " 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819\n",
            " 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833\n",
            " 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847\n",
            " 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861\n",
            " 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875\n",
            " 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889\n",
            " 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903\n",
            " 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917\n",
            " 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931\n",
            " 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945\n",
            " 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959\n",
            " 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973\n",
            " 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987\n",
            " 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001\n",
            " 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015\n",
            " 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029\n",
            " 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043\n",
            " 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057\n",
            " 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071\n",
            " 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085\n",
            " 2086 2087 2088 2089]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432] TEST: [433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
            " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n",
            " 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n",
            " 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
            " 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n",
            " 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
            " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
            " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
            " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
            " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612\n",
            " 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630\n",
            " 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648\n",
            " 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666\n",
            " 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684\n",
            " 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702\n",
            " 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
            " 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738\n",
            " 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
            " 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774\n",
            " 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792\n",
            " 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810\n",
            " 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828\n",
            " 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846\n",
            " 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
            " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
            " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
            " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
            " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
            " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
            " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
            " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
            " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
            " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
            " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
            " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
            " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
            " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
            " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
            " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
            " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
            " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
            " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
            " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
            " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
            " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
            " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
            " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
            " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
            " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
            " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
            " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
            " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
            " 864] TEST: [ 865  866  867  868  869  870  871  872  873  874  875  876  877  878\n",
            "  879  880  881  882  883  884  885  886  887  888  889  890  891  892\n",
            "  893  894  895  896  897  898  899  900  901  902  903  904  905  906\n",
            "  907  908  909  910  911  912  913  914  915  916  917  918  919  920\n",
            "  921  922  923  924  925  926  927  928  929  930  931  932  933  934\n",
            "  935  936  937  938  939  940  941  942  943  944  945  946  947  948\n",
            "  949  950  951  952  953  954  955  956  957  958  959  960  961  962\n",
            "  963  964  965  966  967  968  969  970  971  972  973  974  975  976\n",
            "  977  978  979  980  981  982  983  984  985  986  987  988  989  990\n",
            "  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004\n",
            " 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018\n",
            " 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032\n",
            " 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046\n",
            " 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060\n",
            " 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074\n",
            " 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088\n",
            " 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102\n",
            " 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116\n",
            " 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130\n",
            " 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144\n",
            " 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158\n",
            " 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172\n",
            " 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186\n",
            " 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200\n",
            " 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214\n",
            " 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228\n",
            " 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242\n",
            " 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256\n",
            " 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270\n",
            " 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284\n",
            " 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [   0    1    2 ... 1294 1295 1296] TEST: [1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310\n",
            " 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324\n",
            " 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338\n",
            " 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352\n",
            " 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366\n",
            " 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380\n",
            " 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394\n",
            " 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408\n",
            " 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422\n",
            " 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436\n",
            " 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450\n",
            " 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464\n",
            " 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478\n",
            " 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492\n",
            " 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506\n",
            " 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520\n",
            " 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534\n",
            " 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548\n",
            " 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562\n",
            " 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576\n",
            " 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590\n",
            " 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604\n",
            " 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618\n",
            " 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632\n",
            " 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646\n",
            " 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660\n",
            " 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674\n",
            " 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688\n",
            " 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702\n",
            " 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716\n",
            " 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728]\n",
            "max_lag 350\n",
            "MAPE CO      0.593726\n",
            "NO2     0.337413\n",
            "NO      2.407065\n",
            "PM10    0.628355\n",
            "dtype: float64\n",
            "\n",
            "SMAPE CO      0.606683\n",
            "NO2     0.486723\n",
            "NO      1.159730\n",
            "PM10    0.683238\n",
            "dtype: float64\n",
            "------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiOutputRegressor(CatBoostRegressor(iterations=150, \n",
        "                          depth=3, \n",
        "                          learning_rate=0.15, \n",
        "                          loss_function='MAPE'))"
      ],
      "metadata": {
        "id": "3iSzyYBTm1W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find optimal hyperparameters\n",
        "grid_params = [{'estimator__iterations': [50], \n",
        "                'estimator__depth': np.arange(3, 11, 2), \n",
        "                'estimator__learning_rate': [0.3, 0.15, 0.11]}] "
      ],
      "metadata": {
        "id": "UcAfUWGgnnCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "gsearch = GridSearchCV(estimator=model,\n",
        "                       cv=tscv,\n",
        "                       param_grid=grid_params,\n",
        "                       scoring='neg_mean_absolute_percentage_error') \n",
        "\n",
        "gsearch.fit(features, target)\n",
        "print(gsearch.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhIeImDxnqSI",
        "outputId": "f6107288-1f66-4615-d1b8-49bb24a06734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "1:\tlearn: 0.2009326\ttotal: 86.1ms\tremaining: 2.07s\n",
            "2:\tlearn: 0.1852204\ttotal: 130ms\tremaining: 2.04s\n",
            "3:\tlearn: 0.1722051\ttotal: 164ms\tremaining: 1.89s\n",
            "4:\tlearn: 0.1589395\ttotal: 199ms\tremaining: 1.79s\n",
            "5:\tlearn: 0.1470537\ttotal: 235ms\tremaining: 1.73s\n",
            "6:\tlearn: 0.1395181\ttotal: 271ms\tremaining: 1.66s\n",
            "7:\tlearn: 0.1313402\ttotal: 311ms\tremaining: 1.63s\n",
            "8:\tlearn: 0.1254023\ttotal: 354ms\tremaining: 1.61s\n",
            "9:\tlearn: 0.1202007\ttotal: 393ms\tremaining: 1.57s\n",
            "10:\tlearn: 0.1154662\ttotal: 429ms\tremaining: 1.52s\n",
            "11:\tlearn: 0.1109605\ttotal: 466ms\tremaining: 1.47s\n",
            "12:\tlearn: 0.1083424\ttotal: 500ms\tremaining: 1.42s\n",
            "13:\tlearn: 0.1054877\ttotal: 535ms\tremaining: 1.37s\n",
            "14:\tlearn: 0.1021734\ttotal: 583ms\tremaining: 1.36s\n",
            "15:\tlearn: 0.1002140\ttotal: 620ms\tremaining: 1.32s\n",
            "16:\tlearn: 0.0980867\ttotal: 659ms\tremaining: 1.28s\n",
            "17:\tlearn: 0.0951828\ttotal: 695ms\tremaining: 1.24s\n",
            "18:\tlearn: 0.0931481\ttotal: 730ms\tremaining: 1.19s\n",
            "19:\tlearn: 0.0918982\ttotal: 768ms\tremaining: 1.15s\n",
            "20:\tlearn: 0.0905493\ttotal: 823ms\tremaining: 1.14s\n",
            "21:\tlearn: 0.0893770\ttotal: 860ms\tremaining: 1.09s\n",
            "22:\tlearn: 0.0883793\ttotal: 894ms\tremaining: 1.05s\n",
            "23:\tlearn: 0.0868488\ttotal: 939ms\tremaining: 1.02s\n",
            "24:\tlearn: 0.0859797\ttotal: 976ms\tremaining: 976ms\n",
            "25:\tlearn: 0.0849844\ttotal: 1.01s\tremaining: 933ms\n",
            "26:\tlearn: 0.0843782\ttotal: 1.06s\tremaining: 901ms\n",
            "27:\tlearn: 0.0838075\ttotal: 1.09s\tremaining: 859ms\n",
            "28:\tlearn: 0.0828651\ttotal: 1.13s\tremaining: 816ms\n",
            "29:\tlearn: 0.0822830\ttotal: 1.16s\tremaining: 773ms\n",
            "30:\tlearn: 0.0814745\ttotal: 1.19s\tremaining: 732ms\n",
            "31:\tlearn: 0.0809743\ttotal: 1.23s\tremaining: 691ms\n",
            "32:\tlearn: 0.0803821\ttotal: 1.27s\tremaining: 656ms\n",
            "33:\tlearn: 0.0796289\ttotal: 1.31s\tremaining: 616ms\n",
            "34:\tlearn: 0.0790073\ttotal: 1.34s\tremaining: 576ms\n",
            "35:\tlearn: 0.0785376\ttotal: 1.38s\tremaining: 538ms\n",
            "36:\tlearn: 0.0772867\ttotal: 1.42s\tremaining: 498ms\n",
            "37:\tlearn: 0.0765528\ttotal: 1.45s\tremaining: 459ms\n",
            "38:\tlearn: 0.0763101\ttotal: 1.5s\tremaining: 423ms\n",
            "39:\tlearn: 0.0753898\ttotal: 1.53s\tremaining: 383ms\n",
            "40:\tlearn: 0.0750939\ttotal: 1.57s\tremaining: 344ms\n",
            "41:\tlearn: 0.0745738\ttotal: 1.6s\tremaining: 305ms\n",
            "42:\tlearn: 0.0742254\ttotal: 1.64s\tremaining: 266ms\n",
            "43:\tlearn: 0.0735128\ttotal: 1.67s\tremaining: 228ms\n",
            "44:\tlearn: 0.0730384\ttotal: 1.71s\tremaining: 190ms\n",
            "45:\tlearn: 0.0724889\ttotal: 1.75s\tremaining: 152ms\n",
            "46:\tlearn: 0.0719379\ttotal: 1.79s\tremaining: 114ms\n",
            "47:\tlearn: 0.0715740\ttotal: 1.82s\tremaining: 75.8ms\n",
            "48:\tlearn: 0.0711751\ttotal: 1.85s\tremaining: 37.8ms\n",
            "49:\tlearn: 0.0707597\ttotal: 1.89s\tremaining: 0us\n",
            "0:\tlearn: 0.0373464\ttotal: 47.6ms\tremaining: 2.33s\n",
            "1:\tlearn: 0.0338667\ttotal: 80.9ms\tremaining: 1.94s\n",
            "2:\tlearn: 0.0308307\ttotal: 114ms\tremaining: 1.79s\n",
            "3:\tlearn: 0.0284266\ttotal: 148ms\tremaining: 1.7s\n",
            "4:\tlearn: 0.0261559\ttotal: 180ms\tremaining: 1.62s\n",
            "5:\tlearn: 0.0241757\ttotal: 214ms\tremaining: 1.57s\n",
            "6:\tlearn: 0.0225423\ttotal: 250ms\tremaining: 1.53s\n",
            "7:\tlearn: 0.0210972\ttotal: 289ms\tremaining: 1.51s\n",
            "8:\tlearn: 0.0202992\ttotal: 321ms\tremaining: 1.46s\n",
            "9:\tlearn: 0.0190493\ttotal: 353ms\tremaining: 1.41s\n",
            "10:\tlearn: 0.0182980\ttotal: 385ms\tremaining: 1.36s\n",
            "11:\tlearn: 0.0175522\ttotal: 425ms\tremaining: 1.34s\n",
            "12:\tlearn: 0.0168598\ttotal: 461ms\tremaining: 1.31s\n",
            "13:\tlearn: 0.0164044\ttotal: 501ms\tremaining: 1.29s\n",
            "14:\tlearn: 0.0158202\ttotal: 537ms\tremaining: 1.25s\n",
            "15:\tlearn: 0.0154244\ttotal: 571ms\tremaining: 1.21s\n",
            "16:\tlearn: 0.0151020\ttotal: 605ms\tremaining: 1.17s\n",
            "17:\tlearn: 0.0148197\ttotal: 639ms\tremaining: 1.14s\n",
            "18:\tlearn: 0.0145162\ttotal: 683ms\tremaining: 1.11s\n",
            "19:\tlearn: 0.0141997\ttotal: 718ms\tremaining: 1.08s\n",
            "20:\tlearn: 0.0139255\ttotal: 752ms\tremaining: 1.04s\n",
            "21:\tlearn: 0.0136097\ttotal: 787ms\tremaining: 1s\n",
            "22:\tlearn: 0.0134317\ttotal: 820ms\tremaining: 962ms\n",
            "23:\tlearn: 0.0131861\ttotal: 853ms\tremaining: 924ms\n",
            "24:\tlearn: 0.0129777\ttotal: 893ms\tremaining: 893ms\n",
            "25:\tlearn: 0.0128343\ttotal: 933ms\tremaining: 861ms\n",
            "26:\tlearn: 0.0126414\ttotal: 968ms\tremaining: 825ms\n",
            "27:\tlearn: 0.0125069\ttotal: 1.01s\tremaining: 798ms\n",
            "28:\tlearn: 0.0123710\ttotal: 1.05s\tremaining: 760ms\n",
            "29:\tlearn: 0.0121459\ttotal: 1.08s\tremaining: 723ms\n",
            "30:\tlearn: 0.0120188\ttotal: 1.13s\tremaining: 691ms\n",
            "31:\tlearn: 0.0119454\ttotal: 1.16s\tremaining: 653ms\n",
            "32:\tlearn: 0.0118300\ttotal: 1.2s\tremaining: 618ms\n",
            "33:\tlearn: 0.0117663\ttotal: 1.23s\tremaining: 580ms\n",
            "34:\tlearn: 0.0117101\ttotal: 1.27s\tremaining: 543ms\n",
            "35:\tlearn: 0.0115992\ttotal: 1.31s\tremaining: 509ms\n",
            "36:\tlearn: 0.0115400\ttotal: 1.35s\tremaining: 475ms\n",
            "37:\tlearn: 0.0114047\ttotal: 1.39s\tremaining: 438ms\n",
            "38:\tlearn: 0.0113096\ttotal: 1.42s\tremaining: 401ms\n",
            "39:\tlearn: 0.0112424\ttotal: 1.45s\tremaining: 364ms\n",
            "40:\tlearn: 0.0111886\ttotal: 1.49s\tremaining: 326ms\n",
            "41:\tlearn: 0.0110609\ttotal: 1.52s\tremaining: 290ms\n",
            "42:\tlearn: 0.0109332\ttotal: 1.56s\tremaining: 255ms\n",
            "43:\tlearn: 0.0108568\ttotal: 1.6s\tremaining: 219ms\n",
            "44:\tlearn: 0.0107828\ttotal: 1.64s\tremaining: 182ms\n",
            "45:\tlearn: 0.0107170\ttotal: 1.67s\tremaining: 146ms\n",
            "46:\tlearn: 0.0106425\ttotal: 1.71s\tremaining: 109ms\n",
            "47:\tlearn: 0.0105740\ttotal: 1.74s\tremaining: 72.6ms\n",
            "48:\tlearn: 0.0105134\ttotal: 1.78s\tremaining: 36.4ms\n",
            "49:\tlearn: 0.0104840\ttotal: 1.82s\tremaining: 0us\n",
            "0:\tlearn: 0.0158967\ttotal: 50.7ms\tremaining: 2.48s\n",
            "1:\tlearn: 0.0150067\ttotal: 98.5ms\tremaining: 2.36s\n",
            "2:\tlearn: 0.0140946\ttotal: 136ms\tremaining: 2.13s\n",
            "3:\tlearn: 0.0133806\ttotal: 170ms\tremaining: 1.96s\n",
            "4:\tlearn: 0.0129928\ttotal: 204ms\tremaining: 1.83s\n",
            "5:\tlearn: 0.0122043\ttotal: 239ms\tremaining: 1.75s\n",
            "6:\tlearn: 0.0117260\ttotal: 275ms\tremaining: 1.69s\n",
            "7:\tlearn: 0.0114247\ttotal: 320ms\tremaining: 1.68s\n",
            "8:\tlearn: 0.0108594\ttotal: 356ms\tremaining: 1.62s\n",
            "9:\tlearn: 0.0103817\ttotal: 391ms\tremaining: 1.56s\n",
            "10:\tlearn: 0.0103044\ttotal: 426ms\tremaining: 1.51s\n",
            "11:\tlearn: 0.0101468\ttotal: 460ms\tremaining: 1.46s\n",
            "12:\tlearn: 0.0098259\ttotal: 494ms\tremaining: 1.41s\n",
            "13:\tlearn: 0.0094062\ttotal: 535ms\tremaining: 1.38s\n",
            "14:\tlearn: 0.0092905\ttotal: 570ms\tremaining: 1.33s\n",
            "15:\tlearn: 0.0092479\ttotal: 603ms\tremaining: 1.28s\n",
            "16:\tlearn: 0.0088073\ttotal: 636ms\tremaining: 1.24s\n",
            "17:\tlearn: 0.0086117\ttotal: 671ms\tremaining: 1.19s\n",
            "18:\tlearn: 0.0082531\ttotal: 704ms\tremaining: 1.15s\n",
            "19:\tlearn: 0.0079769\ttotal: 746ms\tremaining: 1.12s\n",
            "20:\tlearn: 0.0079490\ttotal: 784ms\tremaining: 1.08s\n",
            "21:\tlearn: 0.0079111\ttotal: 818ms\tremaining: 1.04s\n",
            "22:\tlearn: 0.0077522\ttotal: 852ms\tremaining: 1s\n",
            "23:\tlearn: 0.0076467\ttotal: 888ms\tremaining: 962ms\n",
            "24:\tlearn: 0.0075666\ttotal: 922ms\tremaining: 922ms\n",
            "25:\tlearn: 0.0075028\ttotal: 967ms\tremaining: 893ms\n",
            "26:\tlearn: 0.0074901\ttotal: 1s\tremaining: 852ms\n",
            "27:\tlearn: 0.0074679\ttotal: 1.03s\tremaining: 811ms\n",
            "28:\tlearn: 0.0073709\ttotal: 1.07s\tremaining: 773ms\n",
            "29:\tlearn: 0.0072771\ttotal: 1.11s\tremaining: 739ms\n",
            "30:\tlearn: 0.0071669\ttotal: 1.15s\tremaining: 705ms\n",
            "31:\tlearn: 0.0071183\ttotal: 1.2s\tremaining: 673ms\n",
            "32:\tlearn: 0.0070953\ttotal: 1.23s\tremaining: 635ms\n",
            "33:\tlearn: 0.0070452\ttotal: 1.27s\tremaining: 596ms\n",
            "34:\tlearn: 0.0069271\ttotal: 1.3s\tremaining: 558ms\n",
            "35:\tlearn: 0.0067822\ttotal: 1.33s\tremaining: 519ms\n",
            "36:\tlearn: 0.0067713\ttotal: 1.37s\tremaining: 481ms\n",
            "37:\tlearn: 0.0065037\ttotal: 1.41s\tremaining: 444ms\n",
            "38:\tlearn: 0.0063956\ttotal: 1.46s\tremaining: 411ms\n",
            "39:\tlearn: 0.0063420\ttotal: 1.49s\tremaining: 373ms\n",
            "40:\tlearn: 0.0063343\ttotal: 1.53s\tremaining: 335ms\n",
            "41:\tlearn: 0.0062962\ttotal: 1.56s\tremaining: 298ms\n",
            "42:\tlearn: 0.0061785\ttotal: 1.6s\tremaining: 260ms\n",
            "43:\tlearn: 0.0061373\ttotal: 1.64s\tremaining: 224ms\n",
            "44:\tlearn: 0.0061180\ttotal: 1.68s\tremaining: 186ms\n",
            "45:\tlearn: 0.0061110\ttotal: 1.71s\tremaining: 149ms\n",
            "46:\tlearn: 0.0060143\ttotal: 1.75s\tremaining: 112ms\n",
            "47:\tlearn: 0.0059337\ttotal: 1.78s\tremaining: 74.3ms\n",
            "48:\tlearn: 0.0058524\ttotal: 1.82s\tremaining: 37.1ms\n",
            "49:\tlearn: 0.0058427\ttotal: 1.86s\tremaining: 0us\n",
            "0:\tlearn: 0.0389591\ttotal: 49.9ms\tremaining: 2.45s\n",
            "1:\tlearn: 0.0350801\ttotal: 83.6ms\tremaining: 2s\n",
            "2:\tlearn: 0.0316858\ttotal: 123ms\tremaining: 1.93s\n",
            "3:\tlearn: 0.0288114\ttotal: 161ms\tremaining: 1.85s\n",
            "4:\tlearn: 0.0262494\ttotal: 206ms\tremaining: 1.85s\n",
            "5:\tlearn: 0.0240562\ttotal: 240ms\tremaining: 1.76s\n",
            "6:\tlearn: 0.0225761\ttotal: 275ms\tremaining: 1.69s\n",
            "7:\tlearn: 0.0213923\ttotal: 308ms\tremaining: 1.62s\n",
            "8:\tlearn: 0.0200693\ttotal: 353ms\tremaining: 1.61s\n",
            "9:\tlearn: 0.0188653\ttotal: 389ms\tremaining: 1.56s\n",
            "10:\tlearn: 0.0179499\ttotal: 427ms\tremaining: 1.51s\n",
            "11:\tlearn: 0.0172822\ttotal: 462ms\tremaining: 1.46s\n",
            "12:\tlearn: 0.0165189\ttotal: 498ms\tremaining: 1.42s\n",
            "13:\tlearn: 0.0158564\ttotal: 534ms\tremaining: 1.37s\n",
            "14:\tlearn: 0.0153214\ttotal: 580ms\tremaining: 1.35s\n",
            "15:\tlearn: 0.0149423\ttotal: 621ms\tremaining: 1.32s\n",
            "16:\tlearn: 0.0145911\ttotal: 655ms\tremaining: 1.27s\n",
            "17:\tlearn: 0.0142694\ttotal: 690ms\tremaining: 1.23s\n",
            "18:\tlearn: 0.0140264\ttotal: 723ms\tremaining: 1.18s\n",
            "19:\tlearn: 0.0137431\ttotal: 756ms\tremaining: 1.13s\n",
            "20:\tlearn: 0.0136154\ttotal: 800ms\tremaining: 1.1s\n",
            "21:\tlearn: 0.0132802\ttotal: 838ms\tremaining: 1.07s\n",
            "22:\tlearn: 0.0129785\ttotal: 872ms\tremaining: 1.02s\n",
            "23:\tlearn: 0.0129027\ttotal: 907ms\tremaining: 983ms\n",
            "24:\tlearn: 0.0128001\ttotal: 945ms\tremaining: 945ms\n",
            "25:\tlearn: 0.0126833\ttotal: 982ms\tremaining: 906ms\n",
            "26:\tlearn: 0.0126001\ttotal: 1.03s\tremaining: 875ms\n",
            "27:\tlearn: 0.0124489\ttotal: 1.06s\tremaining: 834ms\n",
            "28:\tlearn: 0.0121710\ttotal: 1.09s\tremaining: 793ms\n",
            "29:\tlearn: 0.0119403\ttotal: 1.13s\tremaining: 752ms\n",
            "30:\tlearn: 0.0118424\ttotal: 1.16s\tremaining: 712ms\n",
            "31:\tlearn: 0.0117723\ttotal: 1.21s\tremaining: 681ms\n",
            "32:\tlearn: 0.0117061\ttotal: 1.25s\tremaining: 646ms\n",
            "33:\tlearn: 0.0115472\ttotal: 1.29s\tremaining: 608ms\n",
            "34:\tlearn: 0.0113967\ttotal: 1.33s\tremaining: 569ms\n",
            "35:\tlearn: 0.0111944\ttotal: 1.36s\tremaining: 530ms\n",
            "36:\tlearn: 0.0111212\ttotal: 1.4s\tremaining: 491ms\n",
            "37:\tlearn: 0.0109952\ttotal: 1.43s\tremaining: 451ms\n",
            "38:\tlearn: 0.0108691\ttotal: 1.47s\tremaining: 415ms\n",
            "39:\tlearn: 0.0107867\ttotal: 1.51s\tremaining: 377ms\n",
            "40:\tlearn: 0.0106458\ttotal: 1.54s\tremaining: 338ms\n",
            "41:\tlearn: 0.0105010\ttotal: 1.57s\tremaining: 299ms\n",
            "42:\tlearn: 0.0104333\ttotal: 1.61s\tremaining: 262ms\n",
            "43:\tlearn: 0.0103672\ttotal: 1.65s\tremaining: 226ms\n",
            "44:\tlearn: 0.0102879\ttotal: 1.7s\tremaining: 189ms\n",
            "45:\tlearn: 0.0101830\ttotal: 1.74s\tremaining: 151ms\n",
            "46:\tlearn: 0.0100956\ttotal: 1.77s\tremaining: 113ms\n",
            "47:\tlearn: 0.0100113\ttotal: 1.8s\tremaining: 75.2ms\n",
            "48:\tlearn: 0.0098834\ttotal: 1.84s\tremaining: 37.5ms\n",
            "49:\tlearn: 0.0098047\ttotal: 1.87s\tremaining: 0us\n",
            "0:\tlearn: 0.2338411\ttotal: 54.2ms\tremaining: 2.66s\n",
            "1:\tlearn: 0.2116607\ttotal: 91.2ms\tremaining: 2.19s\n",
            "2:\tlearn: 0.1964793\ttotal: 127ms\tremaining: 2s\n",
            "3:\tlearn: 0.1824492\ttotal: 165ms\tremaining: 1.9s\n",
            "4:\tlearn: 0.1697000\ttotal: 217ms\tremaining: 1.96s\n",
            "5:\tlearn: 0.1575769\ttotal: 262ms\tremaining: 1.92s\n",
            "6:\tlearn: 0.1472295\ttotal: 308ms\tremaining: 1.89s\n",
            "7:\tlearn: 0.1400747\ttotal: 348ms\tremaining: 1.82s\n",
            "8:\tlearn: 0.1340987\ttotal: 387ms\tremaining: 1.76s\n",
            "9:\tlearn: 0.1286354\ttotal: 424ms\tremaining: 1.7s\n",
            "10:\tlearn: 0.1233290\ttotal: 464ms\tremaining: 1.64s\n",
            "11:\tlearn: 0.1181732\ttotal: 522ms\tremaining: 1.65s\n",
            "12:\tlearn: 0.1146352\ttotal: 559ms\tremaining: 1.59s\n",
            "13:\tlearn: 0.1114888\ttotal: 599ms\tremaining: 1.54s\n",
            "14:\tlearn: 0.1077582\ttotal: 637ms\tremaining: 1.49s\n",
            "15:\tlearn: 0.1062719\ttotal: 676ms\tremaining: 1.44s\n",
            "16:\tlearn: 0.1030797\ttotal: 716ms\tremaining: 1.39s\n",
            "17:\tlearn: 0.1008644\ttotal: 774ms\tremaining: 1.38s\n",
            "18:\tlearn: 0.0994491\ttotal: 811ms\tremaining: 1.32s\n",
            "19:\tlearn: 0.0976037\ttotal: 849ms\tremaining: 1.27s\n",
            "20:\tlearn: 0.0968126\ttotal: 888ms\tremaining: 1.23s\n",
            "21:\tlearn: 0.0958347\ttotal: 935ms\tremaining: 1.19s\n",
            "22:\tlearn: 0.0952195\ttotal: 979ms\tremaining: 1.15s\n",
            "23:\tlearn: 0.0937600\ttotal: 1.03s\tremaining: 1.11s\n",
            "24:\tlearn: 0.0929174\ttotal: 1.07s\tremaining: 1.07s\n",
            "25:\tlearn: 0.0918966\ttotal: 1.1s\tremaining: 1.02s\n",
            "26:\tlearn: 0.0912686\ttotal: 1.15s\tremaining: 976ms\n",
            "27:\tlearn: 0.0907980\ttotal: 1.19s\tremaining: 937ms\n",
            "28:\tlearn: 0.0897741\ttotal: 1.25s\tremaining: 908ms\n",
            "29:\tlearn: 0.0889523\ttotal: 1.29s\tremaining: 860ms\n",
            "30:\tlearn: 0.0884328\ttotal: 1.33s\tremaining: 814ms\n",
            "31:\tlearn: 0.0882235\ttotal: 1.37s\tremaining: 770ms\n",
            "32:\tlearn: 0.0878318\ttotal: 1.43s\tremaining: 737ms\n",
            "33:\tlearn: 0.0868063\ttotal: 1.47s\tremaining: 693ms\n",
            "34:\tlearn: 0.0864723\ttotal: 1.52s\tremaining: 651ms\n",
            "35:\tlearn: 0.0859871\ttotal: 1.56s\tremaining: 608ms\n",
            "36:\tlearn: 0.0849176\ttotal: 1.6s\tremaining: 563ms\n",
            "37:\tlearn: 0.0843072\ttotal: 1.65s\tremaining: 522ms\n",
            "38:\tlearn: 0.0837733\ttotal: 1.69s\tremaining: 477ms\n",
            "39:\tlearn: 0.0833186\ttotal: 1.73s\tremaining: 433ms\n",
            "40:\tlearn: 0.0829766\ttotal: 1.77s\tremaining: 390ms\n",
            "41:\tlearn: 0.0825649\ttotal: 1.82s\tremaining: 347ms\n",
            "42:\tlearn: 0.0820682\ttotal: 1.88s\tremaining: 305ms\n",
            "43:\tlearn: 0.0816495\ttotal: 1.92s\tremaining: 262ms\n",
            "44:\tlearn: 0.0807896\ttotal: 1.96s\tremaining: 217ms\n",
            "45:\tlearn: 0.0802145\ttotal: 2s\tremaining: 174ms\n",
            "46:\tlearn: 0.0794454\ttotal: 2.04s\tremaining: 130ms\n",
            "47:\tlearn: 0.0787980\ttotal: 2.08s\tremaining: 86.8ms\n",
            "48:\tlearn: 0.0782849\ttotal: 2.13s\tremaining: 43.5ms\n",
            "49:\tlearn: 0.0778259\ttotal: 2.17s\tremaining: 0us\n",
            "0:\tlearn: 0.0362653\ttotal: 62.7ms\tremaining: 3.07s\n",
            "1:\tlearn: 0.0326722\ttotal: 102ms\tremaining: 2.45s\n",
            "2:\tlearn: 0.0297618\ttotal: 140ms\tremaining: 2.2s\n",
            "3:\tlearn: 0.0272301\ttotal: 181ms\tremaining: 2.08s\n",
            "4:\tlearn: 0.0248203\ttotal: 220ms\tremaining: 1.98s\n",
            "5:\tlearn: 0.0229523\ttotal: 258ms\tremaining: 1.9s\n",
            "6:\tlearn: 0.0214717\ttotal: 309ms\tremaining: 1.9s\n",
            "7:\tlearn: 0.0201309\ttotal: 350ms\tremaining: 1.84s\n",
            "8:\tlearn: 0.0188942\ttotal: 386ms\tremaining: 1.76s\n",
            "9:\tlearn: 0.0181091\ttotal: 423ms\tremaining: 1.69s\n",
            "10:\tlearn: 0.0173447\ttotal: 462ms\tremaining: 1.64s\n",
            "11:\tlearn: 0.0165988\ttotal: 497ms\tremaining: 1.57s\n",
            "12:\tlearn: 0.0159300\ttotal: 544ms\tremaining: 1.55s\n",
            "13:\tlearn: 0.0154203\ttotal: 580ms\tremaining: 1.49s\n",
            "14:\tlearn: 0.0150698\ttotal: 616ms\tremaining: 1.44s\n",
            "15:\tlearn: 0.0145106\ttotal: 654ms\tremaining: 1.39s\n",
            "16:\tlearn: 0.0141551\ttotal: 693ms\tremaining: 1.34s\n",
            "17:\tlearn: 0.0137859\ttotal: 731ms\tremaining: 1.3s\n",
            "18:\tlearn: 0.0136237\ttotal: 777ms\tremaining: 1.27s\n",
            "19:\tlearn: 0.0133454\ttotal: 816ms\tremaining: 1.22s\n",
            "20:\tlearn: 0.0130434\ttotal: 858ms\tremaining: 1.18s\n",
            "21:\tlearn: 0.0127865\ttotal: 897ms\tremaining: 1.14s\n",
            "22:\tlearn: 0.0126977\ttotal: 934ms\tremaining: 1.1s\n",
            "23:\tlearn: 0.0124862\ttotal: 988ms\tremaining: 1.07s\n",
            "24:\tlearn: 0.0124110\ttotal: 1.04s\tremaining: 1.04s\n",
            "25:\tlearn: 0.0122495\ttotal: 1.08s\tremaining: 995ms\n",
            "26:\tlearn: 0.0121079\ttotal: 1.12s\tremaining: 951ms\n",
            "27:\tlearn: 0.0120285\ttotal: 1.16s\tremaining: 908ms\n",
            "28:\tlearn: 0.0119006\ttotal: 1.21s\tremaining: 879ms\n",
            "29:\tlearn: 0.0118109\ttotal: 1.26s\tremaining: 841ms\n",
            "30:\tlearn: 0.0116995\ttotal: 1.3s\tremaining: 797ms\n",
            "31:\tlearn: 0.0116043\ttotal: 1.34s\tremaining: 753ms\n",
            "32:\tlearn: 0.0115225\ttotal: 1.38s\tremaining: 711ms\n",
            "33:\tlearn: 0.0114392\ttotal: 1.43s\tremaining: 674ms\n",
            "34:\tlearn: 0.0113112\ttotal: 1.48s\tremaining: 636ms\n",
            "35:\tlearn: 0.0112446\ttotal: 1.52s\tremaining: 593ms\n",
            "36:\tlearn: 0.0111691\ttotal: 1.56s\tremaining: 550ms\n",
            "37:\tlearn: 0.0110619\ttotal: 1.6s\tremaining: 506ms\n",
            "38:\tlearn: 0.0110201\ttotal: 1.64s\tremaining: 463ms\n",
            "39:\tlearn: 0.0109543\ttotal: 1.69s\tremaining: 422ms\n",
            "40:\tlearn: 0.0108564\ttotal: 1.72s\tremaining: 379ms\n",
            "41:\tlearn: 0.0107485\ttotal: 1.76s\tremaining: 336ms\n",
            "42:\tlearn: 0.0107081\ttotal: 1.8s\tremaining: 293ms\n",
            "43:\tlearn: 0.0106449\ttotal: 1.83s\tremaining: 250ms\n",
            "44:\tlearn: 0.0106052\ttotal: 1.88s\tremaining: 209ms\n",
            "45:\tlearn: 0.0105420\ttotal: 1.92s\tremaining: 167ms\n",
            "46:\tlearn: 0.0104807\ttotal: 1.95s\tremaining: 125ms\n",
            "47:\tlearn: 0.0104084\ttotal: 1.99s\tremaining: 82.9ms\n",
            "48:\tlearn: 0.0103830\ttotal: 2.04s\tremaining: 41.6ms\n",
            "49:\tlearn: 0.0103289\ttotal: 2.08s\tremaining: 0us\n",
            "0:\tlearn: 0.0167087\ttotal: 50.4ms\tremaining: 2.47s\n",
            "1:\tlearn: 0.0160054\ttotal: 85.6ms\tremaining: 2.05s\n",
            "2:\tlearn: 0.0153118\ttotal: 123ms\tremaining: 1.92s\n",
            "3:\tlearn: 0.0141353\ttotal: 157ms\tremaining: 1.8s\n",
            "4:\tlearn: 0.0136286\ttotal: 192ms\tremaining: 1.72s\n",
            "5:\tlearn: 0.0127657\ttotal: 228ms\tremaining: 1.67s\n",
            "6:\tlearn: 0.0121652\ttotal: 275ms\tremaining: 1.69s\n",
            "7:\tlearn: 0.0117577\ttotal: 312ms\tremaining: 1.64s\n",
            "8:\tlearn: 0.0111757\ttotal: 347ms\tremaining: 1.58s\n",
            "9:\tlearn: 0.0106516\ttotal: 381ms\tremaining: 1.52s\n",
            "10:\tlearn: 0.0100940\ttotal: 417ms\tremaining: 1.48s\n",
            "11:\tlearn: 0.0095134\ttotal: 453ms\tremaining: 1.43s\n",
            "12:\tlearn: 0.0094770\ttotal: 505ms\tremaining: 1.44s\n",
            "13:\tlearn: 0.0093746\ttotal: 544ms\tremaining: 1.4s\n",
            "14:\tlearn: 0.0093286\ttotal: 579ms\tremaining: 1.35s\n",
            "15:\tlearn: 0.0092930\ttotal: 613ms\tremaining: 1.3s\n",
            "16:\tlearn: 0.0088660\ttotal: 652ms\tremaining: 1.26s\n",
            "17:\tlearn: 0.0086095\ttotal: 688ms\tremaining: 1.22s\n",
            "18:\tlearn: 0.0082899\ttotal: 730ms\tremaining: 1.19s\n",
            "19:\tlearn: 0.0081658\ttotal: 766ms\tremaining: 1.15s\n",
            "20:\tlearn: 0.0081038\ttotal: 802ms\tremaining: 1.11s\n",
            "21:\tlearn: 0.0080010\ttotal: 842ms\tremaining: 1.07s\n",
            "22:\tlearn: 0.0078340\ttotal: 895ms\tremaining: 1.05s\n",
            "23:\tlearn: 0.0076240\ttotal: 932ms\tremaining: 1.01s\n",
            "24:\tlearn: 0.0076039\ttotal: 982ms\tremaining: 982ms\n",
            "25:\tlearn: 0.0075900\ttotal: 1.02s\tremaining: 941ms\n",
            "26:\tlearn: 0.0075719\ttotal: 1.05s\tremaining: 900ms\n",
            "27:\tlearn: 0.0075398\ttotal: 1.09s\tremaining: 860ms\n",
            "28:\tlearn: 0.0073981\ttotal: 1.13s\tremaining: 819ms\n",
            "29:\tlearn: 0.0073364\ttotal: 1.18s\tremaining: 785ms\n",
            "30:\tlearn: 0.0073173\ttotal: 1.22s\tremaining: 745ms\n",
            "31:\tlearn: 0.0072763\ttotal: 1.25s\tremaining: 706ms\n",
            "32:\tlearn: 0.0071938\ttotal: 1.29s\tremaining: 665ms\n",
            "33:\tlearn: 0.0070569\ttotal: 1.32s\tremaining: 623ms\n",
            "34:\tlearn: 0.0068246\ttotal: 1.36s\tremaining: 583ms\n",
            "35:\tlearn: 0.0067739\ttotal: 1.41s\tremaining: 547ms\n",
            "36:\tlearn: 0.0066447\ttotal: 1.44s\tremaining: 506ms\n",
            "37:\tlearn: 0.0064912\ttotal: 1.48s\tremaining: 466ms\n",
            "38:\tlearn: 0.0063824\ttotal: 1.52s\tremaining: 430ms\n",
            "39:\tlearn: 0.0063403\ttotal: 1.56s\tremaining: 390ms\n",
            "40:\tlearn: 0.0063132\ttotal: 1.6s\tremaining: 351ms\n",
            "41:\tlearn: 0.0062519\ttotal: 1.64s\tremaining: 313ms\n",
            "42:\tlearn: 0.0061437\ttotal: 1.68s\tremaining: 273ms\n",
            "43:\tlearn: 0.0060947\ttotal: 1.71s\tremaining: 234ms\n",
            "44:\tlearn: 0.0060123\ttotal: 1.75s\tremaining: 195ms\n",
            "45:\tlearn: 0.0060008\ttotal: 1.79s\tremaining: 155ms\n",
            "46:\tlearn: 0.0058346\ttotal: 1.82s\tremaining: 116ms\n",
            "47:\tlearn: 0.0058199\ttotal: 1.87s\tremaining: 77.8ms\n",
            "48:\tlearn: 0.0058167\ttotal: 1.92s\tremaining: 39.1ms\n",
            "49:\tlearn: 0.0057428\ttotal: 1.95s\tremaining: 0us\n",
            "0:\tlearn: 0.0379246\ttotal: 64.7ms\tremaining: 3.17s\n",
            "1:\tlearn: 0.0340874\ttotal: 103ms\tremaining: 2.47s\n",
            "2:\tlearn: 0.0310521\ttotal: 142ms\tremaining: 2.23s\n",
            "3:\tlearn: 0.0285235\ttotal: 181ms\tremaining: 2.08s\n",
            "4:\tlearn: 0.0262570\ttotal: 217ms\tremaining: 1.95s\n",
            "5:\tlearn: 0.0242816\ttotal: 255ms\tremaining: 1.87s\n",
            "6:\tlearn: 0.0225536\ttotal: 303ms\tremaining: 1.86s\n",
            "7:\tlearn: 0.0214663\ttotal: 340ms\tremaining: 1.78s\n",
            "8:\tlearn: 0.0203281\ttotal: 376ms\tremaining: 1.72s\n",
            "9:\tlearn: 0.0192476\ttotal: 413ms\tremaining: 1.65s\n",
            "10:\tlearn: 0.0183581\ttotal: 451ms\tremaining: 1.6s\n",
            "11:\tlearn: 0.0175336\ttotal: 487ms\tremaining: 1.54s\n",
            "12:\tlearn: 0.0169171\ttotal: 532ms\tremaining: 1.51s\n",
            "13:\tlearn: 0.0163295\ttotal: 570ms\tremaining: 1.47s\n",
            "14:\tlearn: 0.0158583\ttotal: 608ms\tremaining: 1.42s\n",
            "15:\tlearn: 0.0155354\ttotal: 649ms\tremaining: 1.38s\n",
            "16:\tlearn: 0.0151053\ttotal: 690ms\tremaining: 1.34s\n",
            "17:\tlearn: 0.0146525\ttotal: 725ms\tremaining: 1.29s\n",
            "18:\tlearn: 0.0144062\ttotal: 774ms\tremaining: 1.26s\n",
            "19:\tlearn: 0.0141780\ttotal: 811ms\tremaining: 1.22s\n",
            "20:\tlearn: 0.0139432\ttotal: 860ms\tremaining: 1.19s\n",
            "21:\tlearn: 0.0137299\ttotal: 903ms\tremaining: 1.15s\n",
            "22:\tlearn: 0.0135839\ttotal: 941ms\tremaining: 1.1s\n",
            "23:\tlearn: 0.0134916\ttotal: 979ms\tremaining: 1.06s\n",
            "24:\tlearn: 0.0133362\ttotal: 1.03s\tremaining: 1.03s\n",
            "25:\tlearn: 0.0132128\ttotal: 1.07s\tremaining: 987ms\n",
            "26:\tlearn: 0.0131267\ttotal: 1.11s\tremaining: 942ms\n",
            "27:\tlearn: 0.0130579\ttotal: 1.14s\tremaining: 897ms\n",
            "28:\tlearn: 0.0128469\ttotal: 1.18s\tremaining: 855ms\n",
            "29:\tlearn: 0.0127670\ttotal: 1.23s\tremaining: 819ms\n",
            "30:\tlearn: 0.0127031\ttotal: 1.27s\tremaining: 777ms\n",
            "31:\tlearn: 0.0126484\ttotal: 1.3s\tremaining: 733ms\n",
            "32:\tlearn: 0.0125305\ttotal: 1.34s\tremaining: 691ms\n",
            "33:\tlearn: 0.0124827\ttotal: 1.38s\tremaining: 650ms\n",
            "34:\tlearn: 0.0123868\ttotal: 1.42s\tremaining: 608ms\n",
            "35:\tlearn: 0.0122866\ttotal: 1.46s\tremaining: 570ms\n",
            "36:\tlearn: 0.0122457\ttotal: 1.5s\tremaining: 528ms\n",
            "37:\tlearn: 0.0121793\ttotal: 1.54s\tremaining: 486ms\n",
            "38:\tlearn: 0.0121127\ttotal: 1.58s\tremaining: 445ms\n",
            "39:\tlearn: 0.0120209\ttotal: 1.62s\tremaining: 405ms\n",
            "40:\tlearn: 0.0119407\ttotal: 1.66s\tremaining: 364ms\n",
            "41:\tlearn: 0.0118657\ttotal: 1.7s\tremaining: 325ms\n",
            "42:\tlearn: 0.0117679\ttotal: 1.74s\tremaining: 284ms\n",
            "43:\tlearn: 0.0116995\ttotal: 1.78s\tremaining: 243ms\n",
            "44:\tlearn: 0.0116005\ttotal: 1.81s\tremaining: 202ms\n",
            "45:\tlearn: 0.0115455\ttotal: 1.85s\tremaining: 161ms\n",
            "46:\tlearn: 0.0114844\ttotal: 1.91s\tremaining: 122ms\n",
            "47:\tlearn: 0.0114047\ttotal: 1.95s\tremaining: 81.3ms\n",
            "48:\tlearn: 0.0113514\ttotal: 1.99s\tremaining: 40.6ms\n",
            "49:\tlearn: 0.0113141\ttotal: 2.02s\tremaining: 0us\n",
            "0:\tlearn: 0.2481337\ttotal: 54.3ms\tremaining: 2.66s\n",
            "1:\tlearn: 0.2237285\ttotal: 91.7ms\tremaining: 2.2s\n",
            "2:\tlearn: 0.2054647\ttotal: 134ms\tremaining: 2.1s\n",
            "3:\tlearn: 0.1891619\ttotal: 171ms\tremaining: 1.97s\n",
            "4:\tlearn: 0.1758472\ttotal: 210ms\tremaining: 1.89s\n",
            "5:\tlearn: 0.1648105\ttotal: 249ms\tremaining: 1.82s\n",
            "6:\tlearn: 0.1556005\ttotal: 298ms\tremaining: 1.83s\n",
            "7:\tlearn: 0.1479187\ttotal: 335ms\tremaining: 1.76s\n",
            "8:\tlearn: 0.1408109\ttotal: 372ms\tremaining: 1.69s\n",
            "9:\tlearn: 0.1338893\ttotal: 407ms\tremaining: 1.63s\n",
            "10:\tlearn: 0.1292097\ttotal: 446ms\tremaining: 1.58s\n",
            "11:\tlearn: 0.1254447\ttotal: 484ms\tremaining: 1.53s\n",
            "12:\tlearn: 0.1221852\ttotal: 535ms\tremaining: 1.52s\n",
            "13:\tlearn: 0.1183139\ttotal: 574ms\tremaining: 1.48s\n",
            "14:\tlearn: 0.1154017\ttotal: 611ms\tremaining: 1.43s\n",
            "15:\tlearn: 0.1136298\ttotal: 649ms\tremaining: 1.38s\n",
            "16:\tlearn: 0.1097689\ttotal: 684ms\tremaining: 1.33s\n",
            "17:\tlearn: 0.1072664\ttotal: 736ms\tremaining: 1.31s\n",
            "18:\tlearn: 0.1051601\ttotal: 793ms\tremaining: 1.29s\n",
            "19:\tlearn: 0.1039451\ttotal: 834ms\tremaining: 1.25s\n",
            "20:\tlearn: 0.1025752\ttotal: 874ms\tremaining: 1.21s\n",
            "21:\tlearn: 0.1014851\ttotal: 912ms\tremaining: 1.16s\n",
            "22:\tlearn: 0.1005668\ttotal: 958ms\tremaining: 1.12s\n",
            "23:\tlearn: 0.0987484\ttotal: 1s\tremaining: 1.09s\n",
            "24:\tlearn: 0.0980609\ttotal: 1.04s\tremaining: 1.04s\n",
            "25:\tlearn: 0.0970074\ttotal: 1.08s\tremaining: 995ms\n",
            "26:\tlearn: 0.0963759\ttotal: 1.11s\tremaining: 950ms\n",
            "27:\tlearn: 0.0958844\ttotal: 1.16s\tremaining: 911ms\n",
            "28:\tlearn: 0.0955290\ttotal: 1.21s\tremaining: 873ms\n",
            "29:\tlearn: 0.0948936\ttotal: 1.24s\tremaining: 829ms\n",
            "30:\tlearn: 0.0945161\ttotal: 1.28s\tremaining: 786ms\n",
            "31:\tlearn: 0.0938600\ttotal: 1.32s\tremaining: 744ms\n",
            "32:\tlearn: 0.0934892\ttotal: 1.36s\tremaining: 702ms\n",
            "33:\tlearn: 0.0924209\ttotal: 1.41s\tremaining: 666ms\n",
            "34:\tlearn: 0.0919541\ttotal: 1.46s\tremaining: 625ms\n",
            "35:\tlearn: 0.0916007\ttotal: 1.5s\tremaining: 583ms\n",
            "36:\tlearn: 0.0905963\ttotal: 1.54s\tremaining: 540ms\n",
            "37:\tlearn: 0.0902476\ttotal: 1.58s\tremaining: 501ms\n",
            "38:\tlearn: 0.0894557\ttotal: 1.62s\tremaining: 457ms\n",
            "39:\tlearn: 0.0889303\ttotal: 1.66s\tremaining: 415ms\n",
            "40:\tlearn: 0.0887278\ttotal: 1.7s\tremaining: 374ms\n",
            "41:\tlearn: 0.0883828\ttotal: 1.76s\tremaining: 335ms\n",
            "42:\tlearn: 0.0879270\ttotal: 1.8s\tremaining: 294ms\n",
            "43:\tlearn: 0.0876311\ttotal: 1.84s\tremaining: 252ms\n",
            "44:\tlearn: 0.0868129\ttotal: 1.88s\tremaining: 209ms\n",
            "45:\tlearn: 0.0864574\ttotal: 1.92s\tremaining: 167ms\n",
            "46:\tlearn: 0.0858687\ttotal: 1.96s\tremaining: 125ms\n",
            "47:\tlearn: 0.0851661\ttotal: 2s\tremaining: 83.3ms\n",
            "48:\tlearn: 0.0845312\ttotal: 2.05s\tremaining: 41.9ms\n",
            "49:\tlearn: 0.0839022\ttotal: 2.09s\tremaining: 0us\n",
            "0:\tlearn: 0.0361430\ttotal: 52.7ms\tremaining: 2.58s\n",
            "1:\tlearn: 0.0326649\ttotal: 98.9ms\tremaining: 2.37s\n",
            "2:\tlearn: 0.0300504\ttotal: 146ms\tremaining: 2.29s\n",
            "3:\tlearn: 0.0275937\ttotal: 184ms\tremaining: 2.11s\n",
            "4:\tlearn: 0.0250113\ttotal: 223ms\tremaining: 2s\n",
            "5:\tlearn: 0.0233426\ttotal: 261ms\tremaining: 1.91s\n",
            "6:\tlearn: 0.0218396\ttotal: 305ms\tremaining: 1.87s\n",
            "7:\tlearn: 0.0204813\ttotal: 351ms\tremaining: 1.84s\n",
            "8:\tlearn: 0.0191311\ttotal: 390ms\tremaining: 1.78s\n",
            "9:\tlearn: 0.0179971\ttotal: 431ms\tremaining: 1.72s\n",
            "10:\tlearn: 0.0171246\ttotal: 469ms\tremaining: 1.66s\n",
            "11:\tlearn: 0.0163360\ttotal: 522ms\tremaining: 1.65s\n",
            "12:\tlearn: 0.0156399\ttotal: 575ms\tremaining: 1.64s\n",
            "13:\tlearn: 0.0151620\ttotal: 615ms\tremaining: 1.58s\n",
            "14:\tlearn: 0.0145950\ttotal: 654ms\tremaining: 1.53s\n",
            "15:\tlearn: 0.0142066\ttotal: 692ms\tremaining: 1.47s\n",
            "16:\tlearn: 0.0137675\ttotal: 741ms\tremaining: 1.44s\n",
            "17:\tlearn: 0.0134534\ttotal: 783ms\tremaining: 1.39s\n",
            "18:\tlearn: 0.0132783\ttotal: 820ms\tremaining: 1.34s\n",
            "19:\tlearn: 0.0129889\ttotal: 859ms\tremaining: 1.29s\n",
            "20:\tlearn: 0.0128160\ttotal: 898ms\tremaining: 1.24s\n",
            "21:\tlearn: 0.0125034\ttotal: 936ms\tremaining: 1.19s\n",
            "22:\tlearn: 0.0122415\ttotal: 987ms\tremaining: 1.16s\n",
            "23:\tlearn: 0.0120673\ttotal: 1.02s\tremaining: 1.11s\n",
            "24:\tlearn: 0.0119714\ttotal: 1.06s\tremaining: 1.06s\n",
            "25:\tlearn: 0.0118178\ttotal: 1.1s\tremaining: 1.02s\n",
            "26:\tlearn: 0.0116855\ttotal: 1.14s\tremaining: 972ms\n",
            "27:\tlearn: 0.0116252\ttotal: 1.18s\tremaining: 926ms\n",
            "28:\tlearn: 0.0115575\ttotal: 1.23s\tremaining: 889ms\n",
            "29:\tlearn: 0.0114655\ttotal: 1.27s\tremaining: 850ms\n",
            "30:\tlearn: 0.0113560\ttotal: 1.31s\tremaining: 805ms\n",
            "31:\tlearn: 0.0112805\ttotal: 1.35s\tremaining: 761ms\n",
            "32:\tlearn: 0.0112153\ttotal: 1.39s\tremaining: 718ms\n",
            "33:\tlearn: 0.0110883\ttotal: 1.45s\tremaining: 683ms\n",
            "34:\tlearn: 0.0110060\ttotal: 1.49s\tremaining: 641ms\n",
            "35:\tlearn: 0.0109603\ttotal: 1.53s\tremaining: 596ms\n",
            "36:\tlearn: 0.0109069\ttotal: 1.58s\tremaining: 556ms\n",
            "37:\tlearn: 0.0108193\ttotal: 1.62s\tremaining: 512ms\n",
            "38:\tlearn: 0.0107677\ttotal: 1.67s\tremaining: 470ms\n",
            "39:\tlearn: 0.0107176\ttotal: 1.71s\tremaining: 426ms\n",
            "40:\tlearn: 0.0106709\ttotal: 1.75s\tremaining: 384ms\n",
            "41:\tlearn: 0.0106086\ttotal: 1.8s\tremaining: 342ms\n",
            "42:\tlearn: 0.0105632\ttotal: 1.83s\tremaining: 299ms\n",
            "43:\tlearn: 0.0104932\ttotal: 1.88s\tremaining: 257ms\n",
            "44:\tlearn: 0.0104214\ttotal: 1.92s\tremaining: 213ms\n",
            "45:\tlearn: 0.0103650\ttotal: 1.96s\tremaining: 170ms\n",
            "46:\tlearn: 0.0102839\ttotal: 2s\tremaining: 127ms\n",
            "47:\tlearn: 0.0102453\ttotal: 2.04s\tremaining: 84.8ms\n",
            "48:\tlearn: 0.0102124\ttotal: 2.07s\tremaining: 42.3ms\n",
            "49:\tlearn: 0.0101371\ttotal: 2.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0197841\ttotal: 54.1ms\tremaining: 2.65s\n",
            "1:\tlearn: 0.0185339\ttotal: 98.7ms\tremaining: 2.37s\n",
            "2:\tlearn: 0.0179062\ttotal: 152ms\tremaining: 2.37s\n",
            "3:\tlearn: 0.0166209\ttotal: 191ms\tremaining: 2.2s\n",
            "4:\tlearn: 0.0157109\ttotal: 236ms\tremaining: 2.12s\n",
            "5:\tlearn: 0.0152354\ttotal: 279ms\tremaining: 2.05s\n",
            "6:\tlearn: 0.0143950\ttotal: 320ms\tremaining: 1.96s\n",
            "7:\tlearn: 0.0134230\ttotal: 384ms\tremaining: 2.02s\n",
            "8:\tlearn: 0.0128126\ttotal: 428ms\tremaining: 1.95s\n",
            "9:\tlearn: 0.0122473\ttotal: 478ms\tremaining: 1.91s\n",
            "10:\tlearn: 0.0116771\ttotal: 519ms\tremaining: 1.84s\n",
            "11:\tlearn: 0.0116096\ttotal: 558ms\tremaining: 1.77s\n",
            "12:\tlearn: 0.0111196\ttotal: 603ms\tremaining: 1.72s\n",
            "13:\tlearn: 0.0106864\ttotal: 645ms\tremaining: 1.66s\n",
            "14:\tlearn: 0.0105742\ttotal: 742ms\tremaining: 1.73s\n",
            "15:\tlearn: 0.0105050\ttotal: 854ms\tremaining: 1.81s\n",
            "16:\tlearn: 0.0101391\ttotal: 897ms\tremaining: 1.74s\n",
            "17:\tlearn: 0.0098619\ttotal: 935ms\tremaining: 1.66s\n",
            "18:\tlearn: 0.0095570\ttotal: 975ms\tremaining: 1.59s\n",
            "19:\tlearn: 0.0094110\ttotal: 1.01s\tremaining: 1.52s\n",
            "20:\tlearn: 0.0093285\ttotal: 1.05s\tremaining: 1.46s\n",
            "21:\tlearn: 0.0092995\ttotal: 1.1s\tremaining: 1.4s\n",
            "22:\tlearn: 0.0092287\ttotal: 1.14s\tremaining: 1.34s\n",
            "23:\tlearn: 0.0090422\ttotal: 1.2s\tremaining: 1.3s\n",
            "24:\tlearn: 0.0088911\ttotal: 1.24s\tremaining: 1.24s\n",
            "25:\tlearn: 0.0088690\ttotal: 1.28s\tremaining: 1.18s\n",
            "26:\tlearn: 0.0088446\ttotal: 1.33s\tremaining: 1.13s\n",
            "27:\tlearn: 0.0088188\ttotal: 1.37s\tremaining: 1.07s\n",
            "28:\tlearn: 0.0086607\ttotal: 1.42s\tremaining: 1.03s\n",
            "29:\tlearn: 0.0084627\ttotal: 1.46s\tremaining: 973ms\n",
            "30:\tlearn: 0.0083454\ttotal: 1.5s\tremaining: 917ms\n",
            "31:\tlearn: 0.0081487\ttotal: 1.54s\tremaining: 870ms\n",
            "32:\tlearn: 0.0080433\ttotal: 1.58s\tremaining: 817ms\n",
            "33:\tlearn: 0.0079438\ttotal: 1.62s\tremaining: 762ms\n",
            "34:\tlearn: 0.0078582\ttotal: 1.66s\tremaining: 710ms\n",
            "35:\tlearn: 0.0076886\ttotal: 1.69s\tremaining: 658ms\n",
            "36:\tlearn: 0.0076610\ttotal: 1.73s\tremaining: 609ms\n",
            "37:\tlearn: 0.0075583\ttotal: 1.78s\tremaining: 563ms\n",
            "38:\tlearn: 0.0075157\ttotal: 1.82s\tremaining: 514ms\n",
            "39:\tlearn: 0.0074465\ttotal: 1.86s\tremaining: 466ms\n",
            "40:\tlearn: 0.0074311\ttotal: 1.9s\tremaining: 417ms\n",
            "41:\tlearn: 0.0073734\ttotal: 1.94s\tremaining: 370ms\n",
            "42:\tlearn: 0.0072844\ttotal: 1.98s\tremaining: 322ms\n",
            "43:\tlearn: 0.0072719\ttotal: 2.03s\tremaining: 277ms\n",
            "44:\tlearn: 0.0071871\ttotal: 2.07s\tremaining: 230ms\n",
            "45:\tlearn: 0.0071643\ttotal: 2.11s\tremaining: 184ms\n",
            "46:\tlearn: 0.0070735\ttotal: 2.15s\tremaining: 138ms\n",
            "47:\tlearn: 0.0070090\ttotal: 2.19s\tremaining: 91.4ms\n",
            "48:\tlearn: 0.0069419\ttotal: 2.24s\tremaining: 45.7ms\n",
            "49:\tlearn: 0.0069247\ttotal: 2.29s\tremaining: 0us\n",
            "0:\tlearn: 0.0369901\ttotal: 94.5ms\tremaining: 4.63s\n",
            "1:\tlearn: 0.0333813\ttotal: 134ms\tremaining: 3.21s\n",
            "2:\tlearn: 0.0305187\ttotal: 172ms\tremaining: 2.7s\n",
            "3:\tlearn: 0.0279901\ttotal: 209ms\tremaining: 2.41s\n",
            "4:\tlearn: 0.0257681\ttotal: 249ms\tremaining: 2.24s\n",
            "5:\tlearn: 0.0239108\ttotal: 290ms\tremaining: 2.12s\n",
            "6:\tlearn: 0.0225194\ttotal: 343ms\tremaining: 2.11s\n",
            "7:\tlearn: 0.0211501\ttotal: 381ms\tremaining: 2s\n",
            "8:\tlearn: 0.0199546\ttotal: 417ms\tremaining: 1.9s\n",
            "9:\tlearn: 0.0188651\ttotal: 454ms\tremaining: 1.81s\n",
            "10:\tlearn: 0.0179764\ttotal: 492ms\tremaining: 1.75s\n",
            "11:\tlearn: 0.0172220\ttotal: 531ms\tremaining: 1.68s\n",
            "12:\tlearn: 0.0166462\ttotal: 586ms\tremaining: 1.67s\n",
            "13:\tlearn: 0.0160911\ttotal: 630ms\tremaining: 1.62s\n",
            "14:\tlearn: 0.0156826\ttotal: 670ms\tremaining: 1.56s\n",
            "15:\tlearn: 0.0153587\ttotal: 708ms\tremaining: 1.5s\n",
            "16:\tlearn: 0.0150349\ttotal: 746ms\tremaining: 1.45s\n",
            "17:\tlearn: 0.0147493\ttotal: 784ms\tremaining: 1.39s\n",
            "18:\tlearn: 0.0144913\ttotal: 834ms\tremaining: 1.36s\n",
            "19:\tlearn: 0.0143089\ttotal: 875ms\tremaining: 1.31s\n",
            "20:\tlearn: 0.0141293\ttotal: 912ms\tremaining: 1.26s\n",
            "21:\tlearn: 0.0139614\ttotal: 950ms\tremaining: 1.21s\n",
            "22:\tlearn: 0.0137888\ttotal: 990ms\tremaining: 1.16s\n",
            "23:\tlearn: 0.0136389\ttotal: 1.03s\tremaining: 1.12s\n",
            "24:\tlearn: 0.0135708\ttotal: 1.1s\tremaining: 1.1s\n",
            "25:\tlearn: 0.0134934\ttotal: 1.14s\tremaining: 1.05s\n",
            "26:\tlearn: 0.0133858\ttotal: 1.18s\tremaining: 1s\n",
            "27:\tlearn: 0.0132403\ttotal: 1.22s\tremaining: 956ms\n",
            "28:\tlearn: 0.0131074\ttotal: 1.25s\tremaining: 909ms\n",
            "29:\tlearn: 0.0130463\ttotal: 1.29s\tremaining: 862ms\n",
            "30:\tlearn: 0.0129973\ttotal: 1.34s\tremaining: 820ms\n",
            "31:\tlearn: 0.0129300\ttotal: 1.38s\tremaining: 774ms\n",
            "32:\tlearn: 0.0128153\ttotal: 1.41s\tremaining: 728ms\n",
            "33:\tlearn: 0.0127586\ttotal: 1.45s\tremaining: 683ms\n",
            "34:\tlearn: 0.0126195\ttotal: 1.5s\tremaining: 641ms\n",
            "35:\tlearn: 0.0125164\ttotal: 1.53s\tremaining: 597ms\n",
            "36:\tlearn: 0.0124463\ttotal: 1.59s\tremaining: 558ms\n",
            "37:\tlearn: 0.0123782\ttotal: 1.63s\tremaining: 513ms\n",
            "38:\tlearn: 0.0122985\ttotal: 1.66s\tremaining: 469ms\n",
            "39:\tlearn: 0.0122521\ttotal: 1.7s\tremaining: 425ms\n",
            "40:\tlearn: 0.0122193\ttotal: 1.74s\tremaining: 383ms\n",
            "41:\tlearn: 0.0121762\ttotal: 1.78s\tremaining: 340ms\n",
            "42:\tlearn: 0.0121129\ttotal: 1.83s\tremaining: 298ms\n",
            "43:\tlearn: 0.0120721\ttotal: 1.87s\tremaining: 255ms\n",
            "44:\tlearn: 0.0119745\ttotal: 1.91s\tremaining: 212ms\n",
            "45:\tlearn: 0.0119155\ttotal: 1.95s\tremaining: 169ms\n",
            "46:\tlearn: 0.0118694\ttotal: 1.98s\tremaining: 127ms\n",
            "47:\tlearn: 0.0118196\ttotal: 2.02s\tremaining: 84.3ms\n",
            "48:\tlearn: 0.0117585\ttotal: 2.08s\tremaining: 42.4ms\n",
            "49:\tlearn: 0.0117257\ttotal: 2.13s\tremaining: 0us\n",
            "0:\tlearn: 0.2719658\ttotal: 63.7ms\tremaining: 3.12s\n",
            "1:\tlearn: 0.2425261\ttotal: 104ms\tremaining: 2.49s\n",
            "2:\tlearn: 0.2215885\ttotal: 143ms\tremaining: 2.23s\n",
            "3:\tlearn: 0.2020849\ttotal: 180ms\tremaining: 2.07s\n",
            "4:\tlearn: 0.1868911\ttotal: 220ms\tremaining: 1.98s\n",
            "5:\tlearn: 0.1749070\ttotal: 258ms\tremaining: 1.89s\n",
            "6:\tlearn: 0.1647314\ttotal: 310ms\tremaining: 1.91s\n",
            "7:\tlearn: 0.1562640\ttotal: 354ms\tremaining: 1.85s\n",
            "8:\tlearn: 0.1477318\ttotal: 391ms\tremaining: 1.78s\n",
            "9:\tlearn: 0.1411694\ttotal: 429ms\tremaining: 1.72s\n",
            "10:\tlearn: 0.1358495\ttotal: 468ms\tremaining: 1.66s\n",
            "11:\tlearn: 0.1300282\ttotal: 507ms\tremaining: 1.6s\n",
            "12:\tlearn: 0.1257804\ttotal: 556ms\tremaining: 1.58s\n",
            "13:\tlearn: 0.1222189\ttotal: 596ms\tremaining: 1.53s\n",
            "14:\tlearn: 0.1194741\ttotal: 635ms\tremaining: 1.48s\n",
            "15:\tlearn: 0.1180114\ttotal: 676ms\tremaining: 1.44s\n",
            "16:\tlearn: 0.1146087\ttotal: 714ms\tremaining: 1.39s\n",
            "17:\tlearn: 0.1122346\ttotal: 752ms\tremaining: 1.34s\n",
            "18:\tlearn: 0.1103913\ttotal: 803ms\tremaining: 1.31s\n",
            "19:\tlearn: 0.1093436\ttotal: 854ms\tremaining: 1.28s\n",
            "20:\tlearn: 0.1078191\ttotal: 898ms\tremaining: 1.24s\n",
            "21:\tlearn: 0.1068141\ttotal: 940ms\tremaining: 1.2s\n",
            "22:\tlearn: 0.1053888\ttotal: 987ms\tremaining: 1.16s\n",
            "23:\tlearn: 0.1042899\ttotal: 1.03s\tremaining: 1.12s\n",
            "24:\tlearn: 0.1037063\ttotal: 1.07s\tremaining: 1.07s\n",
            "25:\tlearn: 0.1027984\ttotal: 1.11s\tremaining: 1.03s\n",
            "26:\tlearn: 0.1021035\ttotal: 1.16s\tremaining: 986ms\n",
            "27:\tlearn: 0.1016576\ttotal: 1.2s\tremaining: 942ms\n",
            "28:\tlearn: 0.1011519\ttotal: 1.25s\tremaining: 903ms\n",
            "29:\tlearn: 0.1006427\ttotal: 1.29s\tremaining: 861ms\n",
            "30:\tlearn: 0.1000658\ttotal: 1.34s\tremaining: 820ms\n",
            "31:\tlearn: 0.0990735\ttotal: 1.38s\tremaining: 776ms\n",
            "32:\tlearn: 0.0986809\ttotal: 1.42s\tremaining: 731ms\n",
            "33:\tlearn: 0.0982095\ttotal: 1.47s\tremaining: 691ms\n",
            "34:\tlearn: 0.0968052\ttotal: 1.5s\tremaining: 645ms\n",
            "35:\tlearn: 0.0959140\ttotal: 1.55s\tremaining: 602ms\n",
            "36:\tlearn: 0.0954396\ttotal: 1.59s\tremaining: 557ms\n",
            "37:\tlearn: 0.0945052\ttotal: 1.63s\tremaining: 514ms\n",
            "38:\tlearn: 0.0937864\ttotal: 1.67s\tremaining: 470ms\n",
            "39:\tlearn: 0.0933585\ttotal: 1.71s\tremaining: 428ms\n",
            "40:\tlearn: 0.0928659\ttotal: 1.75s\tremaining: 384ms\n",
            "41:\tlearn: 0.0925742\ttotal: 1.79s\tremaining: 342ms\n",
            "42:\tlearn: 0.0920223\ttotal: 1.83s\tremaining: 299ms\n",
            "43:\tlearn: 0.0917579\ttotal: 1.89s\tremaining: 257ms\n",
            "44:\tlearn: 0.0908895\ttotal: 1.93s\tremaining: 215ms\n",
            "45:\tlearn: 0.0906055\ttotal: 1.97s\tremaining: 171ms\n",
            "46:\tlearn: 0.0901445\ttotal: 2.01s\tremaining: 128ms\n",
            "47:\tlearn: 0.0896338\ttotal: 2.05s\tremaining: 85.5ms\n",
            "48:\tlearn: 0.0890277\ttotal: 2.09s\tremaining: 42.6ms\n",
            "49:\tlearn: 0.0885770\ttotal: 2.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0381689\ttotal: 54.7ms\tremaining: 2.68s\n",
            "1:\tlearn: 0.0345293\ttotal: 94.1ms\tremaining: 2.26s\n",
            "2:\tlearn: 0.0315393\ttotal: 131ms\tremaining: 2.06s\n",
            "3:\tlearn: 0.0287447\ttotal: 169ms\tremaining: 1.95s\n",
            "4:\tlearn: 0.0263025\ttotal: 212ms\tremaining: 1.91s\n",
            "5:\tlearn: 0.0242732\ttotal: 250ms\tremaining: 1.83s\n",
            "6:\tlearn: 0.0227851\ttotal: 299ms\tremaining: 1.84s\n",
            "7:\tlearn: 0.0212248\ttotal: 337ms\tremaining: 1.77s\n",
            "8:\tlearn: 0.0197465\ttotal: 375ms\tremaining: 1.71s\n",
            "9:\tlearn: 0.0185548\ttotal: 413ms\tremaining: 1.65s\n",
            "10:\tlearn: 0.0176849\ttotal: 452ms\tremaining: 1.6s\n",
            "11:\tlearn: 0.0168973\ttotal: 493ms\tremaining: 1.56s\n",
            "12:\tlearn: 0.0161778\ttotal: 545ms\tremaining: 1.55s\n",
            "13:\tlearn: 0.0154371\ttotal: 586ms\tremaining: 1.51s\n",
            "14:\tlearn: 0.0148712\ttotal: 633ms\tremaining: 1.48s\n",
            "15:\tlearn: 0.0143549\ttotal: 680ms\tremaining: 1.44s\n",
            "16:\tlearn: 0.0139412\ttotal: 718ms\tremaining: 1.39s\n",
            "17:\tlearn: 0.0136112\ttotal: 763ms\tremaining: 1.36s\n",
            "18:\tlearn: 0.0133002\ttotal: 803ms\tremaining: 1.31s\n",
            "19:\tlearn: 0.0129804\ttotal: 842ms\tremaining: 1.26s\n",
            "20:\tlearn: 0.0128408\ttotal: 883ms\tremaining: 1.22s\n",
            "21:\tlearn: 0.0126809\ttotal: 925ms\tremaining: 1.18s\n",
            "22:\tlearn: 0.0125735\ttotal: 963ms\tremaining: 1.13s\n",
            "23:\tlearn: 0.0124279\ttotal: 1.01s\tremaining: 1.09s\n",
            "24:\tlearn: 0.0123563\ttotal: 1.05s\tremaining: 1.05s\n",
            "25:\tlearn: 0.0121911\ttotal: 1.11s\tremaining: 1.03s\n",
            "26:\tlearn: 0.0120478\ttotal: 1.15s\tremaining: 981ms\n",
            "27:\tlearn: 0.0119229\ttotal: 1.2s\tremaining: 945ms\n",
            "28:\tlearn: 0.0118138\ttotal: 1.24s\tremaining: 900ms\n",
            "29:\tlearn: 0.0117094\ttotal: 1.28s\tremaining: 856ms\n",
            "30:\tlearn: 0.0116276\ttotal: 1.32s\tremaining: 811ms\n",
            "31:\tlearn: 0.0115833\ttotal: 1.36s\tremaining: 766ms\n",
            "32:\tlearn: 0.0115210\ttotal: 1.41s\tremaining: 724ms\n",
            "33:\tlearn: 0.0114814\ttotal: 1.45s\tremaining: 684ms\n",
            "34:\tlearn: 0.0113961\ttotal: 1.5s\tremaining: 643ms\n",
            "35:\tlearn: 0.0113098\ttotal: 1.54s\tremaining: 598ms\n",
            "36:\tlearn: 0.0112381\ttotal: 1.57s\tremaining: 554ms\n",
            "37:\tlearn: 0.0111827\ttotal: 1.62s\tremaining: 512ms\n",
            "38:\tlearn: 0.0111209\ttotal: 1.68s\tremaining: 473ms\n",
            "39:\tlearn: 0.0110633\ttotal: 1.72s\tremaining: 429ms\n",
            "40:\tlearn: 0.0110158\ttotal: 1.76s\tremaining: 386ms\n",
            "41:\tlearn: 0.0109575\ttotal: 1.8s\tremaining: 342ms\n",
            "42:\tlearn: 0.0108994\ttotal: 1.84s\tremaining: 300ms\n",
            "43:\tlearn: 0.0108442\ttotal: 1.89s\tremaining: 257ms\n",
            "44:\tlearn: 0.0107860\ttotal: 1.93s\tremaining: 214ms\n",
            "45:\tlearn: 0.0107364\ttotal: 1.97s\tremaining: 171ms\n",
            "46:\tlearn: 0.0106802\ttotal: 2.01s\tremaining: 128ms\n",
            "47:\tlearn: 0.0106195\ttotal: 2.05s\tremaining: 85.3ms\n",
            "48:\tlearn: 0.0105713\ttotal: 2.09s\tremaining: 42.7ms\n",
            "49:\tlearn: 0.0105405\ttotal: 2.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0248148\ttotal: 68.5ms\tremaining: 3.35s\n",
            "1:\tlearn: 0.0232899\ttotal: 114ms\tremaining: 2.74s\n",
            "2:\tlearn: 0.0222070\ttotal: 157ms\tremaining: 2.46s\n",
            "3:\tlearn: 0.0205570\ttotal: 196ms\tremaining: 2.25s\n",
            "4:\tlearn: 0.0193206\ttotal: 235ms\tremaining: 2.11s\n",
            "5:\tlearn: 0.0180799\ttotal: 280ms\tremaining: 2.05s\n",
            "6:\tlearn: 0.0172441\ttotal: 322ms\tremaining: 1.98s\n",
            "7:\tlearn: 0.0160786\ttotal: 361ms\tremaining: 1.89s\n",
            "8:\tlearn: 0.0151222\ttotal: 396ms\tremaining: 1.8s\n",
            "9:\tlearn: 0.0143994\ttotal: 438ms\tremaining: 1.75s\n",
            "10:\tlearn: 0.0137810\ttotal: 490ms\tremaining: 1.74s\n",
            "11:\tlearn: 0.0131391\ttotal: 529ms\tremaining: 1.68s\n",
            "12:\tlearn: 0.0126887\ttotal: 567ms\tremaining: 1.61s\n",
            "13:\tlearn: 0.0124855\ttotal: 605ms\tremaining: 1.55s\n",
            "14:\tlearn: 0.0121686\ttotal: 642ms\tremaining: 1.5s\n",
            "15:\tlearn: 0.0118329\ttotal: 680ms\tremaining: 1.45s\n",
            "16:\tlearn: 0.0114715\ttotal: 728ms\tremaining: 1.41s\n",
            "17:\tlearn: 0.0113952\ttotal: 769ms\tremaining: 1.37s\n",
            "18:\tlearn: 0.0111019\ttotal: 807ms\tremaining: 1.32s\n",
            "19:\tlearn: 0.0108734\ttotal: 847ms\tremaining: 1.27s\n",
            "20:\tlearn: 0.0108311\ttotal: 887ms\tremaining: 1.22s\n",
            "21:\tlearn: 0.0107848\ttotal: 925ms\tremaining: 1.18s\n",
            "22:\tlearn: 0.0107670\ttotal: 981ms\tremaining: 1.15s\n",
            "23:\tlearn: 0.0106516\ttotal: 1.02s\tremaining: 1.1s\n",
            "24:\tlearn: 0.0105700\ttotal: 1.06s\tremaining: 1.06s\n",
            "25:\tlearn: 0.0104957\ttotal: 1.1s\tremaining: 1.02s\n",
            "26:\tlearn: 0.0104638\ttotal: 1.14s\tremaining: 972ms\n",
            "27:\tlearn: 0.0103423\ttotal: 1.18s\tremaining: 927ms\n",
            "28:\tlearn: 0.0101740\ttotal: 1.23s\tremaining: 888ms\n",
            "29:\tlearn: 0.0099730\ttotal: 1.27s\tremaining: 846ms\n",
            "30:\tlearn: 0.0098229\ttotal: 1.31s\tremaining: 805ms\n",
            "31:\tlearn: 0.0097809\ttotal: 1.35s\tremaining: 762ms\n",
            "32:\tlearn: 0.0097364\ttotal: 1.39s\tremaining: 718ms\n",
            "33:\tlearn: 0.0096375\ttotal: 1.44s\tremaining: 679ms\n",
            "34:\tlearn: 0.0094670\ttotal: 1.5s\tremaining: 642ms\n",
            "35:\tlearn: 0.0094146\ttotal: 1.54s\tremaining: 598ms\n",
            "36:\tlearn: 0.0093574\ttotal: 1.58s\tremaining: 554ms\n",
            "37:\tlearn: 0.0092298\ttotal: 1.62s\tremaining: 511ms\n",
            "38:\tlearn: 0.0091821\ttotal: 1.66s\tremaining: 468ms\n",
            "39:\tlearn: 0.0089970\ttotal: 1.7s\tremaining: 424ms\n",
            "40:\tlearn: 0.0089845\ttotal: 1.73s\tremaining: 380ms\n",
            "41:\tlearn: 0.0089627\ttotal: 1.77s\tremaining: 338ms\n",
            "42:\tlearn: 0.0088647\ttotal: 1.81s\tremaining: 295ms\n",
            "43:\tlearn: 0.0088433\ttotal: 1.85s\tremaining: 252ms\n",
            "44:\tlearn: 0.0088344\ttotal: 1.9s\tremaining: 211ms\n",
            "45:\tlearn: 0.0087504\ttotal: 1.93s\tremaining: 168ms\n",
            "46:\tlearn: 0.0086902\ttotal: 1.96s\tremaining: 125ms\n",
            "47:\tlearn: 0.0086673\ttotal: 2s\tremaining: 83.3ms\n",
            "48:\tlearn: 0.0085943\ttotal: 2.04s\tremaining: 41.5ms\n",
            "49:\tlearn: 0.0085711\ttotal: 2.08s\tremaining: 0us\n",
            "0:\tlearn: 0.0403468\ttotal: 50.2ms\tremaining: 2.46s\n",
            "1:\tlearn: 0.0364295\ttotal: 89.3ms\tremaining: 2.14s\n",
            "2:\tlearn: 0.0331397\ttotal: 124ms\tremaining: 1.94s\n",
            "3:\tlearn: 0.0302096\ttotal: 159ms\tremaining: 1.83s\n",
            "4:\tlearn: 0.0278107\ttotal: 194ms\tremaining: 1.74s\n",
            "5:\tlearn: 0.0254245\ttotal: 228ms\tremaining: 1.67s\n",
            "6:\tlearn: 0.0236474\ttotal: 274ms\tremaining: 1.69s\n",
            "7:\tlearn: 0.0222336\ttotal: 320ms\tremaining: 1.68s\n",
            "8:\tlearn: 0.0209282\ttotal: 361ms\tremaining: 1.64s\n",
            "9:\tlearn: 0.0197744\ttotal: 402ms\tremaining: 1.61s\n",
            "10:\tlearn: 0.0187556\ttotal: 438ms\tremaining: 1.55s\n",
            "11:\tlearn: 0.0180139\ttotal: 474ms\tremaining: 1.5s\n",
            "12:\tlearn: 0.0174698\ttotal: 518ms\tremaining: 1.47s\n",
            "13:\tlearn: 0.0169633\ttotal: 553ms\tremaining: 1.42s\n",
            "14:\tlearn: 0.0165078\ttotal: 592ms\tremaining: 1.38s\n",
            "15:\tlearn: 0.0160544\ttotal: 629ms\tremaining: 1.34s\n",
            "16:\tlearn: 0.0157601\ttotal: 665ms\tremaining: 1.29s\n",
            "17:\tlearn: 0.0154634\ttotal: 713ms\tremaining: 1.27s\n",
            "18:\tlearn: 0.0151973\ttotal: 747ms\tremaining: 1.22s\n",
            "19:\tlearn: 0.0149706\ttotal: 783ms\tremaining: 1.17s\n",
            "20:\tlearn: 0.0148283\ttotal: 820ms\tremaining: 1.13s\n",
            "21:\tlearn: 0.0145021\ttotal: 856ms\tremaining: 1.09s\n",
            "22:\tlearn: 0.0143236\ttotal: 894ms\tremaining: 1.05s\n",
            "23:\tlearn: 0.0141867\ttotal: 935ms\tremaining: 1.01s\n",
            "24:\tlearn: 0.0141040\ttotal: 970ms\tremaining: 970ms\n",
            "25:\tlearn: 0.0139822\ttotal: 1s\tremaining: 926ms\n",
            "26:\tlearn: 0.0138809\ttotal: 1.04s\tremaining: 884ms\n",
            "27:\tlearn: 0.0137383\ttotal: 1.07s\tremaining: 842ms\n",
            "28:\tlearn: 0.0136486\ttotal: 1.1s\tremaining: 801ms\n",
            "29:\tlearn: 0.0135493\ttotal: 1.15s\tremaining: 765ms\n",
            "30:\tlearn: 0.0134630\ttotal: 1.19s\tremaining: 727ms\n",
            "31:\tlearn: 0.0134033\ttotal: 1.22s\tremaining: 687ms\n",
            "32:\tlearn: 0.0133235\ttotal: 1.26s\tremaining: 647ms\n",
            "33:\tlearn: 0.0132204\ttotal: 1.29s\tremaining: 608ms\n",
            "34:\tlearn: 0.0131421\ttotal: 1.33s\tremaining: 572ms\n",
            "35:\tlearn: 0.0130322\ttotal: 1.38s\tremaining: 538ms\n",
            "36:\tlearn: 0.0129769\ttotal: 1.42s\tremaining: 498ms\n",
            "37:\tlearn: 0.0128866\ttotal: 1.45s\tremaining: 458ms\n",
            "38:\tlearn: 0.0128193\ttotal: 1.49s\tremaining: 419ms\n",
            "39:\tlearn: 0.0127156\ttotal: 1.52s\tremaining: 380ms\n",
            "40:\tlearn: 0.0126860\ttotal: 1.55s\tremaining: 341ms\n",
            "41:\tlearn: 0.0126258\ttotal: 1.6s\tremaining: 304ms\n",
            "42:\tlearn: 0.0125589\ttotal: 1.64s\tremaining: 266ms\n",
            "43:\tlearn: 0.0125317\ttotal: 1.67s\tremaining: 228ms\n",
            "44:\tlearn: 0.0124835\ttotal: 1.71s\tremaining: 190ms\n",
            "45:\tlearn: 0.0124435\ttotal: 1.74s\tremaining: 151ms\n",
            "46:\tlearn: 0.0124079\ttotal: 1.77s\tremaining: 113ms\n",
            "47:\tlearn: 0.0123370\ttotal: 1.82s\tremaining: 75.8ms\n",
            "48:\tlearn: 0.0123079\ttotal: 1.85s\tremaining: 37.9ms\n",
            "49:\tlearn: 0.0122276\ttotal: 1.89s\tremaining: 0us\n",
            "0:\tlearn: 0.2113278\ttotal: 42.1ms\tremaining: 2.06s\n",
            "1:\tlearn: 0.1964489\ttotal: 79.9ms\tremaining: 1.92s\n",
            "2:\tlearn: 0.1846440\ttotal: 119ms\tremaining: 1.86s\n",
            "3:\tlearn: 0.1712146\ttotal: 148ms\tremaining: 1.71s\n",
            "4:\tlearn: 0.1622136\ttotal: 177ms\tremaining: 1.59s\n",
            "5:\tlearn: 0.1530662\ttotal: 204ms\tremaining: 1.5s\n",
            "6:\tlearn: 0.1441532\ttotal: 233ms\tremaining: 1.43s\n",
            "7:\tlearn: 0.1367233\ttotal: 262ms\tremaining: 1.37s\n",
            "8:\tlearn: 0.1321270\ttotal: 298ms\tremaining: 1.36s\n",
            "9:\tlearn: 0.1266618\ttotal: 327ms\tremaining: 1.31s\n",
            "10:\tlearn: 0.1217885\ttotal: 358ms\tremaining: 1.27s\n",
            "11:\tlearn: 0.1165821\ttotal: 397ms\tremaining: 1.26s\n",
            "12:\tlearn: 0.1131089\ttotal: 425ms\tremaining: 1.21s\n",
            "13:\tlearn: 0.1096837\ttotal: 454ms\tremaining: 1.17s\n",
            "14:\tlearn: 0.1061485\ttotal: 482ms\tremaining: 1.12s\n",
            "15:\tlearn: 0.1035032\ttotal: 518ms\tremaining: 1.1s\n",
            "16:\tlearn: 0.1007544\ttotal: 546ms\tremaining: 1.06s\n",
            "17:\tlearn: 0.0983692\ttotal: 574ms\tremaining: 1.02s\n",
            "18:\tlearn: 0.0963115\ttotal: 602ms\tremaining: 982ms\n",
            "19:\tlearn: 0.0947821\ttotal: 630ms\tremaining: 945ms\n",
            "20:\tlearn: 0.0937527\ttotal: 658ms\tremaining: 909ms\n",
            "21:\tlearn: 0.0918194\ttotal: 687ms\tremaining: 875ms\n",
            "22:\tlearn: 0.0908190\ttotal: 714ms\tremaining: 838ms\n",
            "23:\tlearn: 0.0895311\ttotal: 750ms\tremaining: 812ms\n",
            "24:\tlearn: 0.0883815\ttotal: 785ms\tremaining: 785ms\n",
            "25:\tlearn: 0.0873997\ttotal: 814ms\tremaining: 751ms\n",
            "26:\tlearn: 0.0866187\ttotal: 842ms\tremaining: 717ms\n",
            "27:\tlearn: 0.0856819\ttotal: 870ms\tremaining: 684ms\n",
            "28:\tlearn: 0.0840735\ttotal: 898ms\tremaining: 651ms\n",
            "29:\tlearn: 0.0832153\ttotal: 927ms\tremaining: 618ms\n",
            "30:\tlearn: 0.0818563\ttotal: 960ms\tremaining: 589ms\n",
            "31:\tlearn: 0.0813384\ttotal: 990ms\tremaining: 557ms\n",
            "32:\tlearn: 0.0796589\ttotal: 1.02s\tremaining: 526ms\n",
            "33:\tlearn: 0.0786059\ttotal: 1.05s\tremaining: 494ms\n",
            "34:\tlearn: 0.0780383\ttotal: 1.08s\tremaining: 463ms\n",
            "35:\tlearn: 0.0771583\ttotal: 1.11s\tremaining: 431ms\n",
            "36:\tlearn: 0.0763991\ttotal: 1.14s\tremaining: 399ms\n",
            "37:\tlearn: 0.0753567\ttotal: 1.17s\tremaining: 370ms\n",
            "38:\tlearn: 0.0742969\ttotal: 1.2s\tremaining: 339ms\n",
            "39:\tlearn: 0.0737140\ttotal: 1.23s\tremaining: 307ms\n",
            "40:\tlearn: 0.0728509\ttotal: 1.26s\tremaining: 276ms\n",
            "41:\tlearn: 0.0723999\ttotal: 1.28s\tremaining: 245ms\n",
            "42:\tlearn: 0.0718852\ttotal: 1.31s\tremaining: 214ms\n",
            "43:\tlearn: 0.0712965\ttotal: 1.34s\tremaining: 183ms\n",
            "44:\tlearn: 0.0705413\ttotal: 1.39s\tremaining: 154ms\n",
            "45:\tlearn: 0.0698799\ttotal: 1.41s\tremaining: 123ms\n",
            "46:\tlearn: 0.0692192\ttotal: 1.44s\tremaining: 92ms\n",
            "47:\tlearn: 0.0683110\ttotal: 1.47s\tremaining: 61.2ms\n",
            "48:\tlearn: 0.0673455\ttotal: 1.5s\tremaining: 30.5ms\n",
            "49:\tlearn: 0.0669437\ttotal: 1.52s\tremaining: 0us\n",
            "0:\tlearn: 0.0348581\ttotal: 47.2ms\tremaining: 2.31s\n",
            "1:\tlearn: 0.0323137\ttotal: 75ms\tremaining: 1.8s\n",
            "2:\tlearn: 0.0298802\ttotal: 104ms\tremaining: 1.63s\n",
            "3:\tlearn: 0.0280108\ttotal: 136ms\tremaining: 1.56s\n",
            "4:\tlearn: 0.0265121\ttotal: 164ms\tremaining: 1.48s\n",
            "5:\tlearn: 0.0249943\ttotal: 193ms\tremaining: 1.41s\n",
            "6:\tlearn: 0.0236094\ttotal: 220ms\tremaining: 1.35s\n",
            "7:\tlearn: 0.0228534\ttotal: 252ms\tremaining: 1.32s\n",
            "8:\tlearn: 0.0216716\ttotal: 284ms\tremaining: 1.29s\n",
            "9:\tlearn: 0.0206136\ttotal: 312ms\tremaining: 1.25s\n",
            "10:\tlearn: 0.0198496\ttotal: 339ms\tremaining: 1.2s\n",
            "11:\tlearn: 0.0193536\ttotal: 368ms\tremaining: 1.17s\n",
            "12:\tlearn: 0.0185702\ttotal: 396ms\tremaining: 1.13s\n",
            "13:\tlearn: 0.0176301\ttotal: 426ms\tremaining: 1.09s\n",
            "14:\tlearn: 0.0170588\ttotal: 462ms\tremaining: 1.08s\n",
            "15:\tlearn: 0.0163707\ttotal: 493ms\tremaining: 1.05s\n",
            "16:\tlearn: 0.0158831\ttotal: 520ms\tremaining: 1.01s\n",
            "17:\tlearn: 0.0156741\ttotal: 549ms\tremaining: 976ms\n",
            "18:\tlearn: 0.0151314\ttotal: 576ms\tremaining: 940ms\n",
            "19:\tlearn: 0.0147559\ttotal: 604ms\tremaining: 905ms\n",
            "20:\tlearn: 0.0143350\ttotal: 631ms\tremaining: 871ms\n",
            "21:\tlearn: 0.0139383\ttotal: 658ms\tremaining: 838ms\n",
            "22:\tlearn: 0.0136503\ttotal: 699ms\tremaining: 821ms\n",
            "23:\tlearn: 0.0135019\ttotal: 727ms\tremaining: 787ms\n",
            "24:\tlearn: 0.0132078\ttotal: 754ms\tremaining: 754ms\n",
            "25:\tlearn: 0.0130550\ttotal: 782ms\tremaining: 722ms\n",
            "26:\tlearn: 0.0128303\ttotal: 817ms\tremaining: 696ms\n",
            "27:\tlearn: 0.0126635\ttotal: 848ms\tremaining: 667ms\n",
            "28:\tlearn: 0.0123944\ttotal: 886ms\tremaining: 642ms\n",
            "29:\tlearn: 0.0122032\ttotal: 914ms\tremaining: 609ms\n",
            "30:\tlearn: 0.0120039\ttotal: 941ms\tremaining: 577ms\n",
            "31:\tlearn: 0.0118544\ttotal: 969ms\tremaining: 545ms\n",
            "32:\tlearn: 0.0117707\ttotal: 996ms\tremaining: 513ms\n",
            "33:\tlearn: 0.0116603\ttotal: 1.02s\tremaining: 482ms\n",
            "34:\tlearn: 0.0115240\ttotal: 1.05s\tremaining: 451ms\n",
            "35:\tlearn: 0.0113616\ttotal: 1.08s\tremaining: 421ms\n",
            "36:\tlearn: 0.0112381\ttotal: 1.12s\tremaining: 392ms\n",
            "37:\tlearn: 0.0111273\ttotal: 1.14s\tremaining: 361ms\n",
            "38:\tlearn: 0.0110630\ttotal: 1.17s\tremaining: 330ms\n",
            "39:\tlearn: 0.0109644\ttotal: 1.2s\tremaining: 300ms\n",
            "40:\tlearn: 0.0108516\ttotal: 1.23s\tremaining: 269ms\n",
            "41:\tlearn: 0.0107823\ttotal: 1.25s\tremaining: 239ms\n",
            "42:\tlearn: 0.0106871\ttotal: 1.28s\tremaining: 209ms\n",
            "43:\tlearn: 0.0106262\ttotal: 1.31s\tremaining: 179ms\n",
            "44:\tlearn: 0.0105108\ttotal: 1.36s\tremaining: 151ms\n",
            "45:\tlearn: 0.0103826\ttotal: 1.38s\tremaining: 120ms\n",
            "46:\tlearn: 0.0103120\ttotal: 1.41s\tremaining: 90.1ms\n",
            "47:\tlearn: 0.0101760\ttotal: 1.44s\tremaining: 59.9ms\n",
            "48:\tlearn: 0.0101113\ttotal: 1.47s\tremaining: 29.9ms\n",
            "49:\tlearn: 0.0099798\ttotal: 1.49s\tremaining: 0us\n",
            "0:\tlearn: 0.0175114\ttotal: 50.1ms\tremaining: 2.46s\n",
            "1:\tlearn: 0.0164243\ttotal: 77.1ms\tremaining: 1.85s\n",
            "2:\tlearn: 0.0153729\ttotal: 104ms\tremaining: 1.63s\n",
            "3:\tlearn: 0.0142532\ttotal: 131ms\tremaining: 1.5s\n",
            "4:\tlearn: 0.0139091\ttotal: 158ms\tremaining: 1.42s\n",
            "5:\tlearn: 0.0128167\ttotal: 184ms\tremaining: 1.35s\n",
            "6:\tlearn: 0.0121162\ttotal: 213ms\tremaining: 1.31s\n",
            "7:\tlearn: 0.0117409\ttotal: 241ms\tremaining: 1.26s\n",
            "8:\tlearn: 0.0114443\ttotal: 287ms\tremaining: 1.31s\n",
            "9:\tlearn: 0.0111924\ttotal: 325ms\tremaining: 1.3s\n",
            "10:\tlearn: 0.0110640\ttotal: 352ms\tremaining: 1.25s\n",
            "11:\tlearn: 0.0108859\ttotal: 379ms\tremaining: 1.2s\n",
            "12:\tlearn: 0.0107842\ttotal: 407ms\tremaining: 1.16s\n",
            "13:\tlearn: 0.0105287\ttotal: 438ms\tremaining: 1.13s\n",
            "14:\tlearn: 0.0102760\ttotal: 465ms\tremaining: 1.08s\n",
            "15:\tlearn: 0.0102195\ttotal: 501ms\tremaining: 1.06s\n",
            "16:\tlearn: 0.0099084\ttotal: 530ms\tremaining: 1.03s\n",
            "17:\tlearn: 0.0098152\ttotal: 557ms\tremaining: 990ms\n",
            "18:\tlearn: 0.0097305\ttotal: 583ms\tremaining: 952ms\n",
            "19:\tlearn: 0.0093937\ttotal: 611ms\tremaining: 916ms\n",
            "20:\tlearn: 0.0092441\ttotal: 638ms\tremaining: 881ms\n",
            "21:\tlearn: 0.0088528\ttotal: 669ms\tremaining: 851ms\n",
            "22:\tlearn: 0.0086718\ttotal: 695ms\tremaining: 816ms\n",
            "23:\tlearn: 0.0082720\ttotal: 733ms\tremaining: 794ms\n",
            "24:\tlearn: 0.0080479\ttotal: 762ms\tremaining: 762ms\n",
            "25:\tlearn: 0.0079302\ttotal: 789ms\tremaining: 729ms\n",
            "26:\tlearn: 0.0079010\ttotal: 817ms\tremaining: 696ms\n",
            "27:\tlearn: 0.0076420\ttotal: 844ms\tremaining: 663ms\n",
            "28:\tlearn: 0.0073568\ttotal: 871ms\tremaining: 631ms\n",
            "29:\tlearn: 0.0071581\ttotal: 897ms\tremaining: 598ms\n",
            "30:\tlearn: 0.0071319\ttotal: 925ms\tremaining: 567ms\n",
            "31:\tlearn: 0.0071096\ttotal: 959ms\tremaining: 540ms\n",
            "32:\tlearn: 0.0070688\ttotal: 989ms\tremaining: 509ms\n",
            "33:\tlearn: 0.0070217\ttotal: 1.02s\tremaining: 478ms\n",
            "34:\tlearn: 0.0069689\ttotal: 1.04s\tremaining: 447ms\n",
            "35:\tlearn: 0.0067940\ttotal: 1.07s\tremaining: 416ms\n",
            "36:\tlearn: 0.0067690\ttotal: 1.1s\tremaining: 386ms\n",
            "37:\tlearn: 0.0067301\ttotal: 1.13s\tremaining: 355ms\n",
            "38:\tlearn: 0.0066618\ttotal: 1.15s\tremaining: 325ms\n",
            "39:\tlearn: 0.0065930\ttotal: 1.19s\tremaining: 297ms\n",
            "40:\tlearn: 0.0065866\ttotal: 1.22s\tremaining: 267ms\n",
            "41:\tlearn: 0.0065308\ttotal: 1.24s\tremaining: 237ms\n",
            "42:\tlearn: 0.0064880\ttotal: 1.27s\tremaining: 207ms\n",
            "43:\tlearn: 0.0064616\ttotal: 1.31s\tremaining: 179ms\n",
            "44:\tlearn: 0.0064357\ttotal: 1.36s\tremaining: 151ms\n",
            "45:\tlearn: 0.0063574\ttotal: 1.4s\tremaining: 121ms\n",
            "46:\tlearn: 0.0063454\ttotal: 1.43s\tremaining: 91.3ms\n",
            "47:\tlearn: 0.0062559\ttotal: 1.46s\tremaining: 60.7ms\n",
            "48:\tlearn: 0.0062468\ttotal: 1.48s\tremaining: 30.3ms\n",
            "49:\tlearn: 0.0061977\ttotal: 1.51s\tremaining: 0us\n",
            "0:\tlearn: 0.0418187\ttotal: 40.4ms\tremaining: 1.98s\n",
            "1:\tlearn: 0.0386777\ttotal: 79.9ms\tremaining: 1.92s\n",
            "2:\tlearn: 0.0359562\ttotal: 107ms\tremaining: 1.68s\n",
            "3:\tlearn: 0.0333913\ttotal: 134ms\tremaining: 1.54s\n",
            "4:\tlearn: 0.0309104\ttotal: 171ms\tremaining: 1.54s\n",
            "5:\tlearn: 0.0288098\ttotal: 199ms\tremaining: 1.46s\n",
            "6:\tlearn: 0.0271584\ttotal: 226ms\tremaining: 1.39s\n",
            "7:\tlearn: 0.0253345\ttotal: 265ms\tremaining: 1.39s\n",
            "8:\tlearn: 0.0241958\ttotal: 291ms\tremaining: 1.33s\n",
            "9:\tlearn: 0.0228937\ttotal: 319ms\tremaining: 1.27s\n",
            "10:\tlearn: 0.0218051\ttotal: 346ms\tremaining: 1.23s\n",
            "11:\tlearn: 0.0208677\ttotal: 374ms\tremaining: 1.18s\n",
            "12:\tlearn: 0.0203171\ttotal: 401ms\tremaining: 1.14s\n",
            "13:\tlearn: 0.0195471\ttotal: 429ms\tremaining: 1.1s\n",
            "14:\tlearn: 0.0188383\ttotal: 457ms\tremaining: 1.06s\n",
            "15:\tlearn: 0.0179939\ttotal: 492ms\tremaining: 1.05s\n",
            "16:\tlearn: 0.0174036\ttotal: 521ms\tremaining: 1.01s\n",
            "17:\tlearn: 0.0171100\ttotal: 548ms\tremaining: 975ms\n",
            "18:\tlearn: 0.0167102\ttotal: 576ms\tremaining: 940ms\n",
            "19:\tlearn: 0.0162796\ttotal: 603ms\tremaining: 904ms\n",
            "20:\tlearn: 0.0161065\ttotal: 629ms\tremaining: 869ms\n",
            "21:\tlearn: 0.0156740\ttotal: 656ms\tremaining: 835ms\n",
            "22:\tlearn: 0.0151401\ttotal: 683ms\tremaining: 802ms\n",
            "23:\tlearn: 0.0147443\ttotal: 720ms\tremaining: 780ms\n",
            "24:\tlearn: 0.0144252\ttotal: 747ms\tremaining: 747ms\n",
            "25:\tlearn: 0.0142897\ttotal: 774ms\tremaining: 715ms\n",
            "26:\tlearn: 0.0141118\ttotal: 809ms\tremaining: 690ms\n",
            "27:\tlearn: 0.0136875\ttotal: 841ms\tremaining: 661ms\n",
            "28:\tlearn: 0.0132586\ttotal: 868ms\tremaining: 628ms\n",
            "29:\tlearn: 0.0130253\ttotal: 895ms\tremaining: 597ms\n",
            "30:\tlearn: 0.0126616\ttotal: 928ms\tremaining: 569ms\n",
            "31:\tlearn: 0.0124507\ttotal: 956ms\tremaining: 538ms\n",
            "32:\tlearn: 0.0121704\ttotal: 984ms\tremaining: 507ms\n",
            "33:\tlearn: 0.0120509\ttotal: 1.01s\tremaining: 476ms\n",
            "34:\tlearn: 0.0119169\ttotal: 1.04s\tremaining: 445ms\n",
            "35:\tlearn: 0.0116790\ttotal: 1.06s\tremaining: 414ms\n",
            "36:\tlearn: 0.0114788\ttotal: 1.09s\tremaining: 384ms\n",
            "37:\tlearn: 0.0113571\ttotal: 1.12s\tremaining: 353ms\n",
            "38:\tlearn: 0.0111552\ttotal: 1.16s\tremaining: 327ms\n",
            "39:\tlearn: 0.0111181\ttotal: 1.19s\tremaining: 297ms\n",
            "40:\tlearn: 0.0109265\ttotal: 1.22s\tremaining: 267ms\n",
            "41:\tlearn: 0.0107145\ttotal: 1.24s\tremaining: 237ms\n",
            "42:\tlearn: 0.0106201\ttotal: 1.27s\tremaining: 207ms\n",
            "43:\tlearn: 0.0104558\ttotal: 1.3s\tremaining: 177ms\n",
            "44:\tlearn: 0.0103646\ttotal: 1.32s\tremaining: 147ms\n",
            "45:\tlearn: 0.0102903\ttotal: 1.35s\tremaining: 118ms\n",
            "46:\tlearn: 0.0101978\ttotal: 1.39s\tremaining: 88.6ms\n",
            "47:\tlearn: 0.0099781\ttotal: 1.42s\tremaining: 59ms\n",
            "48:\tlearn: 0.0098844\ttotal: 1.44s\tremaining: 29.5ms\n",
            "49:\tlearn: 0.0097655\ttotal: 1.47s\tremaining: 0us\n",
            "0:\tlearn: 0.2277149\ttotal: 48.9ms\tremaining: 2.4s\n",
            "1:\tlearn: 0.2118116\ttotal: 88.5ms\tremaining: 2.12s\n",
            "2:\tlearn: 0.1980207\ttotal: 119ms\tremaining: 1.86s\n",
            "3:\tlearn: 0.1861892\ttotal: 149ms\tremaining: 1.71s\n",
            "4:\tlearn: 0.1757623\ttotal: 179ms\tremaining: 1.61s\n",
            "5:\tlearn: 0.1663660\ttotal: 210ms\tremaining: 1.54s\n",
            "6:\tlearn: 0.1574791\ttotal: 240ms\tremaining: 1.48s\n",
            "7:\tlearn: 0.1509639\ttotal: 296ms\tremaining: 1.55s\n",
            "8:\tlearn: 0.1441445\ttotal: 326ms\tremaining: 1.48s\n",
            "9:\tlearn: 0.1377787\ttotal: 356ms\tremaining: 1.42s\n",
            "10:\tlearn: 0.1307342\ttotal: 386ms\tremaining: 1.37s\n",
            "11:\tlearn: 0.1253617\ttotal: 417ms\tremaining: 1.32s\n",
            "12:\tlearn: 0.1208903\ttotal: 447ms\tremaining: 1.27s\n",
            "13:\tlearn: 0.1166742\ttotal: 478ms\tremaining: 1.23s\n",
            "14:\tlearn: 0.1131802\ttotal: 521ms\tremaining: 1.22s\n",
            "15:\tlearn: 0.1108789\ttotal: 553ms\tremaining: 1.18s\n",
            "16:\tlearn: 0.1078507\ttotal: 584ms\tremaining: 1.13s\n",
            "17:\tlearn: 0.1045623\ttotal: 616ms\tremaining: 1.09s\n",
            "18:\tlearn: 0.1023182\ttotal: 646ms\tremaining: 1.05s\n",
            "19:\tlearn: 0.1004935\ttotal: 678ms\tremaining: 1.02s\n",
            "20:\tlearn: 0.0985145\ttotal: 708ms\tremaining: 978ms\n",
            "21:\tlearn: 0.0972037\ttotal: 749ms\tremaining: 953ms\n",
            "22:\tlearn: 0.0962454\ttotal: 779ms\tremaining: 914ms\n",
            "23:\tlearn: 0.0944352\ttotal: 808ms\tremaining: 876ms\n",
            "24:\tlearn: 0.0934888\ttotal: 839ms\tremaining: 839ms\n",
            "25:\tlearn: 0.0927246\ttotal: 870ms\tremaining: 803ms\n",
            "26:\tlearn: 0.0921300\ttotal: 904ms\tremaining: 770ms\n",
            "27:\tlearn: 0.0914060\ttotal: 935ms\tremaining: 734ms\n",
            "28:\tlearn: 0.0898740\ttotal: 974ms\tremaining: 705ms\n",
            "29:\tlearn: 0.0888902\ttotal: 1s\tremaining: 670ms\n",
            "30:\tlearn: 0.0880657\ttotal: 1.03s\tremaining: 635ms\n",
            "31:\tlearn: 0.0873149\ttotal: 1.07s\tremaining: 600ms\n",
            "32:\tlearn: 0.0867908\ttotal: 1.1s\tremaining: 565ms\n",
            "33:\tlearn: 0.0860940\ttotal: 1.13s\tremaining: 531ms\n",
            "34:\tlearn: 0.0854903\ttotal: 1.16s\tremaining: 496ms\n",
            "35:\tlearn: 0.0849407\ttotal: 1.2s\tremaining: 467ms\n",
            "36:\tlearn: 0.0835303\ttotal: 1.23s\tremaining: 432ms\n",
            "37:\tlearn: 0.0829181\ttotal: 1.26s\tremaining: 398ms\n",
            "38:\tlearn: 0.0826170\ttotal: 1.3s\tremaining: 367ms\n",
            "39:\tlearn: 0.0816306\ttotal: 1.33s\tremaining: 333ms\n",
            "40:\tlearn: 0.0813153\ttotal: 1.36s\tremaining: 300ms\n",
            "41:\tlearn: 0.0805447\ttotal: 1.4s\tremaining: 266ms\n",
            "42:\tlearn: 0.0799517\ttotal: 1.44s\tremaining: 234ms\n",
            "43:\tlearn: 0.0795016\ttotal: 1.47s\tremaining: 201ms\n",
            "44:\tlearn: 0.0791095\ttotal: 1.5s\tremaining: 167ms\n",
            "45:\tlearn: 0.0784845\ttotal: 1.55s\tremaining: 135ms\n",
            "46:\tlearn: 0.0780884\ttotal: 1.58s\tremaining: 101ms\n",
            "47:\tlearn: 0.0773354\ttotal: 1.61s\tremaining: 67.1ms\n",
            "48:\tlearn: 0.0768670\ttotal: 1.64s\tremaining: 33.6ms\n",
            "49:\tlearn: 0.0764295\ttotal: 1.69s\tremaining: 0us\n",
            "0:\tlearn: 0.0384161\ttotal: 44.4ms\tremaining: 2.18s\n",
            "1:\tlearn: 0.0359336\ttotal: 75.9ms\tremaining: 1.82s\n",
            "2:\tlearn: 0.0334905\ttotal: 110ms\tremaining: 1.72s\n",
            "3:\tlearn: 0.0312414\ttotal: 150ms\tremaining: 1.72s\n",
            "4:\tlearn: 0.0290656\ttotal: 180ms\tremaining: 1.62s\n",
            "5:\tlearn: 0.0275287\ttotal: 211ms\tremaining: 1.54s\n",
            "6:\tlearn: 0.0261563\ttotal: 242ms\tremaining: 1.49s\n",
            "7:\tlearn: 0.0247681\ttotal: 272ms\tremaining: 1.43s\n",
            "8:\tlearn: 0.0235074\ttotal: 302ms\tremaining: 1.37s\n",
            "9:\tlearn: 0.0224442\ttotal: 347ms\tremaining: 1.39s\n",
            "10:\tlearn: 0.0213344\ttotal: 376ms\tremaining: 1.33s\n",
            "11:\tlearn: 0.0203030\ttotal: 406ms\tremaining: 1.29s\n",
            "12:\tlearn: 0.0196850\ttotal: 437ms\tremaining: 1.24s\n",
            "13:\tlearn: 0.0188677\ttotal: 466ms\tremaining: 1.2s\n",
            "14:\tlearn: 0.0182600\ttotal: 497ms\tremaining: 1.16s\n",
            "15:\tlearn: 0.0176991\ttotal: 527ms\tremaining: 1.12s\n",
            "16:\tlearn: 0.0171389\ttotal: 571ms\tremaining: 1.11s\n",
            "17:\tlearn: 0.0168887\ttotal: 608ms\tremaining: 1.08s\n",
            "18:\tlearn: 0.0165661\ttotal: 638ms\tremaining: 1.04s\n",
            "19:\tlearn: 0.0161967\ttotal: 668ms\tremaining: 1s\n",
            "20:\tlearn: 0.0157114\ttotal: 698ms\tremaining: 964ms\n",
            "21:\tlearn: 0.0153848\ttotal: 729ms\tremaining: 928ms\n",
            "22:\tlearn: 0.0150026\ttotal: 760ms\tremaining: 892ms\n",
            "23:\tlearn: 0.0146723\ttotal: 801ms\tremaining: 868ms\n",
            "24:\tlearn: 0.0144675\ttotal: 831ms\tremaining: 831ms\n",
            "25:\tlearn: 0.0141855\ttotal: 861ms\tremaining: 795ms\n",
            "26:\tlearn: 0.0140602\ttotal: 891ms\tremaining: 759ms\n",
            "27:\tlearn: 0.0139093\ttotal: 922ms\tremaining: 725ms\n",
            "28:\tlearn: 0.0137007\ttotal: 952ms\tremaining: 690ms\n",
            "29:\tlearn: 0.0134043\ttotal: 983ms\tremaining: 655ms\n",
            "30:\tlearn: 0.0132025\ttotal: 1.02s\tremaining: 626ms\n",
            "31:\tlearn: 0.0130251\ttotal: 1.05s\tremaining: 592ms\n",
            "32:\tlearn: 0.0129301\ttotal: 1.08s\tremaining: 557ms\n",
            "33:\tlearn: 0.0128152\ttotal: 1.11s\tremaining: 523ms\n",
            "34:\tlearn: 0.0127457\ttotal: 1.14s\tremaining: 489ms\n",
            "35:\tlearn: 0.0126798\ttotal: 1.17s\tremaining: 455ms\n",
            "36:\tlearn: 0.0126002\ttotal: 1.2s\tremaining: 422ms\n",
            "37:\tlearn: 0.0125315\ttotal: 1.24s\tremaining: 391ms\n",
            "38:\tlearn: 0.0123718\ttotal: 1.27s\tremaining: 358ms\n",
            "39:\tlearn: 0.0122462\ttotal: 1.3s\tremaining: 325ms\n",
            "40:\tlearn: 0.0121418\ttotal: 1.33s\tremaining: 292ms\n",
            "41:\tlearn: 0.0120296\ttotal: 1.36s\tremaining: 260ms\n",
            "42:\tlearn: 0.0119706\ttotal: 1.39s\tremaining: 227ms\n",
            "43:\tlearn: 0.0118333\ttotal: 1.43s\tremaining: 195ms\n",
            "44:\tlearn: 0.0117923\ttotal: 1.47s\tremaining: 163ms\n",
            "45:\tlearn: 0.0117265\ttotal: 1.5s\tremaining: 130ms\n",
            "46:\tlearn: 0.0116480\ttotal: 1.53s\tremaining: 97.5ms\n",
            "47:\tlearn: 0.0115816\ttotal: 1.56s\tremaining: 64.9ms\n",
            "48:\tlearn: 0.0115238\ttotal: 1.59s\tremaining: 32.5ms\n",
            "49:\tlearn: 0.0114267\ttotal: 1.63s\tremaining: 0us\n",
            "0:\tlearn: 0.0161675\ttotal: 45.7ms\tremaining: 2.24s\n",
            "1:\tlearn: 0.0153887\ttotal: 80ms\tremaining: 1.92s\n",
            "2:\tlearn: 0.0145528\ttotal: 111ms\tremaining: 1.73s\n",
            "3:\tlearn: 0.0140326\ttotal: 141ms\tremaining: 1.62s\n",
            "4:\tlearn: 0.0135782\ttotal: 172ms\tremaining: 1.55s\n",
            "5:\tlearn: 0.0128816\ttotal: 204ms\tremaining: 1.5s\n",
            "6:\tlearn: 0.0126941\ttotal: 235ms\tremaining: 1.44s\n",
            "7:\tlearn: 0.0123464\ttotal: 273ms\tremaining: 1.43s\n",
            "8:\tlearn: 0.0117333\ttotal: 304ms\tremaining: 1.38s\n",
            "9:\tlearn: 0.0114622\ttotal: 335ms\tremaining: 1.34s\n",
            "10:\tlearn: 0.0111001\ttotal: 366ms\tremaining: 1.3s\n",
            "11:\tlearn: 0.0107333\ttotal: 397ms\tremaining: 1.26s\n",
            "12:\tlearn: 0.0106981\ttotal: 428ms\tremaining: 1.22s\n",
            "13:\tlearn: 0.0105175\ttotal: 458ms\tremaining: 1.18s\n",
            "14:\tlearn: 0.0104134\ttotal: 501ms\tremaining: 1.17s\n",
            "15:\tlearn: 0.0103296\ttotal: 535ms\tremaining: 1.14s\n",
            "16:\tlearn: 0.0101467\ttotal: 566ms\tremaining: 1.1s\n",
            "17:\tlearn: 0.0098493\ttotal: 597ms\tremaining: 1.06s\n",
            "18:\tlearn: 0.0096464\ttotal: 627ms\tremaining: 1.02s\n",
            "19:\tlearn: 0.0091929\ttotal: 657ms\tremaining: 985ms\n",
            "20:\tlearn: 0.0091585\ttotal: 687ms\tremaining: 949ms\n",
            "21:\tlearn: 0.0091283\ttotal: 729ms\tremaining: 928ms\n",
            "22:\tlearn: 0.0089464\ttotal: 758ms\tremaining: 890ms\n",
            "23:\tlearn: 0.0088583\ttotal: 788ms\tremaining: 854ms\n",
            "24:\tlearn: 0.0087865\ttotal: 818ms\tremaining: 818ms\n",
            "25:\tlearn: 0.0086982\ttotal: 849ms\tremaining: 784ms\n",
            "26:\tlearn: 0.0086564\ttotal: 879ms\tremaining: 749ms\n",
            "27:\tlearn: 0.0086265\ttotal: 916ms\tremaining: 720ms\n",
            "28:\tlearn: 0.0084410\ttotal: 964ms\tremaining: 698ms\n",
            "29:\tlearn: 0.0083274\ttotal: 994ms\tremaining: 662ms\n",
            "30:\tlearn: 0.0082272\ttotal: 1.02s\tremaining: 628ms\n",
            "31:\tlearn: 0.0081802\ttotal: 1.05s\tremaining: 593ms\n",
            "32:\tlearn: 0.0081534\ttotal: 1.08s\tremaining: 559ms\n",
            "33:\tlearn: 0.0081144\ttotal: 1.11s\tremaining: 525ms\n",
            "34:\tlearn: 0.0080196\ttotal: 1.15s\tremaining: 491ms\n",
            "35:\tlearn: 0.0079098\ttotal: 1.19s\tremaining: 464ms\n",
            "36:\tlearn: 0.0078838\ttotal: 1.22s\tremaining: 430ms\n",
            "37:\tlearn: 0.0078712\ttotal: 1.25s\tremaining: 396ms\n",
            "38:\tlearn: 0.0078276\ttotal: 1.28s\tremaining: 362ms\n",
            "39:\tlearn: 0.0077871\ttotal: 1.31s\tremaining: 329ms\n",
            "40:\tlearn: 0.0077494\ttotal: 1.34s\tremaining: 295ms\n",
            "41:\tlearn: 0.0077362\ttotal: 1.38s\tremaining: 262ms\n",
            "42:\tlearn: 0.0077248\ttotal: 1.42s\tremaining: 231ms\n",
            "43:\tlearn: 0.0075656\ttotal: 1.45s\tremaining: 198ms\n",
            "44:\tlearn: 0.0075387\ttotal: 1.48s\tremaining: 164ms\n",
            "45:\tlearn: 0.0075029\ttotal: 1.51s\tremaining: 131ms\n",
            "46:\tlearn: 0.0074236\ttotal: 1.54s\tremaining: 98.4ms\n",
            "47:\tlearn: 0.0072962\ttotal: 1.57s\tremaining: 65.5ms\n",
            "48:\tlearn: 0.0072894\ttotal: 1.6s\tremaining: 32.7ms\n",
            "49:\tlearn: 0.0072357\ttotal: 1.65s\tremaining: 0us\n",
            "0:\tlearn: 0.0402469\ttotal: 44.8ms\tremaining: 2.19s\n",
            "1:\tlearn: 0.0372085\ttotal: 75.1ms\tremaining: 1.8s\n",
            "2:\tlearn: 0.0345514\ttotal: 106ms\tremaining: 1.66s\n",
            "3:\tlearn: 0.0322839\ttotal: 137ms\tremaining: 1.57s\n",
            "4:\tlearn: 0.0298980\ttotal: 180ms\tremaining: 1.62s\n",
            "5:\tlearn: 0.0279781\ttotal: 213ms\tremaining: 1.56s\n",
            "6:\tlearn: 0.0262456\ttotal: 251ms\tremaining: 1.54s\n",
            "7:\tlearn: 0.0249210\ttotal: 281ms\tremaining: 1.48s\n",
            "8:\tlearn: 0.0233981\ttotal: 312ms\tremaining: 1.42s\n",
            "9:\tlearn: 0.0221769\ttotal: 342ms\tremaining: 1.37s\n",
            "10:\tlearn: 0.0210710\ttotal: 372ms\tremaining: 1.32s\n",
            "11:\tlearn: 0.0201531\ttotal: 412ms\tremaining: 1.3s\n",
            "12:\tlearn: 0.0192974\ttotal: 443ms\tremaining: 1.26s\n",
            "13:\tlearn: 0.0185021\ttotal: 474ms\tremaining: 1.22s\n",
            "14:\tlearn: 0.0179302\ttotal: 505ms\tremaining: 1.18s\n",
            "15:\tlearn: 0.0173384\ttotal: 536ms\tremaining: 1.14s\n",
            "16:\tlearn: 0.0167553\ttotal: 568ms\tremaining: 1.1s\n",
            "17:\tlearn: 0.0162530\ttotal: 599ms\tremaining: 1.06s\n",
            "18:\tlearn: 0.0159141\ttotal: 641ms\tremaining: 1.05s\n",
            "19:\tlearn: 0.0154629\ttotal: 673ms\tremaining: 1.01s\n",
            "20:\tlearn: 0.0151458\ttotal: 704ms\tremaining: 972ms\n",
            "21:\tlearn: 0.0147908\ttotal: 733ms\tremaining: 934ms\n",
            "22:\tlearn: 0.0144824\ttotal: 763ms\tremaining: 896ms\n",
            "23:\tlearn: 0.0141840\ttotal: 793ms\tremaining: 859ms\n",
            "24:\tlearn: 0.0140009\ttotal: 823ms\tremaining: 823ms\n",
            "25:\tlearn: 0.0137738\ttotal: 862ms\tremaining: 795ms\n",
            "26:\tlearn: 0.0136125\ttotal: 892ms\tremaining: 760ms\n",
            "27:\tlearn: 0.0134622\ttotal: 922ms\tremaining: 725ms\n",
            "28:\tlearn: 0.0132846\ttotal: 952ms\tremaining: 689ms\n",
            "29:\tlearn: 0.0130900\ttotal: 984ms\tremaining: 656ms\n",
            "30:\tlearn: 0.0129451\ttotal: 1.01s\tremaining: 622ms\n",
            "31:\tlearn: 0.0128667\ttotal: 1.05s\tremaining: 588ms\n",
            "32:\tlearn: 0.0126834\ttotal: 1.08s\tremaining: 559ms\n",
            "33:\tlearn: 0.0125778\ttotal: 1.12s\tremaining: 526ms\n",
            "34:\tlearn: 0.0125117\ttotal: 1.15s\tremaining: 492ms\n",
            "35:\tlearn: 0.0123675\ttotal: 1.18s\tremaining: 458ms\n",
            "36:\tlearn: 0.0122246\ttotal: 1.21s\tremaining: 424ms\n",
            "37:\tlearn: 0.0121383\ttotal: 1.25s\tremaining: 395ms\n",
            "38:\tlearn: 0.0119690\ttotal: 1.28s\tremaining: 361ms\n",
            "39:\tlearn: 0.0118402\ttotal: 1.32s\tremaining: 330ms\n",
            "40:\tlearn: 0.0117454\ttotal: 1.35s\tremaining: 297ms\n",
            "41:\tlearn: 0.0116445\ttotal: 1.38s\tremaining: 263ms\n",
            "42:\tlearn: 0.0115902\ttotal: 1.41s\tremaining: 230ms\n",
            "43:\tlearn: 0.0115481\ttotal: 1.44s\tremaining: 197ms\n",
            "44:\tlearn: 0.0114847\ttotal: 1.48s\tremaining: 164ms\n",
            "45:\tlearn: 0.0113942\ttotal: 1.5s\tremaining: 131ms\n",
            "46:\tlearn: 0.0113614\ttotal: 1.55s\tremaining: 98.7ms\n",
            "47:\tlearn: 0.0112799\ttotal: 1.58s\tremaining: 65.7ms\n",
            "48:\tlearn: 0.0112038\ttotal: 1.61s\tremaining: 32.8ms\n",
            "49:\tlearn: 0.0111823\ttotal: 1.64s\tremaining: 0us\n",
            "0:\tlearn: 0.2410142\ttotal: 59ms\tremaining: 2.89s\n",
            "1:\tlearn: 0.2235167\ttotal: 92ms\tremaining: 2.21s\n",
            "2:\tlearn: 0.2107854\ttotal: 124ms\tremaining: 1.95s\n",
            "3:\tlearn: 0.1970919\ttotal: 157ms\tremaining: 1.8s\n",
            "4:\tlearn: 0.1852350\ttotal: 189ms\tremaining: 1.7s\n",
            "5:\tlearn: 0.1755025\ttotal: 222ms\tremaining: 1.63s\n",
            "6:\tlearn: 0.1672964\ttotal: 255ms\tremaining: 1.56s\n",
            "7:\tlearn: 0.1576037\ttotal: 297ms\tremaining: 1.56s\n",
            "8:\tlearn: 0.1505619\ttotal: 328ms\tremaining: 1.5s\n",
            "9:\tlearn: 0.1439022\ttotal: 361ms\tremaining: 1.44s\n",
            "10:\tlearn: 0.1376065\ttotal: 393ms\tremaining: 1.39s\n",
            "11:\tlearn: 0.1321981\ttotal: 426ms\tremaining: 1.35s\n",
            "12:\tlearn: 0.1282209\ttotal: 458ms\tremaining: 1.3s\n",
            "13:\tlearn: 0.1235047\ttotal: 490ms\tremaining: 1.26s\n",
            "14:\tlearn: 0.1196782\ttotal: 544ms\tremaining: 1.27s\n",
            "15:\tlearn: 0.1175920\ttotal: 577ms\tremaining: 1.23s\n",
            "16:\tlearn: 0.1149141\ttotal: 609ms\tremaining: 1.18s\n",
            "17:\tlearn: 0.1115786\ttotal: 641ms\tremaining: 1.14s\n",
            "18:\tlearn: 0.1087971\ttotal: 673ms\tremaining: 1.1s\n",
            "19:\tlearn: 0.1068672\ttotal: 706ms\tremaining: 1.06s\n",
            "20:\tlearn: 0.1050491\ttotal: 739ms\tremaining: 1.02s\n",
            "21:\tlearn: 0.1034177\ttotal: 782ms\tremaining: 995ms\n",
            "22:\tlearn: 0.1024705\ttotal: 815ms\tremaining: 957ms\n",
            "23:\tlearn: 0.1005920\ttotal: 847ms\tremaining: 918ms\n",
            "24:\tlearn: 0.0993605\ttotal: 880ms\tremaining: 880ms\n",
            "25:\tlearn: 0.0989026\ttotal: 912ms\tremaining: 841ms\n",
            "26:\tlearn: 0.0981794\ttotal: 944ms\tremaining: 804ms\n",
            "27:\tlearn: 0.0975137\ttotal: 976ms\tremaining: 767ms\n",
            "28:\tlearn: 0.0959477\ttotal: 1.02s\tremaining: 738ms\n",
            "29:\tlearn: 0.0949424\ttotal: 1.05s\tremaining: 701ms\n",
            "30:\tlearn: 0.0941822\ttotal: 1.08s\tremaining: 665ms\n",
            "31:\tlearn: 0.0938534\ttotal: 1.12s\tremaining: 629ms\n",
            "32:\tlearn: 0.0933657\ttotal: 1.15s\tremaining: 592ms\n",
            "33:\tlearn: 0.0920112\ttotal: 1.18s\tremaining: 556ms\n",
            "34:\tlearn: 0.0915520\ttotal: 1.21s\tremaining: 520ms\n",
            "35:\tlearn: 0.0905184\ttotal: 1.26s\tremaining: 491ms\n",
            "36:\tlearn: 0.0897016\ttotal: 1.3s\tremaining: 455ms\n",
            "37:\tlearn: 0.0892570\ttotal: 1.33s\tremaining: 420ms\n",
            "38:\tlearn: 0.0884242\ttotal: 1.36s\tremaining: 384ms\n",
            "39:\tlearn: 0.0879049\ttotal: 1.39s\tremaining: 348ms\n",
            "40:\tlearn: 0.0875174\ttotal: 1.43s\tremaining: 313ms\n",
            "41:\tlearn: 0.0868862\ttotal: 1.46s\tremaining: 278ms\n",
            "42:\tlearn: 0.0865519\ttotal: 1.5s\tremaining: 244ms\n",
            "43:\tlearn: 0.0857462\ttotal: 1.55s\tremaining: 211ms\n",
            "44:\tlearn: 0.0854570\ttotal: 1.58s\tremaining: 176ms\n",
            "45:\tlearn: 0.0852313\ttotal: 1.61s\tremaining: 140ms\n",
            "46:\tlearn: 0.0846735\ttotal: 1.65s\tremaining: 105ms\n",
            "47:\tlearn: 0.0839173\ttotal: 1.68s\tremaining: 70ms\n",
            "48:\tlearn: 0.0834799\ttotal: 1.72s\tremaining: 35.1ms\n",
            "49:\tlearn: 0.0827728\ttotal: 1.75s\tremaining: 0us\n",
            "0:\tlearn: 0.0372421\ttotal: 46.6ms\tremaining: 2.28s\n",
            "1:\tlearn: 0.0347898\ttotal: 78.7ms\tremaining: 1.89s\n",
            "2:\tlearn: 0.0323631\ttotal: 117ms\tremaining: 1.84s\n",
            "3:\tlearn: 0.0301635\ttotal: 152ms\tremaining: 1.74s\n",
            "4:\tlearn: 0.0280749\ttotal: 184ms\tremaining: 1.66s\n",
            "5:\tlearn: 0.0265248\ttotal: 216ms\tremaining: 1.59s\n",
            "6:\tlearn: 0.0251072\ttotal: 248ms\tremaining: 1.52s\n",
            "7:\tlearn: 0.0236600\ttotal: 279ms\tremaining: 1.47s\n",
            "8:\tlearn: 0.0224744\ttotal: 311ms\tremaining: 1.42s\n",
            "9:\tlearn: 0.0215496\ttotal: 353ms\tremaining: 1.41s\n",
            "10:\tlearn: 0.0205121\ttotal: 385ms\tremaining: 1.36s\n",
            "11:\tlearn: 0.0195902\ttotal: 418ms\tremaining: 1.32s\n",
            "12:\tlearn: 0.0187906\ttotal: 451ms\tremaining: 1.28s\n",
            "13:\tlearn: 0.0181891\ttotal: 482ms\tremaining: 1.24s\n",
            "14:\tlearn: 0.0175351\ttotal: 514ms\tremaining: 1.2s\n",
            "15:\tlearn: 0.0168507\ttotal: 546ms\tremaining: 1.16s\n",
            "16:\tlearn: 0.0162720\ttotal: 586ms\tremaining: 1.14s\n",
            "17:\tlearn: 0.0158616\ttotal: 619ms\tremaining: 1.1s\n",
            "18:\tlearn: 0.0154914\ttotal: 653ms\tremaining: 1.06s\n",
            "19:\tlearn: 0.0151244\ttotal: 686ms\tremaining: 1.03s\n",
            "20:\tlearn: 0.0147086\ttotal: 718ms\tremaining: 991ms\n",
            "21:\tlearn: 0.0143275\ttotal: 761ms\tremaining: 968ms\n",
            "22:\tlearn: 0.0141318\ttotal: 802ms\tremaining: 941ms\n",
            "23:\tlearn: 0.0138167\ttotal: 845ms\tremaining: 915ms\n",
            "24:\tlearn: 0.0137084\ttotal: 877ms\tremaining: 877ms\n",
            "25:\tlearn: 0.0134244\ttotal: 912ms\tremaining: 842ms\n",
            "26:\tlearn: 0.0131822\ttotal: 945ms\tremaining: 805ms\n",
            "27:\tlearn: 0.0130925\ttotal: 978ms\tremaining: 768ms\n",
            "28:\tlearn: 0.0128978\ttotal: 1.02s\tremaining: 739ms\n",
            "29:\tlearn: 0.0126964\ttotal: 1.06s\tremaining: 704ms\n",
            "30:\tlearn: 0.0125436\ttotal: 1.09s\tremaining: 667ms\n",
            "31:\tlearn: 0.0124590\ttotal: 1.12s\tremaining: 630ms\n",
            "32:\tlearn: 0.0123980\ttotal: 1.15s\tremaining: 594ms\n",
            "33:\tlearn: 0.0123053\ttotal: 1.18s\tremaining: 557ms\n",
            "34:\tlearn: 0.0122321\ttotal: 1.22s\tremaining: 522ms\n",
            "35:\tlearn: 0.0121762\ttotal: 1.26s\tremaining: 491ms\n",
            "36:\tlearn: 0.0120780\ttotal: 1.29s\tremaining: 455ms\n",
            "37:\tlearn: 0.0119730\ttotal: 1.32s\tremaining: 419ms\n",
            "38:\tlearn: 0.0118574\ttotal: 1.36s\tremaining: 383ms\n",
            "39:\tlearn: 0.0117660\ttotal: 1.39s\tremaining: 348ms\n",
            "40:\tlearn: 0.0117193\ttotal: 1.42s\tremaining: 313ms\n",
            "41:\tlearn: 0.0116530\ttotal: 1.46s\tremaining: 277ms\n",
            "42:\tlearn: 0.0115737\ttotal: 1.5s\tremaining: 244ms\n",
            "43:\tlearn: 0.0114707\ttotal: 1.53s\tremaining: 209ms\n",
            "44:\tlearn: 0.0113697\ttotal: 1.56s\tremaining: 173ms\n",
            "45:\tlearn: 0.0113008\ttotal: 1.59s\tremaining: 139ms\n",
            "46:\tlearn: 0.0112172\ttotal: 1.62s\tremaining: 104ms\n",
            "47:\tlearn: 0.0111672\ttotal: 1.66s\tremaining: 69ms\n",
            "48:\tlearn: 0.0110903\ttotal: 1.69s\tremaining: 34.4ms\n",
            "49:\tlearn: 0.0110010\ttotal: 1.73s\tremaining: 0us\n",
            "0:\tlearn: 0.0169981\ttotal: 46.8ms\tremaining: 2.29s\n",
            "1:\tlearn: 0.0164222\ttotal: 92.4ms\tremaining: 2.22s\n",
            "2:\tlearn: 0.0158232\ttotal: 127ms\tremaining: 2s\n",
            "3:\tlearn: 0.0150530\ttotal: 171ms\tremaining: 1.97s\n",
            "4:\tlearn: 0.0145346\ttotal: 204ms\tremaining: 1.83s\n",
            "5:\tlearn: 0.0140849\ttotal: 236ms\tremaining: 1.73s\n",
            "6:\tlearn: 0.0134048\ttotal: 270ms\tremaining: 1.66s\n",
            "7:\tlearn: 0.0128896\ttotal: 303ms\tremaining: 1.59s\n",
            "8:\tlearn: 0.0123471\ttotal: 337ms\tremaining: 1.53s\n",
            "9:\tlearn: 0.0119020\ttotal: 370ms\tremaining: 1.48s\n",
            "10:\tlearn: 0.0116560\ttotal: 416ms\tremaining: 1.47s\n",
            "11:\tlearn: 0.0113032\ttotal: 449ms\tremaining: 1.42s\n",
            "12:\tlearn: 0.0107611\ttotal: 481ms\tremaining: 1.37s\n",
            "13:\tlearn: 0.0106245\ttotal: 513ms\tremaining: 1.32s\n",
            "14:\tlearn: 0.0105343\ttotal: 546ms\tremaining: 1.27s\n",
            "15:\tlearn: 0.0101343\ttotal: 587ms\tremaining: 1.25s\n",
            "16:\tlearn: 0.0096744\ttotal: 622ms\tremaining: 1.21s\n",
            "17:\tlearn: 0.0095722\ttotal: 654ms\tremaining: 1.16s\n",
            "18:\tlearn: 0.0093094\ttotal: 685ms\tremaining: 1.12s\n",
            "19:\tlearn: 0.0089616\ttotal: 717ms\tremaining: 1.07s\n",
            "20:\tlearn: 0.0089316\ttotal: 748ms\tremaining: 1.03s\n",
            "21:\tlearn: 0.0088866\ttotal: 780ms\tremaining: 992ms\n",
            "22:\tlearn: 0.0087706\ttotal: 821ms\tremaining: 964ms\n",
            "23:\tlearn: 0.0086532\ttotal: 853ms\tremaining: 924ms\n",
            "24:\tlearn: 0.0085238\ttotal: 890ms\tremaining: 890ms\n",
            "25:\tlearn: 0.0085132\ttotal: 923ms\tremaining: 852ms\n",
            "26:\tlearn: 0.0084721\ttotal: 959ms\tremaining: 817ms\n",
            "27:\tlearn: 0.0083676\ttotal: 1s\tremaining: 786ms\n",
            "28:\tlearn: 0.0080993\ttotal: 1.04s\tremaining: 754ms\n",
            "29:\tlearn: 0.0079539\ttotal: 1.07s\tremaining: 715ms\n",
            "30:\tlearn: 0.0079161\ttotal: 1.1s\tremaining: 678ms\n",
            "31:\tlearn: 0.0079037\ttotal: 1.14s\tremaining: 639ms\n",
            "32:\tlearn: 0.0078116\ttotal: 1.17s\tremaining: 602ms\n",
            "33:\tlearn: 0.0077966\ttotal: 1.2s\tremaining: 565ms\n",
            "34:\tlearn: 0.0077075\ttotal: 1.23s\tremaining: 528ms\n",
            "35:\tlearn: 0.0076815\ttotal: 1.27s\tremaining: 495ms\n",
            "36:\tlearn: 0.0075685\ttotal: 1.3s\tremaining: 458ms\n",
            "37:\tlearn: 0.0075252\ttotal: 1.33s\tremaining: 422ms\n",
            "38:\tlearn: 0.0074062\ttotal: 1.37s\tremaining: 386ms\n",
            "39:\tlearn: 0.0073662\ttotal: 1.4s\tremaining: 350ms\n",
            "40:\tlearn: 0.0073622\ttotal: 1.43s\tremaining: 315ms\n",
            "41:\tlearn: 0.0072040\ttotal: 1.46s\tremaining: 279ms\n",
            "42:\tlearn: 0.0071167\ttotal: 1.51s\tremaining: 246ms\n",
            "43:\tlearn: 0.0069990\ttotal: 1.54s\tremaining: 210ms\n",
            "44:\tlearn: 0.0069045\ttotal: 1.57s\tremaining: 175ms\n",
            "45:\tlearn: 0.0068204\ttotal: 1.61s\tremaining: 140ms\n",
            "46:\tlearn: 0.0067349\ttotal: 1.64s\tremaining: 105ms\n",
            "47:\tlearn: 0.0065628\ttotal: 1.67s\tremaining: 69.6ms\n",
            "48:\tlearn: 0.0064751\ttotal: 1.7s\tremaining: 34.7ms\n",
            "49:\tlearn: 0.0064436\ttotal: 1.75s\tremaining: 0us\n",
            "0:\tlearn: 0.0391672\ttotal: 48ms\tremaining: 2.35s\n",
            "1:\tlearn: 0.0361487\ttotal: 81.4ms\tremaining: 1.95s\n",
            "2:\tlearn: 0.0336008\ttotal: 113ms\tremaining: 1.77s\n",
            "3:\tlearn: 0.0313039\ttotal: 150ms\tremaining: 1.73s\n",
            "4:\tlearn: 0.0292078\ttotal: 202ms\tremaining: 1.81s\n",
            "5:\tlearn: 0.0274567\ttotal: 233ms\tremaining: 1.71s\n",
            "6:\tlearn: 0.0262203\ttotal: 266ms\tremaining: 1.63s\n",
            "7:\tlearn: 0.0250239\ttotal: 298ms\tremaining: 1.56s\n",
            "8:\tlearn: 0.0238066\ttotal: 329ms\tremaining: 1.5s\n",
            "9:\tlearn: 0.0226052\ttotal: 368ms\tremaining: 1.47s\n",
            "10:\tlearn: 0.0215422\ttotal: 401ms\tremaining: 1.42s\n",
            "11:\tlearn: 0.0206919\ttotal: 433ms\tremaining: 1.37s\n",
            "12:\tlearn: 0.0198784\ttotal: 466ms\tremaining: 1.32s\n",
            "13:\tlearn: 0.0192566\ttotal: 502ms\tremaining: 1.29s\n",
            "14:\tlearn: 0.0185312\ttotal: 533ms\tremaining: 1.24s\n",
            "15:\tlearn: 0.0179462\ttotal: 566ms\tremaining: 1.2s\n",
            "16:\tlearn: 0.0173886\ttotal: 612ms\tremaining: 1.19s\n",
            "17:\tlearn: 0.0168997\ttotal: 644ms\tremaining: 1.15s\n",
            "18:\tlearn: 0.0164053\ttotal: 677ms\tremaining: 1.1s\n",
            "19:\tlearn: 0.0160555\ttotal: 710ms\tremaining: 1.06s\n",
            "20:\tlearn: 0.0157746\ttotal: 742ms\tremaining: 1.02s\n",
            "21:\tlearn: 0.0155019\ttotal: 777ms\tremaining: 989ms\n",
            "22:\tlearn: 0.0152061\ttotal: 819ms\tremaining: 961ms\n",
            "23:\tlearn: 0.0149531\ttotal: 851ms\tremaining: 922ms\n",
            "24:\tlearn: 0.0147861\ttotal: 884ms\tremaining: 884ms\n",
            "25:\tlearn: 0.0146596\ttotal: 920ms\tremaining: 849ms\n",
            "26:\tlearn: 0.0145599\ttotal: 952ms\tremaining: 811ms\n",
            "27:\tlearn: 0.0144251\ttotal: 990ms\tremaining: 778ms\n",
            "28:\tlearn: 0.0142245\ttotal: 1.02s\tremaining: 740ms\n",
            "29:\tlearn: 0.0140302\ttotal: 1.05s\tremaining: 702ms\n",
            "30:\tlearn: 0.0139598\ttotal: 1.09s\tremaining: 666ms\n",
            "31:\tlearn: 0.0138588\ttotal: 1.12s\tremaining: 630ms\n",
            "32:\tlearn: 0.0137622\ttotal: 1.15s\tremaining: 594ms\n",
            "33:\tlearn: 0.0136774\ttotal: 1.2s\tremaining: 567ms\n",
            "34:\tlearn: 0.0135589\ttotal: 1.24s\tremaining: 531ms\n",
            "35:\tlearn: 0.0134597\ttotal: 1.27s\tremaining: 495ms\n",
            "36:\tlearn: 0.0133413\ttotal: 1.3s\tremaining: 459ms\n",
            "37:\tlearn: 0.0132602\ttotal: 1.34s\tremaining: 423ms\n",
            "38:\tlearn: 0.0131573\ttotal: 1.37s\tremaining: 387ms\n",
            "39:\tlearn: 0.0130898\ttotal: 1.4s\tremaining: 351ms\n",
            "40:\tlearn: 0.0129686\ttotal: 1.44s\tremaining: 317ms\n",
            "41:\tlearn: 0.0129262\ttotal: 1.48s\tremaining: 281ms\n",
            "42:\tlearn: 0.0128859\ttotal: 1.51s\tremaining: 246ms\n",
            "43:\tlearn: 0.0128098\ttotal: 1.54s\tremaining: 211ms\n",
            "44:\tlearn: 0.0127203\ttotal: 1.58s\tremaining: 175ms\n",
            "45:\tlearn: 0.0126387\ttotal: 1.61s\tremaining: 140ms\n",
            "46:\tlearn: 0.0125726\ttotal: 1.65s\tremaining: 106ms\n",
            "47:\tlearn: 0.0124529\ttotal: 1.68s\tremaining: 70.2ms\n",
            "48:\tlearn: 0.0123963\ttotal: 1.72s\tremaining: 35.1ms\n",
            "49:\tlearn: 0.0123280\ttotal: 1.75s\tremaining: 0us\n",
            "0:\tlearn: 0.2551233\ttotal: 49.6ms\tremaining: 2.43s\n",
            "1:\tlearn: 0.2363760\ttotal: 84.5ms\tremaining: 2.03s\n",
            "2:\tlearn: 0.2210819\ttotal: 119ms\tremaining: 1.86s\n",
            "3:\tlearn: 0.2067483\ttotal: 153ms\tremaining: 1.76s\n",
            "4:\tlearn: 0.1950369\ttotal: 188ms\tremaining: 1.69s\n",
            "5:\tlearn: 0.1842059\ttotal: 222ms\tremaining: 1.63s\n",
            "6:\tlearn: 0.1752244\ttotal: 265ms\tremaining: 1.63s\n",
            "7:\tlearn: 0.1677249\ttotal: 300ms\tremaining: 1.57s\n",
            "8:\tlearn: 0.1593960\ttotal: 338ms\tremaining: 1.54s\n",
            "9:\tlearn: 0.1535190\ttotal: 379ms\tremaining: 1.52s\n",
            "10:\tlearn: 0.1474673\ttotal: 414ms\tremaining: 1.47s\n",
            "11:\tlearn: 0.1422680\ttotal: 449ms\tremaining: 1.42s\n",
            "12:\tlearn: 0.1370941\ttotal: 493ms\tremaining: 1.4s\n",
            "13:\tlearn: 0.1326663\ttotal: 534ms\tremaining: 1.37s\n",
            "14:\tlearn: 0.1284082\ttotal: 568ms\tremaining: 1.32s\n",
            "15:\tlearn: 0.1259295\ttotal: 602ms\tremaining: 1.28s\n",
            "16:\tlearn: 0.1231963\ttotal: 635ms\tremaining: 1.23s\n",
            "17:\tlearn: 0.1213254\ttotal: 670ms\tremaining: 1.19s\n",
            "18:\tlearn: 0.1184479\ttotal: 714ms\tremaining: 1.16s\n",
            "19:\tlearn: 0.1162602\ttotal: 748ms\tremaining: 1.12s\n",
            "20:\tlearn: 0.1147474\ttotal: 782ms\tremaining: 1.08s\n",
            "21:\tlearn: 0.1130493\ttotal: 816ms\tremaining: 1.04s\n",
            "22:\tlearn: 0.1117737\ttotal: 850ms\tremaining: 997ms\n",
            "23:\tlearn: 0.1094494\ttotal: 884ms\tremaining: 957ms\n",
            "24:\tlearn: 0.1082148\ttotal: 925ms\tremaining: 925ms\n",
            "25:\tlearn: 0.1067401\ttotal: 960ms\tremaining: 886ms\n",
            "26:\tlearn: 0.1057838\ttotal: 994ms\tremaining: 847ms\n",
            "27:\tlearn: 0.1051551\ttotal: 1.03s\tremaining: 808ms\n",
            "28:\tlearn: 0.1034471\ttotal: 1.06s\tremaining: 769ms\n",
            "29:\tlearn: 0.1025590\ttotal: 1.09s\tremaining: 731ms\n",
            "30:\tlearn: 0.1018930\ttotal: 1.14s\tremaining: 698ms\n",
            "31:\tlearn: 0.1006309\ttotal: 1.17s\tremaining: 660ms\n",
            "32:\tlearn: 0.0994443\ttotal: 1.21s\tremaining: 623ms\n",
            "33:\tlearn: 0.0983991\ttotal: 1.24s\tremaining: 586ms\n",
            "34:\tlearn: 0.0977865\ttotal: 1.28s\tremaining: 549ms\n",
            "35:\tlearn: 0.0972948\ttotal: 1.31s\tremaining: 511ms\n",
            "36:\tlearn: 0.0962313\ttotal: 1.36s\tremaining: 478ms\n",
            "37:\tlearn: 0.0956026\ttotal: 1.41s\tremaining: 444ms\n",
            "38:\tlearn: 0.0948286\ttotal: 1.44s\tremaining: 407ms\n",
            "39:\tlearn: 0.0944200\ttotal: 1.48s\tremaining: 369ms\n",
            "40:\tlearn: 0.0939472\ttotal: 1.51s\tremaining: 331ms\n",
            "41:\tlearn: 0.0933553\ttotal: 1.54s\tremaining: 294ms\n",
            "42:\tlearn: 0.0929820\ttotal: 1.59s\tremaining: 258ms\n",
            "43:\tlearn: 0.0927752\ttotal: 1.62s\tremaining: 221ms\n",
            "44:\tlearn: 0.0918132\ttotal: 1.65s\tremaining: 184ms\n",
            "45:\tlearn: 0.0914575\ttotal: 1.69s\tremaining: 147ms\n",
            "46:\tlearn: 0.0908371\ttotal: 1.72s\tremaining: 110ms\n",
            "47:\tlearn: 0.0906431\ttotal: 1.76s\tremaining: 73.3ms\n",
            "48:\tlearn: 0.0904195\ttotal: 1.8s\tremaining: 36.7ms\n",
            "49:\tlearn: 0.0897069\ttotal: 1.83s\tremaining: 0us\n",
            "0:\tlearn: 0.0371678\ttotal: 51.3ms\tremaining: 2.51s\n",
            "1:\tlearn: 0.0347570\ttotal: 88.2ms\tremaining: 2.12s\n",
            "2:\tlearn: 0.0323733\ttotal: 138ms\tremaining: 2.15s\n",
            "3:\tlearn: 0.0303857\ttotal: 172ms\tremaining: 1.98s\n",
            "4:\tlearn: 0.0283608\ttotal: 207ms\tremaining: 1.86s\n",
            "5:\tlearn: 0.0266463\ttotal: 241ms\tremaining: 1.77s\n",
            "6:\tlearn: 0.0250646\ttotal: 276ms\tremaining: 1.7s\n",
            "7:\tlearn: 0.0234941\ttotal: 310ms\tremaining: 1.63s\n",
            "8:\tlearn: 0.0221759\ttotal: 350ms\tremaining: 1.59s\n",
            "9:\tlearn: 0.0211610\ttotal: 395ms\tremaining: 1.58s\n",
            "10:\tlearn: 0.0200507\ttotal: 429ms\tremaining: 1.52s\n",
            "11:\tlearn: 0.0190639\ttotal: 464ms\tremaining: 1.47s\n",
            "12:\tlearn: 0.0182861\ttotal: 509ms\tremaining: 1.45s\n",
            "13:\tlearn: 0.0175619\ttotal: 544ms\tremaining: 1.4s\n",
            "14:\tlearn: 0.0168606\ttotal: 586ms\tremaining: 1.37s\n",
            "15:\tlearn: 0.0162451\ttotal: 620ms\tremaining: 1.32s\n",
            "16:\tlearn: 0.0156968\ttotal: 654ms\tremaining: 1.27s\n",
            "17:\tlearn: 0.0153051\ttotal: 687ms\tremaining: 1.22s\n",
            "18:\tlearn: 0.0149223\ttotal: 721ms\tremaining: 1.18s\n",
            "19:\tlearn: 0.0145733\ttotal: 756ms\tremaining: 1.13s\n",
            "20:\tlearn: 0.0141859\ttotal: 793ms\tremaining: 1.09s\n",
            "21:\tlearn: 0.0137958\ttotal: 832ms\tremaining: 1.06s\n",
            "22:\tlearn: 0.0135985\ttotal: 864ms\tremaining: 1.01s\n",
            "23:\tlearn: 0.0132877\ttotal: 897ms\tremaining: 971ms\n",
            "24:\tlearn: 0.0131650\ttotal: 929ms\tremaining: 929ms\n",
            "25:\tlearn: 0.0129201\ttotal: 968ms\tremaining: 894ms\n",
            "26:\tlearn: 0.0127104\ttotal: 1.01s\tremaining: 858ms\n",
            "27:\tlearn: 0.0125474\ttotal: 1.04s\tremaining: 819ms\n",
            "28:\tlearn: 0.0123919\ttotal: 1.07s\tremaining: 778ms\n",
            "29:\tlearn: 0.0123138\ttotal: 1.11s\tremaining: 738ms\n",
            "30:\tlearn: 0.0121523\ttotal: 1.14s\tremaining: 698ms\n",
            "31:\tlearn: 0.0120589\ttotal: 1.17s\tremaining: 660ms\n",
            "32:\tlearn: 0.0120082\ttotal: 1.21s\tremaining: 622ms\n",
            "33:\tlearn: 0.0118611\ttotal: 1.25s\tremaining: 588ms\n",
            "34:\tlearn: 0.0117800\ttotal: 1.28s\tremaining: 550ms\n",
            "35:\tlearn: 0.0117062\ttotal: 1.32s\tremaining: 512ms\n",
            "36:\tlearn: 0.0116081\ttotal: 1.35s\tremaining: 474ms\n",
            "37:\tlearn: 0.0114583\ttotal: 1.38s\tremaining: 436ms\n",
            "38:\tlearn: 0.0114158\ttotal: 1.42s\tremaining: 401ms\n",
            "39:\tlearn: 0.0113517\ttotal: 1.46s\tremaining: 364ms\n",
            "40:\tlearn: 0.0113176\ttotal: 1.49s\tremaining: 327ms\n",
            "41:\tlearn: 0.0112475\ttotal: 1.54s\tremaining: 293ms\n",
            "42:\tlearn: 0.0112099\ttotal: 1.57s\tremaining: 256ms\n",
            "43:\tlearn: 0.0111144\ttotal: 1.6s\tremaining: 219ms\n",
            "44:\tlearn: 0.0110338\ttotal: 1.64s\tremaining: 183ms\n",
            "45:\tlearn: 0.0109136\ttotal: 1.68s\tremaining: 146ms\n",
            "46:\tlearn: 0.0108557\ttotal: 1.71s\tremaining: 109ms\n",
            "47:\tlearn: 0.0107766\ttotal: 1.74s\tremaining: 72.6ms\n",
            "48:\tlearn: 0.0107205\ttotal: 1.78s\tremaining: 36.4ms\n",
            "49:\tlearn: 0.0106332\ttotal: 1.81s\tremaining: 0us\n",
            "0:\tlearn: 0.0201265\ttotal: 47.8ms\tremaining: 2.34s\n",
            "1:\tlearn: 0.0193136\ttotal: 82.1ms\tremaining: 1.97s\n",
            "2:\tlearn: 0.0186357\ttotal: 115ms\tremaining: 1.8s\n",
            "3:\tlearn: 0.0177458\ttotal: 148ms\tremaining: 1.7s\n",
            "4:\tlearn: 0.0171960\ttotal: 182ms\tremaining: 1.64s\n",
            "5:\tlearn: 0.0165556\ttotal: 215ms\tremaining: 1.58s\n",
            "6:\tlearn: 0.0156805\ttotal: 249ms\tremaining: 1.53s\n",
            "7:\tlearn: 0.0151591\ttotal: 290ms\tremaining: 1.52s\n",
            "8:\tlearn: 0.0145871\ttotal: 323ms\tremaining: 1.47s\n",
            "9:\tlearn: 0.0142565\ttotal: 357ms\tremaining: 1.43s\n",
            "10:\tlearn: 0.0135747\ttotal: 389ms\tremaining: 1.38s\n",
            "11:\tlearn: 0.0131684\ttotal: 423ms\tremaining: 1.34s\n",
            "12:\tlearn: 0.0126389\ttotal: 460ms\tremaining: 1.31s\n",
            "13:\tlearn: 0.0120660\ttotal: 504ms\tremaining: 1.29s\n",
            "14:\tlearn: 0.0119760\ttotal: 537ms\tremaining: 1.25s\n",
            "15:\tlearn: 0.0115781\ttotal: 572ms\tremaining: 1.22s\n",
            "16:\tlearn: 0.0112279\ttotal: 606ms\tremaining: 1.18s\n",
            "17:\tlearn: 0.0111119\ttotal: 651ms\tremaining: 1.16s\n",
            "18:\tlearn: 0.0107568\ttotal: 691ms\tremaining: 1.13s\n",
            "19:\tlearn: 0.0104732\ttotal: 729ms\tremaining: 1.09s\n",
            "20:\tlearn: 0.0103128\ttotal: 762ms\tremaining: 1.05s\n",
            "21:\tlearn: 0.0102707\ttotal: 797ms\tremaining: 1.01s\n",
            "22:\tlearn: 0.0101989\ttotal: 831ms\tremaining: 976ms\n",
            "23:\tlearn: 0.0100700\ttotal: 864ms\tremaining: 936ms\n",
            "24:\tlearn: 0.0100540\ttotal: 902ms\tremaining: 902ms\n",
            "25:\tlearn: 0.0100006\ttotal: 947ms\tremaining: 874ms\n",
            "26:\tlearn: 0.0099815\ttotal: 980ms\tremaining: 835ms\n",
            "27:\tlearn: 0.0099579\ttotal: 1.01s\tremaining: 796ms\n",
            "28:\tlearn: 0.0096905\ttotal: 1.04s\tremaining: 757ms\n",
            "29:\tlearn: 0.0095073\ttotal: 1.08s\tremaining: 719ms\n",
            "30:\tlearn: 0.0094655\ttotal: 1.12s\tremaining: 688ms\n",
            "31:\tlearn: 0.0094154\ttotal: 1.16s\tremaining: 655ms\n",
            "32:\tlearn: 0.0093156\ttotal: 1.2s\tremaining: 617ms\n",
            "33:\tlearn: 0.0091362\ttotal: 1.23s\tremaining: 580ms\n",
            "34:\tlearn: 0.0091116\ttotal: 1.26s\tremaining: 542ms\n",
            "35:\tlearn: 0.0090630\ttotal: 1.3s\tremaining: 505ms\n",
            "36:\tlearn: 0.0089370\ttotal: 1.34s\tremaining: 471ms\n",
            "37:\tlearn: 0.0088568\ttotal: 1.37s\tremaining: 433ms\n",
            "38:\tlearn: 0.0087434\ttotal: 1.41s\tremaining: 398ms\n",
            "39:\tlearn: 0.0087181\ttotal: 1.44s\tremaining: 361ms\n",
            "40:\tlearn: 0.0087141\ttotal: 1.48s\tremaining: 324ms\n",
            "41:\tlearn: 0.0086959\ttotal: 1.51s\tremaining: 288ms\n",
            "42:\tlearn: 0.0086841\ttotal: 1.55s\tremaining: 253ms\n",
            "43:\tlearn: 0.0086592\ttotal: 1.59s\tremaining: 217ms\n",
            "44:\tlearn: 0.0085826\ttotal: 1.63s\tremaining: 181ms\n",
            "45:\tlearn: 0.0085664\ttotal: 1.67s\tremaining: 145ms\n",
            "46:\tlearn: 0.0085204\ttotal: 1.7s\tremaining: 109ms\n",
            "47:\tlearn: 0.0084447\ttotal: 1.74s\tremaining: 72.5ms\n",
            "48:\tlearn: 0.0084408\ttotal: 1.78s\tremaining: 36.4ms\n",
            "49:\tlearn: 0.0083582\ttotal: 1.81s\tremaining: 0us\n",
            "0:\tlearn: 0.0381969\ttotal: 48.5ms\tremaining: 2.38s\n",
            "1:\tlearn: 0.0353513\ttotal: 83.8ms\tremaining: 2.01s\n",
            "2:\tlearn: 0.0329571\ttotal: 128ms\tremaining: 2s\n",
            "3:\tlearn: 0.0310249\ttotal: 165ms\tremaining: 1.9s\n",
            "4:\tlearn: 0.0289237\ttotal: 199ms\tremaining: 1.79s\n",
            "5:\tlearn: 0.0272059\ttotal: 233ms\tremaining: 1.71s\n",
            "6:\tlearn: 0.0257967\ttotal: 267ms\tremaining: 1.64s\n",
            "7:\tlearn: 0.0242794\ttotal: 301ms\tremaining: 1.58s\n",
            "8:\tlearn: 0.0229739\ttotal: 342ms\tremaining: 1.56s\n",
            "9:\tlearn: 0.0218446\ttotal: 375ms\tremaining: 1.5s\n",
            "10:\tlearn: 0.0208257\ttotal: 408ms\tremaining: 1.45s\n",
            "11:\tlearn: 0.0200126\ttotal: 442ms\tremaining: 1.4s\n",
            "12:\tlearn: 0.0192179\ttotal: 476ms\tremaining: 1.35s\n",
            "13:\tlearn: 0.0184772\ttotal: 508ms\tremaining: 1.31s\n",
            "14:\tlearn: 0.0179328\ttotal: 546ms\tremaining: 1.27s\n",
            "15:\tlearn: 0.0173681\ttotal: 582ms\tremaining: 1.24s\n",
            "16:\tlearn: 0.0169382\ttotal: 615ms\tremaining: 1.19s\n",
            "17:\tlearn: 0.0164762\ttotal: 653ms\tremaining: 1.16s\n",
            "18:\tlearn: 0.0161207\ttotal: 687ms\tremaining: 1.12s\n",
            "19:\tlearn: 0.0157165\ttotal: 720ms\tremaining: 1.08s\n",
            "20:\tlearn: 0.0153738\ttotal: 768ms\tremaining: 1.06s\n",
            "21:\tlearn: 0.0151335\ttotal: 809ms\tremaining: 1.03s\n",
            "22:\tlearn: 0.0149414\ttotal: 842ms\tremaining: 989ms\n",
            "23:\tlearn: 0.0146894\ttotal: 875ms\tremaining: 948ms\n",
            "24:\tlearn: 0.0145170\ttotal: 908ms\tremaining: 908ms\n",
            "25:\tlearn: 0.0143513\ttotal: 945ms\tremaining: 873ms\n",
            "26:\tlearn: 0.0141857\ttotal: 987ms\tremaining: 841ms\n",
            "27:\tlearn: 0.0140539\ttotal: 1.02s\tremaining: 806ms\n",
            "28:\tlearn: 0.0139205\ttotal: 1.06s\tremaining: 766ms\n",
            "29:\tlearn: 0.0138353\ttotal: 1.09s\tremaining: 728ms\n",
            "30:\tlearn: 0.0136984\ttotal: 1.12s\tremaining: 689ms\n",
            "31:\tlearn: 0.0135433\ttotal: 1.16s\tremaining: 651ms\n",
            "32:\tlearn: 0.0134712\ttotal: 1.19s\tremaining: 613ms\n",
            "33:\tlearn: 0.0133686\ttotal: 1.23s\tremaining: 581ms\n",
            "34:\tlearn: 0.0132287\ttotal: 1.27s\tremaining: 543ms\n",
            "35:\tlearn: 0.0131647\ttotal: 1.3s\tremaining: 505ms\n",
            "36:\tlearn: 0.0130610\ttotal: 1.33s\tremaining: 468ms\n",
            "37:\tlearn: 0.0129740\ttotal: 1.36s\tremaining: 431ms\n",
            "38:\tlearn: 0.0129318\ttotal: 1.4s\tremaining: 396ms\n",
            "39:\tlearn: 0.0128398\ttotal: 1.45s\tremaining: 362ms\n",
            "40:\tlearn: 0.0127330\ttotal: 1.48s\tremaining: 325ms\n",
            "41:\tlearn: 0.0126803\ttotal: 1.52s\tremaining: 289ms\n",
            "42:\tlearn: 0.0126068\ttotal: 1.55s\tremaining: 253ms\n",
            "43:\tlearn: 0.0125703\ttotal: 1.59s\tremaining: 216ms\n",
            "44:\tlearn: 0.0125388\ttotal: 1.63s\tremaining: 181ms\n",
            "45:\tlearn: 0.0124529\ttotal: 1.67s\tremaining: 145ms\n",
            "46:\tlearn: 0.0123910\ttotal: 1.7s\tremaining: 109ms\n",
            "47:\tlearn: 0.0122767\ttotal: 1.73s\tremaining: 72.2ms\n",
            "48:\tlearn: 0.0122441\ttotal: 1.77s\tremaining: 36.1ms\n",
            "49:\tlearn: 0.0121791\ttotal: 1.81s\tremaining: 0us\n",
            "0:\tlearn: 0.2808483\ttotal: 49.4ms\tremaining: 2.42s\n",
            "1:\tlearn: 0.2587062\ttotal: 84.5ms\tremaining: 2.03s\n",
            "2:\tlearn: 0.2408396\ttotal: 120ms\tremaining: 1.88s\n",
            "3:\tlearn: 0.2258554\ttotal: 155ms\tremaining: 1.78s\n",
            "4:\tlearn: 0.2109962\ttotal: 190ms\tremaining: 1.71s\n",
            "5:\tlearn: 0.1976009\ttotal: 224ms\tremaining: 1.65s\n",
            "6:\tlearn: 0.1869877\ttotal: 269ms\tremaining: 1.65s\n",
            "7:\tlearn: 0.1780506\ttotal: 304ms\tremaining: 1.59s\n",
            "8:\tlearn: 0.1712118\ttotal: 338ms\tremaining: 1.54s\n",
            "9:\tlearn: 0.1644705\ttotal: 373ms\tremaining: 1.49s\n",
            "10:\tlearn: 0.1561862\ttotal: 409ms\tremaining: 1.45s\n",
            "11:\tlearn: 0.1506344\ttotal: 445ms\tremaining: 1.41s\n",
            "12:\tlearn: 0.1455082\ttotal: 486ms\tremaining: 1.38s\n",
            "13:\tlearn: 0.1400744\ttotal: 521ms\tremaining: 1.34s\n",
            "14:\tlearn: 0.1356205\ttotal: 555ms\tremaining: 1.29s\n",
            "15:\tlearn: 0.1313239\ttotal: 590ms\tremaining: 1.25s\n",
            "16:\tlearn: 0.1284695\ttotal: 624ms\tremaining: 1.21s\n",
            "17:\tlearn: 0.1247035\ttotal: 658ms\tremaining: 1.17s\n",
            "18:\tlearn: 0.1216529\ttotal: 697ms\tremaining: 1.14s\n",
            "19:\tlearn: 0.1197942\ttotal: 741ms\tremaining: 1.11s\n",
            "20:\tlearn: 0.1172915\ttotal: 781ms\tremaining: 1.08s\n",
            "21:\tlearn: 0.1145971\ttotal: 816ms\tremaining: 1.04s\n",
            "22:\tlearn: 0.1134334\ttotal: 850ms\tremaining: 997ms\n",
            "23:\tlearn: 0.1119403\ttotal: 894ms\tremaining: 969ms\n",
            "24:\tlearn: 0.1109820\ttotal: 939ms\tremaining: 939ms\n",
            "25:\tlearn: 0.1096461\ttotal: 972ms\tremaining: 897ms\n",
            "26:\tlearn: 0.1086632\ttotal: 1.01s\tremaining: 857ms\n",
            "27:\tlearn: 0.1080464\ttotal: 1.04s\tremaining: 817ms\n",
            "28:\tlearn: 0.1071750\ttotal: 1.07s\tremaining: 777ms\n",
            "29:\tlearn: 0.1062414\ttotal: 1.11s\tremaining: 738ms\n",
            "30:\tlearn: 0.1055667\ttotal: 1.15s\tremaining: 703ms\n",
            "31:\tlearn: 0.1043580\ttotal: 1.18s\tremaining: 666ms\n",
            "32:\tlearn: 0.1038857\ttotal: 1.22s\tremaining: 627ms\n",
            "33:\tlearn: 0.1026392\ttotal: 1.25s\tremaining: 589ms\n",
            "34:\tlearn: 0.1018201\ttotal: 1.28s\tremaining: 551ms\n",
            "35:\tlearn: 0.1009935\ttotal: 1.32s\tremaining: 514ms\n",
            "36:\tlearn: 0.1004125\ttotal: 1.36s\tremaining: 480ms\n",
            "37:\tlearn: 0.0997441\ttotal: 1.4s\tremaining: 443ms\n",
            "38:\tlearn: 0.0989448\ttotal: 1.44s\tremaining: 405ms\n",
            "39:\tlearn: 0.0985246\ttotal: 1.47s\tremaining: 368ms\n",
            "40:\tlearn: 0.0982448\ttotal: 1.51s\tremaining: 331ms\n",
            "41:\tlearn: 0.0979113\ttotal: 1.54s\tremaining: 294ms\n",
            "42:\tlearn: 0.0974296\ttotal: 1.58s\tremaining: 258ms\n",
            "43:\tlearn: 0.0972827\ttotal: 1.63s\tremaining: 222ms\n",
            "44:\tlearn: 0.0968752\ttotal: 1.66s\tremaining: 185ms\n",
            "45:\tlearn: 0.0965045\ttotal: 1.7s\tremaining: 148ms\n",
            "46:\tlearn: 0.0961659\ttotal: 1.73s\tremaining: 111ms\n",
            "47:\tlearn: 0.0953978\ttotal: 1.77s\tremaining: 73.7ms\n",
            "48:\tlearn: 0.0951544\ttotal: 1.81s\tremaining: 37ms\n",
            "49:\tlearn: 0.0949768\ttotal: 1.84s\tremaining: 0us\n",
            "0:\tlearn: 0.0392334\ttotal: 49.5ms\tremaining: 2.42s\n",
            "1:\tlearn: 0.0365584\ttotal: 90.2ms\tremaining: 2.17s\n",
            "2:\tlearn: 0.0340335\ttotal: 131ms\tremaining: 2.05s\n",
            "3:\tlearn: 0.0315755\ttotal: 165ms\tremaining: 1.9s\n",
            "4:\tlearn: 0.0293192\ttotal: 199ms\tremaining: 1.79s\n",
            "5:\tlearn: 0.0276951\ttotal: 233ms\tremaining: 1.71s\n",
            "6:\tlearn: 0.0262923\ttotal: 267ms\tremaining: 1.64s\n",
            "7:\tlearn: 0.0247113\ttotal: 308ms\tremaining: 1.62s\n",
            "8:\tlearn: 0.0234356\ttotal: 342ms\tremaining: 1.56s\n",
            "9:\tlearn: 0.0220422\ttotal: 377ms\tremaining: 1.51s\n",
            "10:\tlearn: 0.0209887\ttotal: 412ms\tremaining: 1.46s\n",
            "11:\tlearn: 0.0199853\ttotal: 447ms\tremaining: 1.42s\n",
            "12:\tlearn: 0.0193407\ttotal: 481ms\tremaining: 1.37s\n",
            "13:\tlearn: 0.0187102\ttotal: 523ms\tremaining: 1.34s\n",
            "14:\tlearn: 0.0180313\ttotal: 558ms\tremaining: 1.3s\n",
            "15:\tlearn: 0.0173919\ttotal: 592ms\tremaining: 1.26s\n",
            "16:\tlearn: 0.0167586\ttotal: 629ms\tremaining: 1.22s\n",
            "17:\tlearn: 0.0162432\ttotal: 664ms\tremaining: 1.18s\n",
            "18:\tlearn: 0.0158724\ttotal: 698ms\tremaining: 1.14s\n",
            "19:\tlearn: 0.0153734\ttotal: 742ms\tremaining: 1.11s\n",
            "20:\tlearn: 0.0149816\ttotal: 778ms\tremaining: 1.07s\n",
            "21:\tlearn: 0.0145244\ttotal: 814ms\tremaining: 1.03s\n",
            "22:\tlearn: 0.0141778\ttotal: 848ms\tremaining: 996ms\n",
            "23:\tlearn: 0.0138751\ttotal: 882ms\tremaining: 955ms\n",
            "24:\tlearn: 0.0137346\ttotal: 915ms\tremaining: 915ms\n",
            "25:\tlearn: 0.0134083\ttotal: 957ms\tremaining: 883ms\n",
            "26:\tlearn: 0.0131657\ttotal: 1s\tremaining: 857ms\n",
            "27:\tlearn: 0.0129418\ttotal: 1.04s\tremaining: 817ms\n",
            "28:\tlearn: 0.0127810\ttotal: 1.07s\tremaining: 778ms\n",
            "29:\tlearn: 0.0125943\ttotal: 1.11s\tremaining: 740ms\n",
            "30:\tlearn: 0.0124292\ttotal: 1.14s\tremaining: 701ms\n",
            "31:\tlearn: 0.0123289\ttotal: 1.19s\tremaining: 669ms\n",
            "32:\tlearn: 0.0122753\ttotal: 1.22s\tremaining: 630ms\n",
            "33:\tlearn: 0.0121799\ttotal: 1.26s\tremaining: 591ms\n",
            "34:\tlearn: 0.0121106\ttotal: 1.29s\tremaining: 553ms\n",
            "35:\tlearn: 0.0120026\ttotal: 1.32s\tremaining: 515ms\n",
            "36:\tlearn: 0.0119222\ttotal: 1.36s\tremaining: 477ms\n",
            "37:\tlearn: 0.0118167\ttotal: 1.4s\tremaining: 441ms\n",
            "38:\tlearn: 0.0117792\ttotal: 1.43s\tremaining: 404ms\n",
            "39:\tlearn: 0.0117218\ttotal: 1.47s\tremaining: 367ms\n",
            "40:\tlearn: 0.0116363\ttotal: 1.5s\tremaining: 330ms\n",
            "41:\tlearn: 0.0115769\ttotal: 1.54s\tremaining: 293ms\n",
            "42:\tlearn: 0.0115384\ttotal: 1.57s\tremaining: 256ms\n",
            "43:\tlearn: 0.0114637\ttotal: 1.61s\tremaining: 220ms\n",
            "44:\tlearn: 0.0114211\ttotal: 1.65s\tremaining: 184ms\n",
            "45:\tlearn: 0.0113479\ttotal: 1.69s\tremaining: 147ms\n",
            "46:\tlearn: 0.0112749\ttotal: 1.73s\tremaining: 110ms\n",
            "47:\tlearn: 0.0112097\ttotal: 1.76s\tremaining: 73.4ms\n",
            "48:\tlearn: 0.0111483\ttotal: 1.8s\tremaining: 36.7ms\n",
            "49:\tlearn: 0.0110723\ttotal: 1.84s\tremaining: 0us\n",
            "0:\tlearn: 0.0252322\ttotal: 49.9ms\tremaining: 2.44s\n",
            "1:\tlearn: 0.0240210\ttotal: 96.7ms\tremaining: 2.32s\n",
            "2:\tlearn: 0.0229613\ttotal: 137ms\tremaining: 2.15s\n",
            "3:\tlearn: 0.0216295\ttotal: 173ms\tremaining: 1.99s\n",
            "4:\tlearn: 0.0205896\ttotal: 208ms\tremaining: 1.87s\n",
            "5:\tlearn: 0.0194020\ttotal: 242ms\tremaining: 1.77s\n",
            "6:\tlearn: 0.0186120\ttotal: 282ms\tremaining: 1.73s\n",
            "7:\tlearn: 0.0175385\ttotal: 316ms\tremaining: 1.66s\n",
            "8:\tlearn: 0.0166829\ttotal: 359ms\tremaining: 1.63s\n",
            "9:\tlearn: 0.0164638\ttotal: 393ms\tremaining: 1.57s\n",
            "10:\tlearn: 0.0158045\ttotal: 426ms\tremaining: 1.51s\n",
            "11:\tlearn: 0.0150578\ttotal: 461ms\tremaining: 1.46s\n",
            "12:\tlearn: 0.0144552\ttotal: 494ms\tremaining: 1.41s\n",
            "13:\tlearn: 0.0138859\ttotal: 528ms\tremaining: 1.36s\n",
            "14:\tlearn: 0.0134898\ttotal: 569ms\tremaining: 1.33s\n",
            "15:\tlearn: 0.0130550\ttotal: 604ms\tremaining: 1.28s\n",
            "16:\tlearn: 0.0126218\ttotal: 639ms\tremaining: 1.24s\n",
            "17:\tlearn: 0.0125283\ttotal: 673ms\tremaining: 1.2s\n",
            "18:\tlearn: 0.0122379\ttotal: 707ms\tremaining: 1.15s\n",
            "19:\tlearn: 0.0119053\ttotal: 743ms\tremaining: 1.11s\n",
            "20:\tlearn: 0.0116643\ttotal: 787ms\tremaining: 1.09s\n",
            "21:\tlearn: 0.0113756\ttotal: 826ms\tremaining: 1.05s\n",
            "22:\tlearn: 0.0113380\ttotal: 860ms\tremaining: 1.01s\n",
            "23:\tlearn: 0.0111981\ttotal: 894ms\tremaining: 968ms\n",
            "24:\tlearn: 0.0110847\ttotal: 931ms\tremaining: 931ms\n",
            "25:\tlearn: 0.0110261\ttotal: 964ms\tremaining: 890ms\n",
            "26:\tlearn: 0.0110110\ttotal: 1.01s\tremaining: 860ms\n",
            "27:\tlearn: 0.0109930\ttotal: 1.04s\tremaining: 821ms\n",
            "28:\tlearn: 0.0108234\ttotal: 1.08s\tremaining: 786ms\n",
            "29:\tlearn: 0.0106784\ttotal: 1.12s\tremaining: 749ms\n",
            "30:\tlearn: 0.0106026\ttotal: 1.16s\tremaining: 709ms\n",
            "31:\tlearn: 0.0105324\ttotal: 1.19s\tremaining: 670ms\n",
            "32:\tlearn: 0.0104698\ttotal: 1.23s\tremaining: 636ms\n",
            "33:\tlearn: 0.0103911\ttotal: 1.27s\tremaining: 598ms\n",
            "34:\tlearn: 0.0103199\ttotal: 1.3s\tremaining: 559ms\n",
            "35:\tlearn: 0.0103076\ttotal: 1.34s\tremaining: 521ms\n",
            "36:\tlearn: 0.0102034\ttotal: 1.37s\tremaining: 482ms\n",
            "37:\tlearn: 0.0101706\ttotal: 1.41s\tremaining: 444ms\n",
            "38:\tlearn: 0.0100374\ttotal: 1.45s\tremaining: 408ms\n",
            "39:\tlearn: 0.0098904\ttotal: 1.48s\tremaining: 371ms\n",
            "40:\tlearn: 0.0098805\ttotal: 1.52s\tremaining: 334ms\n",
            "41:\tlearn: 0.0098605\ttotal: 1.55s\tremaining: 296ms\n",
            "42:\tlearn: 0.0097457\ttotal: 1.59s\tremaining: 259ms\n",
            "43:\tlearn: 0.0096043\ttotal: 1.63s\tremaining: 222ms\n",
            "44:\tlearn: 0.0095463\ttotal: 1.67s\tremaining: 186ms\n",
            "45:\tlearn: 0.0094687\ttotal: 1.7s\tremaining: 148ms\n",
            "46:\tlearn: 0.0094389\ttotal: 1.74s\tremaining: 111ms\n",
            "47:\tlearn: 0.0093836\ttotal: 1.77s\tremaining: 73.9ms\n",
            "48:\tlearn: 0.0092658\ttotal: 1.81s\tremaining: 36.9ms\n",
            "49:\tlearn: 0.0092209\ttotal: 1.84s\tremaining: 0us\n",
            "0:\tlearn: 0.0415901\ttotal: 48.5ms\tremaining: 2.38s\n",
            "1:\tlearn: 0.0384810\ttotal: 83ms\tremaining: 1.99s\n",
            "2:\tlearn: 0.0360217\ttotal: 117ms\tremaining: 1.83s\n",
            "3:\tlearn: 0.0335322\ttotal: 150ms\tremaining: 1.73s\n",
            "4:\tlearn: 0.0312757\ttotal: 197ms\tremaining: 1.77s\n",
            "5:\tlearn: 0.0294491\ttotal: 231ms\tremaining: 1.69s\n",
            "6:\tlearn: 0.0278258\ttotal: 275ms\tremaining: 1.69s\n",
            "7:\tlearn: 0.0261815\ttotal: 310ms\tremaining: 1.63s\n",
            "8:\tlearn: 0.0248215\ttotal: 344ms\tremaining: 1.57s\n",
            "9:\tlearn: 0.0234495\ttotal: 379ms\tremaining: 1.52s\n",
            "10:\tlearn: 0.0223734\ttotal: 414ms\tremaining: 1.47s\n",
            "11:\tlearn: 0.0214784\ttotal: 448ms\tremaining: 1.42s\n",
            "12:\tlearn: 0.0205541\ttotal: 494ms\tremaining: 1.41s\n",
            "13:\tlearn: 0.0197428\ttotal: 528ms\tremaining: 1.36s\n",
            "14:\tlearn: 0.0191012\ttotal: 562ms\tremaining: 1.31s\n",
            "15:\tlearn: 0.0185007\ttotal: 596ms\tremaining: 1.26s\n",
            "16:\tlearn: 0.0179423\ttotal: 630ms\tremaining: 1.22s\n",
            "17:\tlearn: 0.0173924\ttotal: 664ms\tremaining: 1.18s\n",
            "18:\tlearn: 0.0169470\ttotal: 700ms\tremaining: 1.14s\n",
            "19:\tlearn: 0.0165559\ttotal: 743ms\tremaining: 1.11s\n",
            "20:\tlearn: 0.0162206\ttotal: 778ms\tremaining: 1.07s\n",
            "21:\tlearn: 0.0158742\ttotal: 813ms\tremaining: 1.03s\n",
            "22:\tlearn: 0.0156058\ttotal: 847ms\tremaining: 994ms\n",
            "23:\tlearn: 0.0153328\ttotal: 882ms\tremaining: 955ms\n",
            "24:\tlearn: 0.0151651\ttotal: 925ms\tremaining: 925ms\n",
            "25:\tlearn: 0.0149618\ttotal: 959ms\tremaining: 885ms\n",
            "26:\tlearn: 0.0148147\ttotal: 992ms\tremaining: 845ms\n",
            "27:\tlearn: 0.0146971\ttotal: 1.02s\tremaining: 806ms\n",
            "28:\tlearn: 0.0145403\ttotal: 1.06s\tremaining: 767ms\n",
            "29:\tlearn: 0.0144631\ttotal: 1.09s\tremaining: 728ms\n",
            "30:\tlearn: 0.0143498\ttotal: 1.13s\tremaining: 693ms\n",
            "31:\tlearn: 0.0142701\ttotal: 1.17s\tremaining: 659ms\n",
            "32:\tlearn: 0.0141690\ttotal: 1.21s\tremaining: 625ms\n",
            "33:\tlearn: 0.0140886\ttotal: 1.25s\tremaining: 587ms\n",
            "34:\tlearn: 0.0139945\ttotal: 1.28s\tremaining: 549ms\n",
            "35:\tlearn: 0.0139304\ttotal: 1.31s\tremaining: 512ms\n",
            "36:\tlearn: 0.0138289\ttotal: 1.36s\tremaining: 477ms\n",
            "37:\tlearn: 0.0137568\ttotal: 1.39s\tremaining: 440ms\n",
            "38:\tlearn: 0.0137044\ttotal: 1.43s\tremaining: 402ms\n",
            "39:\tlearn: 0.0136271\ttotal: 1.46s\tremaining: 365ms\n",
            "40:\tlearn: 0.0135316\ttotal: 1.49s\tremaining: 328ms\n",
            "41:\tlearn: 0.0134492\ttotal: 1.53s\tremaining: 291ms\n",
            "42:\tlearn: 0.0133817\ttotal: 1.57s\tremaining: 256ms\n",
            "43:\tlearn: 0.0133352\ttotal: 1.6s\tremaining: 219ms\n",
            "44:\tlearn: 0.0132719\ttotal: 1.64s\tremaining: 182ms\n",
            "45:\tlearn: 0.0132297\ttotal: 1.67s\tremaining: 145ms\n",
            "46:\tlearn: 0.0131984\ttotal: 1.71s\tremaining: 109ms\n",
            "47:\tlearn: 0.0131173\ttotal: 1.74s\tremaining: 72.5ms\n",
            "48:\tlearn: 0.0130267\ttotal: 1.78s\tremaining: 36.4ms\n",
            "49:\tlearn: 0.0129592\ttotal: 1.82s\tremaining: 0us\n",
            "0:\tlearn: 0.1817538\ttotal: 150ms\tremaining: 7.37s\n",
            "1:\tlearn: 0.1519718\ttotal: 245ms\tremaining: 5.89s\n",
            "2:\tlearn: 0.1309733\ttotal: 349ms\tremaining: 5.46s\n",
            "3:\tlearn: 0.1157598\ttotal: 451ms\tremaining: 5.18s\n",
            "4:\tlearn: 0.1061801\ttotal: 545ms\tremaining: 4.9s\n",
            "5:\tlearn: 0.0968111\ttotal: 639ms\tremaining: 4.69s\n",
            "6:\tlearn: 0.0912586\ttotal: 741ms\tremaining: 4.55s\n",
            "7:\tlearn: 0.0861286\ttotal: 833ms\tremaining: 4.37s\n",
            "8:\tlearn: 0.0811542\ttotal: 931ms\tremaining: 4.24s\n",
            "9:\tlearn: 0.0787803\ttotal: 1.03s\tremaining: 4.14s\n",
            "10:\tlearn: 0.0749235\ttotal: 1.13s\tremaining: 4.01s\n",
            "11:\tlearn: 0.0724134\ttotal: 1.23s\tremaining: 3.91s\n",
            "12:\tlearn: 0.0697955\ttotal: 1.34s\tremaining: 3.81s\n",
            "13:\tlearn: 0.0682595\ttotal: 1.44s\tremaining: 3.71s\n",
            "14:\tlearn: 0.0658199\ttotal: 1.53s\tremaining: 3.58s\n",
            "15:\tlearn: 0.0641051\ttotal: 1.64s\tremaining: 3.49s\n",
            "16:\tlearn: 0.0630605\ttotal: 1.73s\tremaining: 3.37s\n",
            "17:\tlearn: 0.0618300\ttotal: 1.83s\tremaining: 3.25s\n",
            "18:\tlearn: 0.0586919\ttotal: 1.93s\tremaining: 3.15s\n",
            "19:\tlearn: 0.0575500\ttotal: 2.03s\tremaining: 3.05s\n",
            "20:\tlearn: 0.0554569\ttotal: 2.13s\tremaining: 2.94s\n",
            "21:\tlearn: 0.0531171\ttotal: 2.24s\tremaining: 2.85s\n",
            "22:\tlearn: 0.0505110\ttotal: 2.33s\tremaining: 2.74s\n",
            "23:\tlearn: 0.0475852\ttotal: 2.44s\tremaining: 2.64s\n",
            "24:\tlearn: 0.0469378\ttotal: 2.54s\tremaining: 2.54s\n",
            "25:\tlearn: 0.0458799\ttotal: 2.63s\tremaining: 2.43s\n",
            "26:\tlearn: 0.0447480\ttotal: 2.73s\tremaining: 2.33s\n",
            "27:\tlearn: 0.0440750\ttotal: 2.83s\tremaining: 2.23s\n",
            "28:\tlearn: 0.0426714\ttotal: 2.92s\tremaining: 2.12s\n",
            "29:\tlearn: 0.0412879\ttotal: 3.03s\tremaining: 2.02s\n",
            "30:\tlearn: 0.0399274\ttotal: 3.13s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0386590\ttotal: 3.22s\tremaining: 1.81s\n",
            "32:\tlearn: 0.0377664\ttotal: 3.33s\tremaining: 1.71s\n",
            "33:\tlearn: 0.0366125\ttotal: 3.43s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0359189\ttotal: 3.53s\tremaining: 1.51s\n",
            "35:\tlearn: 0.0345745\ttotal: 3.63s\tremaining: 1.41s\n",
            "36:\tlearn: 0.0334455\ttotal: 3.73s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0331227\ttotal: 3.83s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0322516\ttotal: 3.92s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0311817\ttotal: 4.03s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0304372\ttotal: 4.13s\tremaining: 907ms\n",
            "41:\tlearn: 0.0301296\ttotal: 4.22s\tremaining: 805ms\n",
            "42:\tlearn: 0.0293129\ttotal: 4.33s\tremaining: 705ms\n",
            "43:\tlearn: 0.0289798\ttotal: 4.43s\tremaining: 605ms\n",
            "44:\tlearn: 0.0283378\ttotal: 4.53s\tremaining: 503ms\n",
            "45:\tlearn: 0.0279995\ttotal: 4.63s\tremaining: 403ms\n",
            "46:\tlearn: 0.0272112\ttotal: 4.72s\tremaining: 301ms\n",
            "47:\tlearn: 0.0267294\ttotal: 4.82s\tremaining: 201ms\n",
            "48:\tlearn: 0.0256971\ttotal: 4.92s\tremaining: 100ms\n",
            "49:\tlearn: 0.0250186\ttotal: 5.01s\tremaining: 0us\n",
            "0:\tlearn: 0.0298324\ttotal: 137ms\tremaining: 6.69s\n",
            "1:\tlearn: 0.0245804\ttotal: 229ms\tremaining: 5.5s\n",
            "2:\tlearn: 0.0209160\ttotal: 322ms\tremaining: 5.04s\n",
            "3:\tlearn: 0.0191460\ttotal: 433ms\tremaining: 4.98s\n",
            "4:\tlearn: 0.0168777\ttotal: 524ms\tremaining: 4.72s\n",
            "5:\tlearn: 0.0149874\ttotal: 617ms\tremaining: 4.52s\n",
            "6:\tlearn: 0.0136467\ttotal: 721ms\tremaining: 4.43s\n",
            "7:\tlearn: 0.0125154\ttotal: 814ms\tremaining: 4.28s\n",
            "8:\tlearn: 0.0118984\ttotal: 911ms\tremaining: 4.15s\n",
            "9:\tlearn: 0.0113098\ttotal: 1.02s\tremaining: 4.08s\n",
            "10:\tlearn: 0.0108620\ttotal: 1.11s\tremaining: 3.95s\n",
            "11:\tlearn: 0.0101510\ttotal: 1.21s\tremaining: 3.82s\n",
            "12:\tlearn: 0.0096857\ttotal: 1.31s\tremaining: 3.74s\n",
            "13:\tlearn: 0.0093822\ttotal: 1.42s\tremaining: 3.65s\n",
            "14:\tlearn: 0.0090034\ttotal: 1.51s\tremaining: 3.52s\n",
            "15:\tlearn: 0.0087000\ttotal: 1.62s\tremaining: 3.44s\n",
            "16:\tlearn: 0.0084800\ttotal: 1.72s\tremaining: 3.33s\n",
            "17:\tlearn: 0.0082514\ttotal: 1.81s\tremaining: 3.22s\n",
            "18:\tlearn: 0.0078496\ttotal: 1.92s\tremaining: 3.13s\n",
            "19:\tlearn: 0.0076283\ttotal: 2.01s\tremaining: 3.01s\n",
            "20:\tlearn: 0.0072944\ttotal: 2.11s\tremaining: 2.91s\n",
            "21:\tlearn: 0.0069363\ttotal: 2.21s\tremaining: 2.81s\n",
            "22:\tlearn: 0.0067877\ttotal: 2.31s\tremaining: 2.71s\n",
            "23:\tlearn: 0.0066494\ttotal: 2.4s\tremaining: 2.6s\n",
            "24:\tlearn: 0.0065119\ttotal: 2.51s\tremaining: 2.51s\n",
            "25:\tlearn: 0.0064120\ttotal: 2.61s\tremaining: 2.4s\n",
            "26:\tlearn: 0.0062493\ttotal: 2.7s\tremaining: 2.3s\n",
            "27:\tlearn: 0.0060222\ttotal: 2.8s\tremaining: 2.2s\n",
            "28:\tlearn: 0.0058996\ttotal: 2.89s\tremaining: 2.1s\n",
            "29:\tlearn: 0.0056153\ttotal: 2.99s\tremaining: 1.99s\n",
            "30:\tlearn: 0.0054850\ttotal: 3.1s\tremaining: 1.9s\n",
            "31:\tlearn: 0.0053536\ttotal: 3.19s\tremaining: 1.8s\n",
            "32:\tlearn: 0.0050535\ttotal: 3.29s\tremaining: 1.69s\n",
            "33:\tlearn: 0.0049181\ttotal: 3.4s\tremaining: 1.6s\n",
            "34:\tlearn: 0.0048305\ttotal: 3.51s\tremaining: 1.5s\n",
            "35:\tlearn: 0.0046832\ttotal: 3.61s\tremaining: 1.4s\n",
            "36:\tlearn: 0.0045530\ttotal: 3.7s\tremaining: 1.3s\n",
            "37:\tlearn: 0.0043401\ttotal: 3.8s\tremaining: 1.2s\n",
            "38:\tlearn: 0.0041766\ttotal: 3.91s\tremaining: 1.1s\n",
            "39:\tlearn: 0.0040198\ttotal: 4s\tremaining: 1000ms\n",
            "40:\tlearn: 0.0039331\ttotal: 4.09s\tremaining: 898ms\n",
            "41:\tlearn: 0.0038401\ttotal: 4.19s\tremaining: 798ms\n",
            "42:\tlearn: 0.0037031\ttotal: 4.28s\tremaining: 697ms\n",
            "43:\tlearn: 0.0036184\ttotal: 4.38s\tremaining: 597ms\n",
            "44:\tlearn: 0.0034940\ttotal: 4.49s\tremaining: 499ms\n",
            "45:\tlearn: 0.0034354\ttotal: 4.59s\tremaining: 399ms\n",
            "46:\tlearn: 0.0033758\ttotal: 4.68s\tremaining: 299ms\n",
            "47:\tlearn: 0.0032684\ttotal: 4.78s\tremaining: 199ms\n",
            "48:\tlearn: 0.0032266\ttotal: 4.88s\tremaining: 99.5ms\n",
            "49:\tlearn: 0.0031479\ttotal: 4.97s\tremaining: 0us\n",
            "0:\tlearn: 0.0160898\ttotal: 141ms\tremaining: 6.93s\n",
            "1:\tlearn: 0.0148184\ttotal: 235ms\tremaining: 5.64s\n",
            "2:\tlearn: 0.0134122\ttotal: 337ms\tremaining: 5.28s\n",
            "3:\tlearn: 0.0125379\ttotal: 445ms\tremaining: 5.12s\n",
            "4:\tlearn: 0.0112636\ttotal: 551ms\tremaining: 4.96s\n",
            "5:\tlearn: 0.0101273\ttotal: 655ms\tremaining: 4.81s\n",
            "6:\tlearn: 0.0098033\ttotal: 763ms\tremaining: 4.69s\n",
            "7:\tlearn: 0.0092178\ttotal: 857ms\tremaining: 4.5s\n",
            "8:\tlearn: 0.0090290\ttotal: 965ms\tremaining: 4.4s\n",
            "9:\tlearn: 0.0087987\ttotal: 1.06s\tremaining: 4.25s\n",
            "10:\tlearn: 0.0080255\ttotal: 1.16s\tremaining: 4.1s\n",
            "11:\tlearn: 0.0077891\ttotal: 1.26s\tremaining: 4s\n",
            "12:\tlearn: 0.0075057\ttotal: 1.36s\tremaining: 3.86s\n",
            "13:\tlearn: 0.0072623\ttotal: 1.45s\tremaining: 3.73s\n",
            "14:\tlearn: 0.0071470\ttotal: 1.57s\tremaining: 3.66s\n",
            "15:\tlearn: 0.0070720\ttotal: 1.66s\tremaining: 3.53s\n",
            "16:\tlearn: 0.0069722\ttotal: 1.76s\tremaining: 3.41s\n",
            "17:\tlearn: 0.0065327\ttotal: 1.86s\tremaining: 3.31s\n",
            "18:\tlearn: 0.0061245\ttotal: 1.96s\tremaining: 3.19s\n",
            "19:\tlearn: 0.0060816\ttotal: 2.05s\tremaining: 3.08s\n",
            "20:\tlearn: 0.0058033\ttotal: 2.15s\tremaining: 2.97s\n",
            "21:\tlearn: 0.0057349\ttotal: 2.25s\tremaining: 2.86s\n",
            "22:\tlearn: 0.0055985\ttotal: 2.34s\tremaining: 2.75s\n",
            "23:\tlearn: 0.0054927\ttotal: 2.44s\tremaining: 2.65s\n",
            "24:\tlearn: 0.0053720\ttotal: 2.55s\tremaining: 2.55s\n",
            "25:\tlearn: 0.0051476\ttotal: 2.64s\tremaining: 2.44s\n",
            "26:\tlearn: 0.0050279\ttotal: 2.75s\tremaining: 2.34s\n",
            "27:\tlearn: 0.0048317\ttotal: 2.84s\tremaining: 2.23s\n",
            "28:\tlearn: 0.0047113\ttotal: 2.94s\tremaining: 2.13s\n",
            "29:\tlearn: 0.0045044\ttotal: 3.04s\tremaining: 2.02s\n",
            "30:\tlearn: 0.0044213\ttotal: 3.13s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0042262\ttotal: 3.23s\tremaining: 1.81s\n",
            "32:\tlearn: 0.0041341\ttotal: 3.33s\tremaining: 1.71s\n",
            "33:\tlearn: 0.0040396\ttotal: 3.42s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0039706\ttotal: 3.51s\tremaining: 1.5s\n",
            "35:\tlearn: 0.0039117\ttotal: 3.63s\tremaining: 1.41s\n",
            "36:\tlearn: 0.0038376\ttotal: 3.72s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0038100\ttotal: 3.81s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0037658\ttotal: 3.92s\tremaining: 1.1s\n",
            "39:\tlearn: 0.0036756\ttotal: 4.01s\tremaining: 1s\n",
            "40:\tlearn: 0.0035976\ttotal: 4.11s\tremaining: 902ms\n",
            "41:\tlearn: 0.0034116\ttotal: 4.22s\tremaining: 803ms\n",
            "42:\tlearn: 0.0033761\ttotal: 4.31s\tremaining: 702ms\n",
            "43:\tlearn: 0.0032478\ttotal: 4.41s\tremaining: 601ms\n",
            "44:\tlearn: 0.0030597\ttotal: 4.52s\tremaining: 502ms\n",
            "45:\tlearn: 0.0030574\ttotal: 4.62s\tremaining: 402ms\n",
            "46:\tlearn: 0.0030496\ttotal: 4.72s\tremaining: 301ms\n",
            "47:\tlearn: 0.0029509\ttotal: 4.82s\tremaining: 201ms\n",
            "48:\tlearn: 0.0029388\ttotal: 4.92s\tremaining: 100ms\n",
            "49:\tlearn: 0.0028861\ttotal: 5.01s\tremaining: 0us\n",
            "0:\tlearn: 0.0352973\ttotal: 138ms\tremaining: 6.74s\n",
            "1:\tlearn: 0.0293330\ttotal: 230ms\tremaining: 5.52s\n",
            "2:\tlearn: 0.0246026\ttotal: 321ms\tremaining: 5.03s\n",
            "3:\tlearn: 0.0213474\ttotal: 424ms\tremaining: 4.88s\n",
            "4:\tlearn: 0.0179505\ttotal: 518ms\tremaining: 4.66s\n",
            "5:\tlearn: 0.0159234\ttotal: 631ms\tremaining: 4.62s\n",
            "6:\tlearn: 0.0148085\ttotal: 734ms\tremaining: 4.51s\n",
            "7:\tlearn: 0.0138645\ttotal: 829ms\tremaining: 4.35s\n",
            "8:\tlearn: 0.0131497\ttotal: 934ms\tremaining: 4.26s\n",
            "9:\tlearn: 0.0124649\ttotal: 1.03s\tremaining: 4.11s\n",
            "10:\tlearn: 0.0119123\ttotal: 1.12s\tremaining: 3.98s\n",
            "11:\tlearn: 0.0115420\ttotal: 1.23s\tremaining: 3.9s\n",
            "12:\tlearn: 0.0109038\ttotal: 1.32s\tremaining: 3.76s\n",
            "13:\tlearn: 0.0103504\ttotal: 1.42s\tremaining: 3.64s\n",
            "14:\tlearn: 0.0098756\ttotal: 1.52s\tremaining: 3.56s\n",
            "15:\tlearn: 0.0096854\ttotal: 1.63s\tremaining: 3.47s\n",
            "16:\tlearn: 0.0092236\ttotal: 1.72s\tremaining: 3.35s\n",
            "17:\tlearn: 0.0088424\ttotal: 1.83s\tremaining: 3.25s\n",
            "18:\tlearn: 0.0084497\ttotal: 1.92s\tremaining: 3.13s\n",
            "19:\tlearn: 0.0079505\ttotal: 2.02s\tremaining: 3.04s\n",
            "20:\tlearn: 0.0078641\ttotal: 2.12s\tremaining: 2.92s\n",
            "21:\tlearn: 0.0076968\ttotal: 2.21s\tremaining: 2.81s\n",
            "22:\tlearn: 0.0074464\ttotal: 2.31s\tremaining: 2.71s\n",
            "23:\tlearn: 0.0072543\ttotal: 2.4s\tremaining: 2.6s\n",
            "24:\tlearn: 0.0070383\ttotal: 2.49s\tremaining: 2.49s\n",
            "25:\tlearn: 0.0067389\ttotal: 2.61s\tremaining: 2.41s\n",
            "26:\tlearn: 0.0064204\ttotal: 2.7s\tremaining: 2.3s\n",
            "27:\tlearn: 0.0061403\ttotal: 2.79s\tremaining: 2.19s\n",
            "28:\tlearn: 0.0059290\ttotal: 2.9s\tremaining: 2.1s\n",
            "29:\tlearn: 0.0057988\ttotal: 2.99s\tremaining: 2s\n",
            "30:\tlearn: 0.0055973\ttotal: 3.08s\tremaining: 1.89s\n",
            "31:\tlearn: 0.0054143\ttotal: 3.19s\tremaining: 1.79s\n",
            "32:\tlearn: 0.0051990\ttotal: 3.28s\tremaining: 1.69s\n",
            "33:\tlearn: 0.0050415\ttotal: 3.38s\tremaining: 1.59s\n",
            "34:\tlearn: 0.0048422\ttotal: 3.48s\tremaining: 1.49s\n",
            "35:\tlearn: 0.0046939\ttotal: 3.58s\tremaining: 1.39s\n",
            "36:\tlearn: 0.0045775\ttotal: 3.7s\tremaining: 1.3s\n",
            "37:\tlearn: 0.0044971\ttotal: 3.79s\tremaining: 1.2s\n",
            "38:\tlearn: 0.0043752\ttotal: 3.89s\tremaining: 1.1s\n",
            "39:\tlearn: 0.0042413\ttotal: 3.99s\tremaining: 998ms\n",
            "40:\tlearn: 0.0041342\ttotal: 4.09s\tremaining: 897ms\n",
            "41:\tlearn: 0.0040282\ttotal: 4.18s\tremaining: 797ms\n",
            "42:\tlearn: 0.0039720\ttotal: 4.29s\tremaining: 699ms\n",
            "43:\tlearn: 0.0038396\ttotal: 4.39s\tremaining: 598ms\n",
            "44:\tlearn: 0.0037180\ttotal: 4.48s\tremaining: 498ms\n",
            "45:\tlearn: 0.0036004\ttotal: 4.58s\tremaining: 399ms\n",
            "46:\tlearn: 0.0035281\ttotal: 4.69s\tremaining: 299ms\n",
            "47:\tlearn: 0.0034739\ttotal: 4.79s\tremaining: 200ms\n",
            "48:\tlearn: 0.0034183\ttotal: 4.89s\tremaining: 99.9ms\n",
            "49:\tlearn: 0.0033398\ttotal: 4.99s\tremaining: 0us\n",
            "0:\tlearn: 0.1955652\ttotal: 156ms\tremaining: 7.65s\n",
            "1:\tlearn: 0.1641460\ttotal: 258ms\tremaining: 6.2s\n",
            "2:\tlearn: 0.1392322\ttotal: 368ms\tremaining: 5.76s\n",
            "3:\tlearn: 0.1210947\ttotal: 479ms\tremaining: 5.5s\n",
            "4:\tlearn: 0.1089485\ttotal: 603ms\tremaining: 5.42s\n",
            "5:\tlearn: 0.0999682\ttotal: 706ms\tremaining: 5.18s\n",
            "6:\tlearn: 0.0959671\ttotal: 820ms\tremaining: 5.04s\n",
            "7:\tlearn: 0.0912275\ttotal: 922ms\tremaining: 4.84s\n",
            "8:\tlearn: 0.0858470\ttotal: 1.03s\tremaining: 4.7s\n",
            "9:\tlearn: 0.0829709\ttotal: 1.13s\tremaining: 4.54s\n",
            "10:\tlearn: 0.0808998\ttotal: 1.25s\tremaining: 4.41s\n",
            "11:\tlearn: 0.0789495\ttotal: 1.35s\tremaining: 4.28s\n",
            "12:\tlearn: 0.0775705\ttotal: 1.47s\tremaining: 4.17s\n",
            "13:\tlearn: 0.0745247\ttotal: 1.57s\tremaining: 4.03s\n",
            "14:\tlearn: 0.0725441\ttotal: 1.69s\tremaining: 3.95s\n",
            "15:\tlearn: 0.0708022\ttotal: 1.8s\tremaining: 3.82s\n",
            "16:\tlearn: 0.0688756\ttotal: 1.91s\tremaining: 3.71s\n",
            "17:\tlearn: 0.0670098\ttotal: 2.02s\tremaining: 3.59s\n",
            "18:\tlearn: 0.0658233\ttotal: 2.13s\tremaining: 3.48s\n",
            "19:\tlearn: 0.0650388\ttotal: 2.23s\tremaining: 3.35s\n",
            "20:\tlearn: 0.0639761\ttotal: 2.35s\tremaining: 3.24s\n",
            "21:\tlearn: 0.0621553\ttotal: 2.45s\tremaining: 3.12s\n",
            "22:\tlearn: 0.0606938\ttotal: 2.56s\tremaining: 3.01s\n",
            "23:\tlearn: 0.0595176\ttotal: 2.68s\tremaining: 2.9s\n",
            "24:\tlearn: 0.0581745\ttotal: 2.79s\tremaining: 2.79s\n",
            "25:\tlearn: 0.0565807\ttotal: 2.9s\tremaining: 2.67s\n",
            "26:\tlearn: 0.0545081\ttotal: 3.01s\tremaining: 2.56s\n",
            "27:\tlearn: 0.0531804\ttotal: 3.11s\tremaining: 2.44s\n",
            "28:\tlearn: 0.0516628\ttotal: 3.21s\tremaining: 2.33s\n",
            "29:\tlearn: 0.0510712\ttotal: 3.33s\tremaining: 2.22s\n",
            "30:\tlearn: 0.0493515\ttotal: 3.44s\tremaining: 2.11s\n",
            "31:\tlearn: 0.0484733\ttotal: 3.55s\tremaining: 2s\n",
            "32:\tlearn: 0.0479789\ttotal: 3.67s\tremaining: 1.89s\n",
            "33:\tlearn: 0.0470564\ttotal: 3.78s\tremaining: 1.78s\n",
            "34:\tlearn: 0.0458233\ttotal: 3.89s\tremaining: 1.67s\n",
            "35:\tlearn: 0.0451486\ttotal: 4s\tremaining: 1.55s\n",
            "36:\tlearn: 0.0442420\ttotal: 4.11s\tremaining: 1.44s\n",
            "37:\tlearn: 0.0436307\ttotal: 4.22s\tremaining: 1.33s\n",
            "38:\tlearn: 0.0432448\ttotal: 4.33s\tremaining: 1.22s\n",
            "39:\tlearn: 0.0427929\ttotal: 4.44s\tremaining: 1.11s\n",
            "40:\tlearn: 0.0420777\ttotal: 4.55s\tremaining: 999ms\n",
            "41:\tlearn: 0.0417012\ttotal: 4.66s\tremaining: 887ms\n",
            "42:\tlearn: 0.0410593\ttotal: 4.78s\tremaining: 778ms\n",
            "43:\tlearn: 0.0405288\ttotal: 4.88s\tremaining: 666ms\n",
            "44:\tlearn: 0.0395659\ttotal: 4.99s\tremaining: 555ms\n",
            "45:\tlearn: 0.0386973\ttotal: 5.1s\tremaining: 443ms\n",
            "46:\tlearn: 0.0382198\ttotal: 5.21s\tremaining: 332ms\n",
            "47:\tlearn: 0.0376505\ttotal: 5.31s\tremaining: 221ms\n",
            "48:\tlearn: 0.0371400\ttotal: 5.42s\tremaining: 111ms\n",
            "49:\tlearn: 0.0364246\ttotal: 5.53s\tremaining: 0us\n",
            "0:\tlearn: 0.0329512\ttotal: 160ms\tremaining: 7.83s\n",
            "1:\tlearn: 0.0266140\ttotal: 255ms\tremaining: 6.12s\n",
            "2:\tlearn: 0.0227289\ttotal: 348ms\tremaining: 5.46s\n",
            "3:\tlearn: 0.0202346\ttotal: 451ms\tremaining: 5.19s\n",
            "4:\tlearn: 0.0184501\ttotal: 546ms\tremaining: 4.92s\n",
            "5:\tlearn: 0.0165104\ttotal: 640ms\tremaining: 4.7s\n",
            "6:\tlearn: 0.0156221\ttotal: 748ms\tremaining: 4.59s\n",
            "7:\tlearn: 0.0146483\ttotal: 842ms\tremaining: 4.42s\n",
            "8:\tlearn: 0.0140162\ttotal: 941ms\tremaining: 4.28s\n",
            "9:\tlearn: 0.0133761\ttotal: 1.05s\tremaining: 4.21s\n",
            "10:\tlearn: 0.0129545\ttotal: 1.16s\tremaining: 4.12s\n",
            "11:\tlearn: 0.0122705\ttotal: 1.28s\tremaining: 4.05s\n",
            "12:\tlearn: 0.0118983\ttotal: 1.38s\tremaining: 3.94s\n",
            "13:\tlearn: 0.0113943\ttotal: 1.49s\tremaining: 3.84s\n",
            "14:\tlearn: 0.0109465\ttotal: 1.59s\tremaining: 3.72s\n",
            "15:\tlearn: 0.0106525\ttotal: 1.7s\tremaining: 3.61s\n",
            "16:\tlearn: 0.0102800\ttotal: 1.81s\tremaining: 3.51s\n",
            "17:\tlearn: 0.0101843\ttotal: 1.91s\tremaining: 3.4s\n",
            "18:\tlearn: 0.0098819\ttotal: 2.02s\tremaining: 3.29s\n",
            "19:\tlearn: 0.0096676\ttotal: 2.12s\tremaining: 3.19s\n",
            "20:\tlearn: 0.0095228\ttotal: 2.24s\tremaining: 3.09s\n",
            "21:\tlearn: 0.0092491\ttotal: 2.35s\tremaining: 2.99s\n",
            "22:\tlearn: 0.0090072\ttotal: 2.46s\tremaining: 2.88s\n",
            "23:\tlearn: 0.0088167\ttotal: 2.56s\tremaining: 2.78s\n",
            "24:\tlearn: 0.0085532\ttotal: 2.67s\tremaining: 2.67s\n",
            "25:\tlearn: 0.0084040\ttotal: 2.79s\tremaining: 2.57s\n",
            "26:\tlearn: 0.0081872\ttotal: 2.89s\tremaining: 2.46s\n",
            "27:\tlearn: 0.0081187\ttotal: 2.99s\tremaining: 2.35s\n",
            "28:\tlearn: 0.0079665\ttotal: 3.11s\tremaining: 2.25s\n",
            "29:\tlearn: 0.0078487\ttotal: 3.23s\tremaining: 2.15s\n",
            "30:\tlearn: 0.0077699\ttotal: 3.33s\tremaining: 2.04s\n",
            "31:\tlearn: 0.0076682\ttotal: 3.44s\tremaining: 1.94s\n",
            "32:\tlearn: 0.0075333\ttotal: 3.55s\tremaining: 1.83s\n",
            "33:\tlearn: 0.0074314\ttotal: 3.66s\tremaining: 1.72s\n",
            "34:\tlearn: 0.0073041\ttotal: 3.76s\tremaining: 1.61s\n",
            "35:\tlearn: 0.0071753\ttotal: 3.87s\tremaining: 1.5s\n",
            "36:\tlearn: 0.0070799\ttotal: 3.98s\tremaining: 1.4s\n",
            "37:\tlearn: 0.0069432\ttotal: 4.09s\tremaining: 1.29s\n",
            "38:\tlearn: 0.0067826\ttotal: 4.19s\tremaining: 1.18s\n",
            "39:\tlearn: 0.0066013\ttotal: 4.32s\tremaining: 1.08s\n",
            "40:\tlearn: 0.0064774\ttotal: 4.43s\tremaining: 972ms\n",
            "41:\tlearn: 0.0063325\ttotal: 4.54s\tremaining: 865ms\n",
            "42:\tlearn: 0.0062153\ttotal: 4.64s\tremaining: 756ms\n",
            "43:\tlearn: 0.0061229\ttotal: 4.75s\tremaining: 648ms\n",
            "44:\tlearn: 0.0060459\ttotal: 4.86s\tremaining: 540ms\n",
            "45:\tlearn: 0.0059213\ttotal: 4.97s\tremaining: 432ms\n",
            "46:\tlearn: 0.0058577\ttotal: 5.08s\tremaining: 324ms\n",
            "47:\tlearn: 0.0056986\ttotal: 5.19s\tremaining: 216ms\n",
            "48:\tlearn: 0.0055421\ttotal: 5.3s\tremaining: 108ms\n",
            "49:\tlearn: 0.0054326\ttotal: 5.41s\tremaining: 0us\n",
            "0:\tlearn: 0.0145898\ttotal: 160ms\tremaining: 7.84s\n",
            "1:\tlearn: 0.0127594\ttotal: 263ms\tremaining: 6.3s\n",
            "2:\tlearn: 0.0118418\ttotal: 380ms\tremaining: 5.95s\n",
            "3:\tlearn: 0.0108328\ttotal: 486ms\tremaining: 5.59s\n",
            "4:\tlearn: 0.0100104\ttotal: 599ms\tremaining: 5.39s\n",
            "5:\tlearn: 0.0091817\ttotal: 702ms\tremaining: 5.15s\n",
            "6:\tlearn: 0.0090114\ttotal: 818ms\tremaining: 5.02s\n",
            "7:\tlearn: 0.0085639\ttotal: 929ms\tremaining: 4.88s\n",
            "8:\tlearn: 0.0080879\ttotal: 1.04s\tremaining: 4.75s\n",
            "9:\tlearn: 0.0075542\ttotal: 1.15s\tremaining: 4.59s\n",
            "10:\tlearn: 0.0073665\ttotal: 1.25s\tremaining: 4.45s\n",
            "11:\tlearn: 0.0071432\ttotal: 1.36s\tremaining: 4.32s\n",
            "12:\tlearn: 0.0070044\ttotal: 1.47s\tremaining: 4.2s\n",
            "13:\tlearn: 0.0069132\ttotal: 1.58s\tremaining: 4.05s\n",
            "14:\tlearn: 0.0068447\ttotal: 1.68s\tremaining: 3.93s\n",
            "15:\tlearn: 0.0067962\ttotal: 1.79s\tremaining: 3.81s\n",
            "16:\tlearn: 0.0066954\ttotal: 1.92s\tremaining: 3.72s\n",
            "17:\tlearn: 0.0064631\ttotal: 2.02s\tremaining: 3.59s\n",
            "18:\tlearn: 0.0061860\ttotal: 2.13s\tremaining: 3.47s\n",
            "19:\tlearn: 0.0059880\ttotal: 2.24s\tremaining: 3.35s\n",
            "20:\tlearn: 0.0058030\ttotal: 2.35s\tremaining: 3.24s\n",
            "21:\tlearn: 0.0056562\ttotal: 2.45s\tremaining: 3.12s\n",
            "22:\tlearn: 0.0052786\ttotal: 2.55s\tremaining: 2.99s\n",
            "23:\tlearn: 0.0051469\ttotal: 2.66s\tremaining: 2.88s\n",
            "24:\tlearn: 0.0049723\ttotal: 2.77s\tremaining: 2.77s\n",
            "25:\tlearn: 0.0047704\ttotal: 2.88s\tremaining: 2.66s\n",
            "26:\tlearn: 0.0045465\ttotal: 2.99s\tremaining: 2.55s\n",
            "27:\tlearn: 0.0042888\ttotal: 3.1s\tremaining: 2.44s\n",
            "28:\tlearn: 0.0042188\ttotal: 3.22s\tremaining: 2.33s\n",
            "29:\tlearn: 0.0041073\ttotal: 3.32s\tremaining: 2.22s\n",
            "30:\tlearn: 0.0040565\ttotal: 3.44s\tremaining: 2.11s\n",
            "31:\tlearn: 0.0040151\ttotal: 3.55s\tremaining: 1.99s\n",
            "32:\tlearn: 0.0040010\ttotal: 3.66s\tremaining: 1.89s\n",
            "33:\tlearn: 0.0038497\ttotal: 3.77s\tremaining: 1.77s\n",
            "34:\tlearn: 0.0037391\ttotal: 3.89s\tremaining: 1.67s\n",
            "35:\tlearn: 0.0037112\ttotal: 4s\tremaining: 1.56s\n",
            "36:\tlearn: 0.0036942\ttotal: 4.12s\tremaining: 1.45s\n",
            "37:\tlearn: 0.0036528\ttotal: 4.22s\tremaining: 1.33s\n",
            "38:\tlearn: 0.0036346\ttotal: 4.34s\tremaining: 1.23s\n",
            "39:\tlearn: 0.0036188\ttotal: 4.46s\tremaining: 1.12s\n",
            "40:\tlearn: 0.0035884\ttotal: 4.58s\tremaining: 1s\n",
            "41:\tlearn: 0.0035527\ttotal: 4.68s\tremaining: 892ms\n",
            "42:\tlearn: 0.0033869\ttotal: 4.79s\tremaining: 780ms\n",
            "43:\tlearn: 0.0031931\ttotal: 4.9s\tremaining: 668ms\n",
            "44:\tlearn: 0.0031082\ttotal: 5.03s\tremaining: 558ms\n",
            "45:\tlearn: 0.0030757\ttotal: 5.13s\tremaining: 446ms\n",
            "46:\tlearn: 0.0029249\ttotal: 5.24s\tremaining: 334ms\n",
            "47:\tlearn: 0.0028957\ttotal: 5.34s\tremaining: 223ms\n",
            "48:\tlearn: 0.0028691\ttotal: 5.45s\tremaining: 111ms\n",
            "49:\tlearn: 0.0028473\ttotal: 5.56s\tremaining: 0us\n",
            "0:\tlearn: 0.0339386\ttotal: 162ms\tremaining: 7.92s\n",
            "1:\tlearn: 0.0269659\ttotal: 263ms\tremaining: 6.31s\n",
            "2:\tlearn: 0.0225215\ttotal: 386ms\tremaining: 6.05s\n",
            "3:\tlearn: 0.0195806\ttotal: 488ms\tremaining: 5.62s\n",
            "4:\tlearn: 0.0172787\ttotal: 598ms\tremaining: 5.38s\n",
            "5:\tlearn: 0.0155071\ttotal: 700ms\tremaining: 5.13s\n",
            "6:\tlearn: 0.0145436\ttotal: 811ms\tremaining: 4.98s\n",
            "7:\tlearn: 0.0138180\ttotal: 912ms\tremaining: 4.79s\n",
            "8:\tlearn: 0.0133024\ttotal: 1.02s\tremaining: 4.65s\n",
            "9:\tlearn: 0.0130230\ttotal: 1.13s\tremaining: 4.52s\n",
            "10:\tlearn: 0.0126137\ttotal: 1.24s\tremaining: 4.39s\n",
            "11:\tlearn: 0.0122152\ttotal: 1.35s\tremaining: 4.28s\n",
            "12:\tlearn: 0.0116333\ttotal: 1.47s\tremaining: 4.19s\n",
            "13:\tlearn: 0.0111008\ttotal: 1.58s\tremaining: 4.05s\n",
            "14:\tlearn: 0.0106592\ttotal: 1.68s\tremaining: 3.93s\n",
            "15:\tlearn: 0.0105086\ttotal: 1.79s\tremaining: 3.8s\n",
            "16:\tlearn: 0.0100270\ttotal: 1.9s\tremaining: 3.69s\n",
            "17:\tlearn: 0.0097658\ttotal: 2.01s\tremaining: 3.58s\n",
            "18:\tlearn: 0.0095877\ttotal: 2.13s\tremaining: 3.47s\n",
            "19:\tlearn: 0.0093721\ttotal: 2.23s\tremaining: 3.34s\n",
            "20:\tlearn: 0.0091712\ttotal: 2.34s\tremaining: 3.23s\n",
            "21:\tlearn: 0.0087942\ttotal: 2.45s\tremaining: 3.12s\n",
            "22:\tlearn: 0.0086065\ttotal: 2.56s\tremaining: 3.01s\n",
            "23:\tlearn: 0.0084386\ttotal: 2.67s\tremaining: 2.89s\n",
            "24:\tlearn: 0.0082333\ttotal: 2.78s\tremaining: 2.78s\n",
            "25:\tlearn: 0.0080674\ttotal: 2.88s\tremaining: 2.66s\n",
            "26:\tlearn: 0.0078552\ttotal: 2.99s\tremaining: 2.55s\n",
            "27:\tlearn: 0.0077346\ttotal: 3.1s\tremaining: 2.43s\n",
            "28:\tlearn: 0.0076591\ttotal: 3.21s\tremaining: 2.32s\n",
            "29:\tlearn: 0.0074366\ttotal: 3.31s\tremaining: 2.21s\n",
            "30:\tlearn: 0.0073167\ttotal: 3.43s\tremaining: 2.1s\n",
            "31:\tlearn: 0.0072073\ttotal: 3.54s\tremaining: 1.99s\n",
            "32:\tlearn: 0.0070309\ttotal: 3.65s\tremaining: 1.88s\n",
            "33:\tlearn: 0.0069355\ttotal: 3.76s\tremaining: 1.77s\n",
            "34:\tlearn: 0.0067886\ttotal: 3.87s\tremaining: 1.66s\n",
            "35:\tlearn: 0.0066023\ttotal: 3.98s\tremaining: 1.54s\n",
            "36:\tlearn: 0.0064345\ttotal: 4.09s\tremaining: 1.44s\n",
            "37:\tlearn: 0.0063834\ttotal: 4.2s\tremaining: 1.32s\n",
            "38:\tlearn: 0.0062529\ttotal: 4.31s\tremaining: 1.22s\n",
            "39:\tlearn: 0.0061501\ttotal: 4.42s\tremaining: 1.11s\n",
            "40:\tlearn: 0.0060028\ttotal: 4.54s\tremaining: 996ms\n",
            "41:\tlearn: 0.0058788\ttotal: 4.64s\tremaining: 884ms\n",
            "42:\tlearn: 0.0057586\ttotal: 4.75s\tremaining: 773ms\n",
            "43:\tlearn: 0.0056455\ttotal: 4.86s\tremaining: 662ms\n",
            "44:\tlearn: 0.0055495\ttotal: 4.97s\tremaining: 552ms\n",
            "45:\tlearn: 0.0054786\ttotal: 5.07s\tremaining: 441ms\n",
            "46:\tlearn: 0.0053747\ttotal: 5.19s\tremaining: 331ms\n",
            "47:\tlearn: 0.0052388\ttotal: 5.3s\tremaining: 221ms\n",
            "48:\tlearn: 0.0051414\ttotal: 5.41s\tremaining: 110ms\n",
            "49:\tlearn: 0.0050520\ttotal: 5.52s\tremaining: 0us\n",
            "0:\tlearn: 0.2065169\ttotal: 171ms\tremaining: 8.38s\n",
            "1:\tlearn: 0.1758331\ttotal: 278ms\tremaining: 6.68s\n",
            "2:\tlearn: 0.1520075\ttotal: 397ms\tremaining: 6.22s\n",
            "3:\tlearn: 0.1360684\ttotal: 508ms\tremaining: 5.84s\n",
            "4:\tlearn: 0.1234959\ttotal: 626ms\tremaining: 5.64s\n",
            "5:\tlearn: 0.1119857\ttotal: 738ms\tremaining: 5.41s\n",
            "6:\tlearn: 0.1062680\ttotal: 866ms\tremaining: 5.32s\n",
            "7:\tlearn: 0.1025201\ttotal: 975ms\tremaining: 5.12s\n",
            "8:\tlearn: 0.0969355\ttotal: 1.09s\tremaining: 4.97s\n",
            "9:\tlearn: 0.0945810\ttotal: 1.2s\tremaining: 4.79s\n",
            "10:\tlearn: 0.0903293\ttotal: 1.32s\tremaining: 4.67s\n",
            "11:\tlearn: 0.0872912\ttotal: 1.43s\tremaining: 4.51s\n",
            "12:\tlearn: 0.0842959\ttotal: 1.54s\tremaining: 4.39s\n",
            "13:\tlearn: 0.0822730\ttotal: 1.65s\tremaining: 4.25s\n",
            "14:\tlearn: 0.0806737\ttotal: 1.78s\tremaining: 4.14s\n",
            "15:\tlearn: 0.0789965\ttotal: 1.9s\tremaining: 4.03s\n",
            "16:\tlearn: 0.0770507\ttotal: 2.01s\tremaining: 3.91s\n",
            "17:\tlearn: 0.0760925\ttotal: 2.12s\tremaining: 3.77s\n",
            "18:\tlearn: 0.0748398\ttotal: 2.24s\tremaining: 3.66s\n",
            "19:\tlearn: 0.0734708\ttotal: 2.35s\tremaining: 3.53s\n",
            "20:\tlearn: 0.0726766\ttotal: 2.47s\tremaining: 3.41s\n",
            "21:\tlearn: 0.0708482\ttotal: 2.58s\tremaining: 3.28s\n",
            "22:\tlearn: 0.0698715\ttotal: 2.7s\tremaining: 3.17s\n",
            "23:\tlearn: 0.0687997\ttotal: 2.81s\tremaining: 3.04s\n",
            "24:\tlearn: 0.0675682\ttotal: 2.94s\tremaining: 2.94s\n",
            "25:\tlearn: 0.0659353\ttotal: 3.05s\tremaining: 2.81s\n",
            "26:\tlearn: 0.0648157\ttotal: 3.16s\tremaining: 2.69s\n",
            "27:\tlearn: 0.0640960\ttotal: 3.27s\tremaining: 2.57s\n",
            "28:\tlearn: 0.0625787\ttotal: 3.39s\tremaining: 2.45s\n",
            "29:\tlearn: 0.0621272\ttotal: 3.5s\tremaining: 2.33s\n",
            "30:\tlearn: 0.0611325\ttotal: 3.63s\tremaining: 2.22s\n",
            "31:\tlearn: 0.0597998\ttotal: 3.74s\tremaining: 2.1s\n",
            "32:\tlearn: 0.0585529\ttotal: 3.86s\tremaining: 1.99s\n",
            "33:\tlearn: 0.0570996\ttotal: 3.98s\tremaining: 1.87s\n",
            "34:\tlearn: 0.0562950\ttotal: 4.11s\tremaining: 1.76s\n",
            "35:\tlearn: 0.0553254\ttotal: 4.22s\tremaining: 1.64s\n",
            "36:\tlearn: 0.0543318\ttotal: 4.34s\tremaining: 1.53s\n",
            "37:\tlearn: 0.0533028\ttotal: 4.46s\tremaining: 1.41s\n",
            "38:\tlearn: 0.0521160\ttotal: 4.57s\tremaining: 1.29s\n",
            "39:\tlearn: 0.0510551\ttotal: 4.68s\tremaining: 1.17s\n",
            "40:\tlearn: 0.0502275\ttotal: 4.81s\tremaining: 1.05s\n",
            "41:\tlearn: 0.0495337\ttotal: 4.92s\tremaining: 937ms\n",
            "42:\tlearn: 0.0486385\ttotal: 5.05s\tremaining: 823ms\n",
            "43:\tlearn: 0.0481257\ttotal: 5.16s\tremaining: 704ms\n",
            "44:\tlearn: 0.0473404\ttotal: 5.29s\tremaining: 588ms\n",
            "45:\tlearn: 0.0465064\ttotal: 5.4s\tremaining: 470ms\n",
            "46:\tlearn: 0.0459425\ttotal: 5.52s\tremaining: 352ms\n",
            "47:\tlearn: 0.0450996\ttotal: 5.63s\tremaining: 234ms\n",
            "48:\tlearn: 0.0441727\ttotal: 5.75s\tremaining: 117ms\n",
            "49:\tlearn: 0.0435698\ttotal: 5.86s\tremaining: 0us\n",
            "0:\tlearn: 0.0325083\ttotal: 179ms\tremaining: 8.75s\n",
            "1:\tlearn: 0.0267448\ttotal: 289ms\tremaining: 6.93s\n",
            "2:\tlearn: 0.0228722\ttotal: 408ms\tremaining: 6.39s\n",
            "3:\tlearn: 0.0200921\ttotal: 517ms\tremaining: 5.94s\n",
            "4:\tlearn: 0.0178923\ttotal: 633ms\tremaining: 5.69s\n",
            "5:\tlearn: 0.0161083\ttotal: 741ms\tremaining: 5.44s\n",
            "6:\tlearn: 0.0148080\ttotal: 868ms\tremaining: 5.33s\n",
            "7:\tlearn: 0.0139342\ttotal: 990ms\tremaining: 5.2s\n",
            "8:\tlearn: 0.0134953\ttotal: 1.12s\tremaining: 5.11s\n",
            "9:\tlearn: 0.0131074\ttotal: 1.23s\tremaining: 4.92s\n",
            "10:\tlearn: 0.0125556\ttotal: 1.35s\tremaining: 4.79s\n",
            "11:\tlearn: 0.0121335\ttotal: 1.46s\tremaining: 4.63s\n",
            "12:\tlearn: 0.0119019\ttotal: 1.58s\tremaining: 4.49s\n",
            "13:\tlearn: 0.0114152\ttotal: 1.69s\tremaining: 4.35s\n",
            "14:\tlearn: 0.0112104\ttotal: 1.81s\tremaining: 4.22s\n",
            "15:\tlearn: 0.0108775\ttotal: 1.92s\tremaining: 4.07s\n",
            "16:\tlearn: 0.0105791\ttotal: 2.03s\tremaining: 3.94s\n",
            "17:\tlearn: 0.0103896\ttotal: 2.16s\tremaining: 3.83s\n",
            "18:\tlearn: 0.0102241\ttotal: 2.28s\tremaining: 3.72s\n",
            "19:\tlearn: 0.0100474\ttotal: 2.39s\tremaining: 3.59s\n",
            "20:\tlearn: 0.0098173\ttotal: 2.51s\tremaining: 3.46s\n",
            "21:\tlearn: 0.0096926\ttotal: 2.62s\tremaining: 3.33s\n",
            "22:\tlearn: 0.0094901\ttotal: 2.74s\tremaining: 3.21s\n",
            "23:\tlearn: 0.0094098\ttotal: 2.85s\tremaining: 3.08s\n",
            "24:\tlearn: 0.0092404\ttotal: 2.97s\tremaining: 2.97s\n",
            "25:\tlearn: 0.0091335\ttotal: 3.08s\tremaining: 2.85s\n",
            "26:\tlearn: 0.0089729\ttotal: 3.22s\tremaining: 2.74s\n",
            "27:\tlearn: 0.0087841\ttotal: 3.33s\tremaining: 2.62s\n",
            "28:\tlearn: 0.0086750\ttotal: 3.45s\tremaining: 2.5s\n",
            "29:\tlearn: 0.0085334\ttotal: 3.56s\tremaining: 2.37s\n",
            "30:\tlearn: 0.0083794\ttotal: 3.68s\tremaining: 2.25s\n",
            "31:\tlearn: 0.0082330\ttotal: 3.79s\tremaining: 2.13s\n",
            "32:\tlearn: 0.0080583\ttotal: 3.9s\tremaining: 2.01s\n",
            "33:\tlearn: 0.0079782\ttotal: 4.01s\tremaining: 1.89s\n",
            "34:\tlearn: 0.0078853\ttotal: 4.13s\tremaining: 1.77s\n",
            "35:\tlearn: 0.0077912\ttotal: 4.25s\tremaining: 1.65s\n",
            "36:\tlearn: 0.0076640\ttotal: 4.36s\tremaining: 1.53s\n",
            "37:\tlearn: 0.0075690\ttotal: 4.47s\tremaining: 1.41s\n",
            "38:\tlearn: 0.0074500\ttotal: 4.59s\tremaining: 1.29s\n",
            "39:\tlearn: 0.0073078\ttotal: 4.7s\tremaining: 1.18s\n",
            "40:\tlearn: 0.0072425\ttotal: 4.83s\tremaining: 1.06s\n",
            "41:\tlearn: 0.0071634\ttotal: 4.94s\tremaining: 941ms\n",
            "42:\tlearn: 0.0070530\ttotal: 5.06s\tremaining: 824ms\n",
            "43:\tlearn: 0.0069760\ttotal: 5.17s\tremaining: 706ms\n",
            "44:\tlearn: 0.0069302\ttotal: 5.3s\tremaining: 589ms\n",
            "45:\tlearn: 0.0068472\ttotal: 5.41s\tremaining: 470ms\n",
            "46:\tlearn: 0.0066145\ttotal: 5.53s\tremaining: 353ms\n",
            "47:\tlearn: 0.0064911\ttotal: 5.63s\tremaining: 235ms\n",
            "48:\tlearn: 0.0063895\ttotal: 5.75s\tremaining: 117ms\n",
            "49:\tlearn: 0.0063016\ttotal: 5.86s\tremaining: 0us\n",
            "0:\tlearn: 0.0155097\ttotal: 170ms\tremaining: 8.34s\n",
            "1:\tlearn: 0.0142204\ttotal: 287ms\tremaining: 6.88s\n",
            "2:\tlearn: 0.0124468\ttotal: 411ms\tremaining: 6.43s\n",
            "3:\tlearn: 0.0111382\ttotal: 521ms\tremaining: 6s\n",
            "4:\tlearn: 0.0099373\ttotal: 638ms\tremaining: 5.74s\n",
            "5:\tlearn: 0.0089933\ttotal: 747ms\tremaining: 5.48s\n",
            "6:\tlearn: 0.0087757\ttotal: 870ms\tremaining: 5.34s\n",
            "7:\tlearn: 0.0082272\ttotal: 978ms\tremaining: 5.13s\n",
            "8:\tlearn: 0.0075724\ttotal: 1.09s\tremaining: 4.98s\n",
            "9:\tlearn: 0.0071981\ttotal: 1.2s\tremaining: 4.8s\n",
            "10:\tlearn: 0.0070394\ttotal: 1.33s\tremaining: 4.73s\n",
            "11:\tlearn: 0.0069581\ttotal: 1.45s\tremaining: 4.58s\n",
            "12:\tlearn: 0.0068964\ttotal: 1.57s\tremaining: 4.46s\n",
            "13:\tlearn: 0.0064702\ttotal: 1.68s\tremaining: 4.32s\n",
            "14:\tlearn: 0.0061387\ttotal: 1.79s\tremaining: 4.18s\n",
            "15:\tlearn: 0.0058611\ttotal: 1.91s\tremaining: 4.05s\n",
            "16:\tlearn: 0.0057298\ttotal: 2.03s\tremaining: 3.93s\n",
            "17:\tlearn: 0.0056343\ttotal: 2.13s\tremaining: 3.79s\n",
            "18:\tlearn: 0.0055883\ttotal: 2.25s\tremaining: 3.68s\n",
            "19:\tlearn: 0.0054692\ttotal: 2.38s\tremaining: 3.56s\n",
            "20:\tlearn: 0.0054056\ttotal: 2.49s\tremaining: 3.44s\n",
            "21:\tlearn: 0.0052988\ttotal: 2.6s\tremaining: 3.31s\n",
            "22:\tlearn: 0.0051820\ttotal: 2.73s\tremaining: 3.2s\n",
            "23:\tlearn: 0.0049983\ttotal: 2.83s\tremaining: 3.07s\n",
            "24:\tlearn: 0.0049291\ttotal: 2.96s\tremaining: 2.96s\n",
            "25:\tlearn: 0.0047976\ttotal: 3.07s\tremaining: 2.83s\n",
            "26:\tlearn: 0.0047061\ttotal: 3.19s\tremaining: 2.71s\n",
            "27:\tlearn: 0.0046585\ttotal: 3.29s\tremaining: 2.59s\n",
            "28:\tlearn: 0.0046004\ttotal: 3.43s\tremaining: 2.48s\n",
            "29:\tlearn: 0.0045653\ttotal: 3.54s\tremaining: 2.36s\n",
            "30:\tlearn: 0.0045312\ttotal: 3.66s\tremaining: 2.24s\n",
            "31:\tlearn: 0.0044724\ttotal: 3.77s\tremaining: 2.12s\n",
            "32:\tlearn: 0.0043299\ttotal: 3.88s\tremaining: 2s\n",
            "33:\tlearn: 0.0042991\ttotal: 4s\tremaining: 1.88s\n",
            "34:\tlearn: 0.0041854\ttotal: 4.11s\tremaining: 1.76s\n",
            "35:\tlearn: 0.0041430\ttotal: 4.23s\tremaining: 1.64s\n",
            "36:\tlearn: 0.0041188\ttotal: 4.35s\tremaining: 1.53s\n",
            "37:\tlearn: 0.0040465\ttotal: 4.47s\tremaining: 1.41s\n",
            "38:\tlearn: 0.0039692\ttotal: 4.59s\tremaining: 1.29s\n",
            "39:\tlearn: 0.0039181\ttotal: 4.7s\tremaining: 1.17s\n",
            "40:\tlearn: 0.0038910\ttotal: 4.82s\tremaining: 1.06s\n",
            "41:\tlearn: 0.0038705\ttotal: 4.93s\tremaining: 940ms\n",
            "42:\tlearn: 0.0038517\ttotal: 5.05s\tremaining: 822ms\n",
            "43:\tlearn: 0.0037879\ttotal: 5.16s\tremaining: 704ms\n",
            "44:\tlearn: 0.0037606\ttotal: 5.29s\tremaining: 587ms\n",
            "45:\tlearn: 0.0037382\ttotal: 5.39s\tremaining: 469ms\n",
            "46:\tlearn: 0.0037238\ttotal: 5.52s\tremaining: 353ms\n",
            "47:\tlearn: 0.0037079\ttotal: 5.63s\tremaining: 235ms\n",
            "48:\tlearn: 0.0036799\ttotal: 5.75s\tremaining: 117ms\n",
            "49:\tlearn: 0.0036470\ttotal: 5.86s\tremaining: 0us\n",
            "0:\tlearn: 0.0333562\ttotal: 168ms\tremaining: 8.22s\n",
            "1:\tlearn: 0.0273621\ttotal: 277ms\tremaining: 6.66s\n",
            "2:\tlearn: 0.0229532\ttotal: 403ms\tremaining: 6.32s\n",
            "3:\tlearn: 0.0198793\ttotal: 519ms\tremaining: 5.97s\n",
            "4:\tlearn: 0.0175824\ttotal: 639ms\tremaining: 5.75s\n",
            "5:\tlearn: 0.0159378\ttotal: 745ms\tremaining: 5.46s\n",
            "6:\tlearn: 0.0147769\ttotal: 864ms\tremaining: 5.3s\n",
            "7:\tlearn: 0.0140732\ttotal: 975ms\tremaining: 5.12s\n",
            "8:\tlearn: 0.0135733\ttotal: 1.1s\tremaining: 5.02s\n",
            "9:\tlearn: 0.0131303\ttotal: 1.22s\tremaining: 4.86s\n",
            "10:\tlearn: 0.0127790\ttotal: 1.33s\tremaining: 4.73s\n",
            "11:\tlearn: 0.0124206\ttotal: 1.44s\tremaining: 4.57s\n",
            "12:\tlearn: 0.0120643\ttotal: 1.58s\tremaining: 4.49s\n",
            "13:\tlearn: 0.0116662\ttotal: 1.68s\tremaining: 4.33s\n",
            "14:\tlearn: 0.0113995\ttotal: 1.8s\tremaining: 4.2s\n",
            "15:\tlearn: 0.0112215\ttotal: 1.91s\tremaining: 4.05s\n",
            "16:\tlearn: 0.0108105\ttotal: 2.02s\tremaining: 3.92s\n",
            "17:\tlearn: 0.0106211\ttotal: 2.13s\tremaining: 3.78s\n",
            "18:\tlearn: 0.0103596\ttotal: 2.25s\tremaining: 3.66s\n",
            "19:\tlearn: 0.0102146\ttotal: 2.35s\tremaining: 3.53s\n",
            "20:\tlearn: 0.0101399\ttotal: 2.47s\tremaining: 3.41s\n",
            "21:\tlearn: 0.0099850\ttotal: 2.58s\tremaining: 3.29s\n",
            "22:\tlearn: 0.0098136\ttotal: 2.71s\tremaining: 3.18s\n",
            "23:\tlearn: 0.0096869\ttotal: 2.81s\tremaining: 3.05s\n",
            "24:\tlearn: 0.0094127\ttotal: 2.93s\tremaining: 2.93s\n",
            "25:\tlearn: 0.0092978\ttotal: 3.04s\tremaining: 2.81s\n",
            "26:\tlearn: 0.0091631\ttotal: 3.16s\tremaining: 2.69s\n",
            "27:\tlearn: 0.0090407\ttotal: 3.27s\tremaining: 2.57s\n",
            "28:\tlearn: 0.0089017\ttotal: 3.39s\tremaining: 2.45s\n",
            "29:\tlearn: 0.0087445\ttotal: 3.5s\tremaining: 2.33s\n",
            "30:\tlearn: 0.0086269\ttotal: 3.63s\tremaining: 2.22s\n",
            "31:\tlearn: 0.0084778\ttotal: 3.73s\tremaining: 2.1s\n",
            "32:\tlearn: 0.0083210\ttotal: 3.86s\tremaining: 1.99s\n",
            "33:\tlearn: 0.0080892\ttotal: 3.97s\tremaining: 1.87s\n",
            "34:\tlearn: 0.0079124\ttotal: 4.08s\tremaining: 1.75s\n",
            "35:\tlearn: 0.0078424\ttotal: 4.2s\tremaining: 1.63s\n",
            "36:\tlearn: 0.0077808\ttotal: 4.32s\tremaining: 1.52s\n",
            "37:\tlearn: 0.0076863\ttotal: 4.43s\tremaining: 1.4s\n",
            "38:\tlearn: 0.0074839\ttotal: 4.55s\tremaining: 1.28s\n",
            "39:\tlearn: 0.0073581\ttotal: 4.67s\tremaining: 1.17s\n",
            "40:\tlearn: 0.0072294\ttotal: 4.79s\tremaining: 1.05s\n",
            "41:\tlearn: 0.0070862\ttotal: 4.89s\tremaining: 932ms\n",
            "42:\tlearn: 0.0070381\ttotal: 5.02s\tremaining: 817ms\n",
            "43:\tlearn: 0.0069832\ttotal: 5.13s\tremaining: 700ms\n",
            "44:\tlearn: 0.0068711\ttotal: 5.25s\tremaining: 583ms\n",
            "45:\tlearn: 0.0067758\ttotal: 5.36s\tremaining: 466ms\n",
            "46:\tlearn: 0.0066595\ttotal: 5.47s\tremaining: 349ms\n",
            "47:\tlearn: 0.0065196\ttotal: 5.59s\tremaining: 233ms\n",
            "48:\tlearn: 0.0064510\ttotal: 5.72s\tremaining: 117ms\n",
            "49:\tlearn: 0.0064032\ttotal: 5.83s\tremaining: 0us\n",
            "0:\tlearn: 0.2199217\ttotal: 172ms\tremaining: 8.4s\n",
            "1:\tlearn: 0.1815280\ttotal: 284ms\tremaining: 6.81s\n",
            "2:\tlearn: 0.1584585\ttotal: 408ms\tremaining: 6.4s\n",
            "3:\tlearn: 0.1385587\ttotal: 521ms\tremaining: 5.99s\n",
            "4:\tlearn: 0.1252809\ttotal: 644ms\tremaining: 5.8s\n",
            "5:\tlearn: 0.1165963\ttotal: 760ms\tremaining: 5.57s\n",
            "6:\tlearn: 0.1087441\ttotal: 897ms\tremaining: 5.51s\n",
            "7:\tlearn: 0.1039003\ttotal: 1.01s\tremaining: 5.32s\n",
            "8:\tlearn: 0.1002889\ttotal: 1.13s\tremaining: 5.16s\n",
            "9:\tlearn: 0.0979542\ttotal: 1.25s\tremaining: 4.99s\n",
            "10:\tlearn: 0.0955946\ttotal: 1.37s\tremaining: 4.87s\n",
            "11:\tlearn: 0.0937976\ttotal: 1.49s\tremaining: 4.71s\n",
            "12:\tlearn: 0.0905485\ttotal: 1.6s\tremaining: 4.57s\n",
            "13:\tlearn: 0.0880863\ttotal: 1.72s\tremaining: 4.42s\n",
            "14:\tlearn: 0.0870188\ttotal: 1.85s\tremaining: 4.33s\n",
            "15:\tlearn: 0.0858302\ttotal: 1.97s\tremaining: 4.19s\n",
            "16:\tlearn: 0.0847704\ttotal: 2.09s\tremaining: 4.06s\n",
            "17:\tlearn: 0.0840646\ttotal: 2.21s\tremaining: 3.94s\n",
            "18:\tlearn: 0.0828715\ttotal: 2.34s\tremaining: 3.82s\n",
            "19:\tlearn: 0.0818951\ttotal: 2.46s\tremaining: 3.69s\n",
            "20:\tlearn: 0.0799324\ttotal: 2.58s\tremaining: 3.56s\n",
            "21:\tlearn: 0.0788700\ttotal: 2.69s\tremaining: 3.43s\n",
            "22:\tlearn: 0.0775431\ttotal: 2.81s\tremaining: 3.3s\n",
            "23:\tlearn: 0.0762993\ttotal: 2.94s\tremaining: 3.18s\n",
            "24:\tlearn: 0.0751821\ttotal: 3.06s\tremaining: 3.06s\n",
            "25:\tlearn: 0.0737489\ttotal: 3.18s\tremaining: 2.93s\n",
            "26:\tlearn: 0.0726474\ttotal: 3.3s\tremaining: 2.81s\n",
            "27:\tlearn: 0.0716876\ttotal: 3.41s\tremaining: 2.68s\n",
            "28:\tlearn: 0.0705029\ttotal: 3.54s\tremaining: 2.56s\n",
            "29:\tlearn: 0.0693528\ttotal: 3.65s\tremaining: 2.44s\n",
            "30:\tlearn: 0.0687171\ttotal: 3.78s\tremaining: 2.32s\n",
            "31:\tlearn: 0.0675767\ttotal: 3.91s\tremaining: 2.2s\n",
            "32:\tlearn: 0.0667744\ttotal: 4.04s\tremaining: 2.08s\n",
            "33:\tlearn: 0.0659365\ttotal: 4.16s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0653325\ttotal: 4.29s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0644827\ttotal: 4.41s\tremaining: 1.71s\n",
            "36:\tlearn: 0.0637519\ttotal: 4.53s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0629355\ttotal: 4.65s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0613694\ttotal: 4.78s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0606082\ttotal: 4.9s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0595966\ttotal: 5.03s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0591706\ttotal: 5.15s\tremaining: 982ms\n",
            "42:\tlearn: 0.0583782\ttotal: 5.28s\tremaining: 859ms\n",
            "43:\tlearn: 0.0575882\ttotal: 5.39s\tremaining: 735ms\n",
            "44:\tlearn: 0.0568484\ttotal: 5.52s\tremaining: 613ms\n",
            "45:\tlearn: 0.0557281\ttotal: 5.63s\tremaining: 490ms\n",
            "46:\tlearn: 0.0550895\ttotal: 5.76s\tremaining: 368ms\n",
            "47:\tlearn: 0.0541912\ttotal: 5.88s\tremaining: 245ms\n",
            "48:\tlearn: 0.0537157\ttotal: 6.01s\tremaining: 123ms\n",
            "49:\tlearn: 0.0526228\ttotal: 6.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0318118\ttotal: 180ms\tremaining: 8.83s\n",
            "1:\tlearn: 0.0253742\ttotal: 292ms\tremaining: 7.02s\n",
            "2:\tlearn: 0.0208328\ttotal: 414ms\tremaining: 6.48s\n",
            "3:\tlearn: 0.0181003\ttotal: 529ms\tremaining: 6.09s\n",
            "4:\tlearn: 0.0163766\ttotal: 655ms\tremaining: 5.9s\n",
            "5:\tlearn: 0.0149559\ttotal: 788ms\tremaining: 5.78s\n",
            "6:\tlearn: 0.0138170\ttotal: 911ms\tremaining: 5.6s\n",
            "7:\tlearn: 0.0130414\ttotal: 1.03s\tremaining: 5.42s\n",
            "8:\tlearn: 0.0124255\ttotal: 1.16s\tremaining: 5.27s\n",
            "9:\tlearn: 0.0121561\ttotal: 1.28s\tremaining: 5.1s\n",
            "10:\tlearn: 0.0119617\ttotal: 1.41s\tremaining: 5s\n",
            "11:\tlearn: 0.0116642\ttotal: 1.54s\tremaining: 4.87s\n",
            "12:\tlearn: 0.0114606\ttotal: 1.66s\tremaining: 4.73s\n",
            "13:\tlearn: 0.0111083\ttotal: 1.8s\tremaining: 4.62s\n",
            "14:\tlearn: 0.0109234\ttotal: 1.93s\tremaining: 4.5s\n",
            "15:\tlearn: 0.0105326\ttotal: 2.05s\tremaining: 4.35s\n",
            "16:\tlearn: 0.0101807\ttotal: 2.17s\tremaining: 4.21s\n",
            "17:\tlearn: 0.0099525\ttotal: 2.28s\tremaining: 4.06s\n",
            "18:\tlearn: 0.0097776\ttotal: 2.41s\tremaining: 3.93s\n",
            "19:\tlearn: 0.0096122\ttotal: 2.52s\tremaining: 3.78s\n",
            "20:\tlearn: 0.0094695\ttotal: 2.65s\tremaining: 3.66s\n",
            "21:\tlearn: 0.0093603\ttotal: 2.76s\tremaining: 3.52s\n",
            "22:\tlearn: 0.0092088\ttotal: 2.89s\tremaining: 3.4s\n",
            "23:\tlearn: 0.0091141\ttotal: 3s\tremaining: 3.26s\n",
            "24:\tlearn: 0.0089715\ttotal: 3.13s\tremaining: 3.13s\n",
            "25:\tlearn: 0.0088542\ttotal: 3.24s\tremaining: 2.99s\n",
            "26:\tlearn: 0.0087321\ttotal: 3.37s\tremaining: 2.87s\n",
            "27:\tlearn: 0.0085841\ttotal: 3.48s\tremaining: 2.73s\n",
            "28:\tlearn: 0.0084701\ttotal: 3.6s\tremaining: 2.61s\n",
            "29:\tlearn: 0.0083605\ttotal: 3.71s\tremaining: 2.47s\n",
            "30:\tlearn: 0.0082922\ttotal: 3.84s\tremaining: 2.36s\n",
            "31:\tlearn: 0.0081442\ttotal: 3.96s\tremaining: 2.23s\n",
            "32:\tlearn: 0.0081013\ttotal: 4.09s\tremaining: 2.1s\n",
            "33:\tlearn: 0.0078842\ttotal: 4.2s\tremaining: 1.98s\n",
            "34:\tlearn: 0.0078032\ttotal: 4.33s\tremaining: 1.85s\n",
            "35:\tlearn: 0.0076466\ttotal: 4.44s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0075295\ttotal: 4.57s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0074257\ttotal: 4.68s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0073554\ttotal: 4.8s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0072561\ttotal: 4.93s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0072095\ttotal: 5.05s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0071163\ttotal: 5.17s\tremaining: 985ms\n",
            "42:\tlearn: 0.0070256\ttotal: 5.29s\tremaining: 861ms\n",
            "43:\tlearn: 0.0069243\ttotal: 5.4s\tremaining: 737ms\n",
            "44:\tlearn: 0.0067813\ttotal: 5.52s\tremaining: 614ms\n",
            "45:\tlearn: 0.0067218\ttotal: 5.64s\tremaining: 490ms\n",
            "46:\tlearn: 0.0066438\ttotal: 5.76s\tremaining: 368ms\n",
            "47:\tlearn: 0.0065592\ttotal: 5.88s\tremaining: 245ms\n",
            "48:\tlearn: 0.0065184\ttotal: 6.01s\tremaining: 123ms\n",
            "49:\tlearn: 0.0064438\ttotal: 6.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0182177\ttotal: 183ms\tremaining: 8.97s\n",
            "1:\tlearn: 0.0161217\ttotal: 296ms\tremaining: 7.09s\n",
            "2:\tlearn: 0.0143879\ttotal: 418ms\tremaining: 6.54s\n",
            "3:\tlearn: 0.0126819\ttotal: 531ms\tremaining: 6.11s\n",
            "4:\tlearn: 0.0111723\ttotal: 652ms\tremaining: 5.87s\n",
            "5:\tlearn: 0.0104888\ttotal: 783ms\tremaining: 5.74s\n",
            "6:\tlearn: 0.0095701\ttotal: 906ms\tremaining: 5.57s\n",
            "7:\tlearn: 0.0090415\ttotal: 1.02s\tremaining: 5.38s\n",
            "8:\tlearn: 0.0086047\ttotal: 1.15s\tremaining: 5.23s\n",
            "9:\tlearn: 0.0082183\ttotal: 1.26s\tremaining: 5.04s\n",
            "10:\tlearn: 0.0079073\ttotal: 1.39s\tremaining: 4.91s\n",
            "11:\tlearn: 0.0078446\ttotal: 1.5s\tremaining: 4.76s\n",
            "12:\tlearn: 0.0074720\ttotal: 1.62s\tremaining: 4.62s\n",
            "13:\tlearn: 0.0071964\ttotal: 1.75s\tremaining: 4.51s\n",
            "14:\tlearn: 0.0069669\ttotal: 1.88s\tremaining: 4.38s\n",
            "15:\tlearn: 0.0068056\ttotal: 2s\tremaining: 4.24s\n",
            "16:\tlearn: 0.0066602\ttotal: 2.12s\tremaining: 4.11s\n",
            "17:\tlearn: 0.0065403\ttotal: 2.24s\tremaining: 3.98s\n",
            "18:\tlearn: 0.0064038\ttotal: 2.36s\tremaining: 3.85s\n",
            "19:\tlearn: 0.0061838\ttotal: 2.47s\tremaining: 3.71s\n",
            "20:\tlearn: 0.0060912\ttotal: 2.6s\tremaining: 3.59s\n",
            "21:\tlearn: 0.0060431\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0059802\ttotal: 2.85s\tremaining: 3.35s\n",
            "23:\tlearn: 0.0058260\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0057475\ttotal: 3.08s\tremaining: 3.08s\n",
            "25:\tlearn: 0.0056954\ttotal: 3.2s\tremaining: 2.95s\n",
            "26:\tlearn: 0.0056565\ttotal: 3.32s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0055427\ttotal: 3.43s\tremaining: 2.7s\n",
            "28:\tlearn: 0.0054812\ttotal: 3.56s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0052887\ttotal: 3.68s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0052249\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0051665\ttotal: 3.94s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0050101\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0048983\ttotal: 4.17s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0048257\ttotal: 4.29s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0047904\ttotal: 4.4s\tremaining: 1.71s\n",
            "36:\tlearn: 0.0046789\ttotal: 4.52s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0046324\ttotal: 4.64s\tremaining: 1.46s\n",
            "38:\tlearn: 0.0046079\ttotal: 4.76s\tremaining: 1.34s\n",
            "39:\tlearn: 0.0045909\ttotal: 4.88s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0045650\ttotal: 5.01s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0044315\ttotal: 5.12s\tremaining: 976ms\n",
            "42:\tlearn: 0.0044018\ttotal: 5.24s\tremaining: 854ms\n",
            "43:\tlearn: 0.0043619\ttotal: 5.36s\tremaining: 731ms\n",
            "44:\tlearn: 0.0043250\ttotal: 5.48s\tremaining: 609ms\n",
            "45:\tlearn: 0.0042727\ttotal: 5.59s\tremaining: 486ms\n",
            "46:\tlearn: 0.0042255\ttotal: 5.71s\tremaining: 365ms\n",
            "47:\tlearn: 0.0042132\ttotal: 5.84s\tremaining: 243ms\n",
            "48:\tlearn: 0.0041796\ttotal: 5.96s\tremaining: 122ms\n",
            "49:\tlearn: 0.0040907\ttotal: 6.08s\tremaining: 0us\n",
            "0:\tlearn: 0.0327340\ttotal: 187ms\tremaining: 9.16s\n",
            "1:\tlearn: 0.0273164\ttotal: 298ms\tremaining: 7.15s\n",
            "2:\tlearn: 0.0229888\ttotal: 417ms\tremaining: 6.54s\n",
            "3:\tlearn: 0.0196256\ttotal: 537ms\tremaining: 6.17s\n",
            "4:\tlearn: 0.0177530\ttotal: 662ms\tremaining: 5.96s\n",
            "5:\tlearn: 0.0163322\ttotal: 793ms\tremaining: 5.81s\n",
            "6:\tlearn: 0.0151771\ttotal: 914ms\tremaining: 5.62s\n",
            "7:\tlearn: 0.0144555\ttotal: 1.03s\tremaining: 5.42s\n",
            "8:\tlearn: 0.0139290\ttotal: 1.16s\tremaining: 5.29s\n",
            "9:\tlearn: 0.0134747\ttotal: 1.27s\tremaining: 5.1s\n",
            "10:\tlearn: 0.0131337\ttotal: 1.4s\tremaining: 4.96s\n",
            "11:\tlearn: 0.0127765\ttotal: 1.51s\tremaining: 4.79s\n",
            "12:\tlearn: 0.0123517\ttotal: 1.64s\tremaining: 4.66s\n",
            "13:\tlearn: 0.0121707\ttotal: 1.76s\tremaining: 4.53s\n",
            "14:\tlearn: 0.0118710\ttotal: 1.89s\tremaining: 4.4s\n",
            "15:\tlearn: 0.0116874\ttotal: 2s\tremaining: 4.25s\n",
            "16:\tlearn: 0.0113011\ttotal: 2.12s\tremaining: 4.12s\n",
            "17:\tlearn: 0.0111357\ttotal: 2.23s\tremaining: 3.97s\n",
            "18:\tlearn: 0.0110283\ttotal: 2.36s\tremaining: 3.85s\n",
            "19:\tlearn: 0.0108925\ttotal: 2.48s\tremaining: 3.71s\n",
            "20:\tlearn: 0.0106938\ttotal: 2.6s\tremaining: 3.59s\n",
            "21:\tlearn: 0.0105935\ttotal: 2.71s\tremaining: 3.45s\n",
            "22:\tlearn: 0.0104395\ttotal: 2.85s\tremaining: 3.34s\n",
            "23:\tlearn: 0.0102435\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0100814\ttotal: 3.08s\tremaining: 3.08s\n",
            "25:\tlearn: 0.0099322\ttotal: 3.2s\tremaining: 2.96s\n",
            "26:\tlearn: 0.0098314\ttotal: 3.33s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0096543\ttotal: 3.44s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0095323\ttotal: 3.57s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0094064\ttotal: 3.69s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0092920\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0091631\ttotal: 3.93s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0090510\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0088755\ttotal: 4.17s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0086754\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0085763\ttotal: 4.42s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0084982\ttotal: 4.54s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0083124\ttotal: 4.65s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0082033\ttotal: 4.77s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0081058\ttotal: 4.9s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0080201\ttotal: 5.02s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0078839\ttotal: 5.13s\tremaining: 978ms\n",
            "42:\tlearn: 0.0077731\ttotal: 5.26s\tremaining: 856ms\n",
            "43:\tlearn: 0.0077255\ttotal: 5.38s\tremaining: 734ms\n",
            "44:\tlearn: 0.0076097\ttotal: 5.5s\tremaining: 611ms\n",
            "45:\tlearn: 0.0074932\ttotal: 5.62s\tremaining: 488ms\n",
            "46:\tlearn: 0.0073958\ttotal: 5.74s\tremaining: 366ms\n",
            "47:\tlearn: 0.0073405\ttotal: 5.85s\tremaining: 244ms\n",
            "48:\tlearn: 0.0072541\ttotal: 5.99s\tremaining: 122ms\n",
            "49:\tlearn: 0.0071476\ttotal: 6.1s\tremaining: 0us\n",
            "0:\tlearn: 0.2384169\ttotal: 167ms\tremaining: 8.17s\n",
            "1:\tlearn: 0.1963859\ttotal: 284ms\tremaining: 6.8s\n",
            "2:\tlearn: 0.1690119\ttotal: 412ms\tremaining: 6.46s\n",
            "3:\tlearn: 0.1484790\ttotal: 527ms\tremaining: 6.06s\n",
            "4:\tlearn: 0.1334943\ttotal: 650ms\tremaining: 5.85s\n",
            "5:\tlearn: 0.1221070\ttotal: 778ms\tremaining: 5.71s\n",
            "6:\tlearn: 0.1159401\ttotal: 901ms\tremaining: 5.54s\n",
            "7:\tlearn: 0.1107958\ttotal: 1.01s\tremaining: 5.32s\n",
            "8:\tlearn: 0.1071237\ttotal: 1.14s\tremaining: 5.18s\n",
            "9:\tlearn: 0.1045413\ttotal: 1.26s\tremaining: 5.04s\n",
            "10:\tlearn: 0.1014757\ttotal: 1.38s\tremaining: 4.9s\n",
            "11:\tlearn: 0.1000043\ttotal: 1.5s\tremaining: 4.75s\n",
            "12:\tlearn: 0.0990446\ttotal: 1.64s\tremaining: 4.65s\n",
            "13:\tlearn: 0.0964366\ttotal: 1.76s\tremaining: 4.54s\n",
            "14:\tlearn: 0.0944081\ttotal: 1.89s\tremaining: 4.41s\n",
            "15:\tlearn: 0.0936336\ttotal: 2.01s\tremaining: 4.27s\n",
            "16:\tlearn: 0.0924594\ttotal: 2.13s\tremaining: 4.14s\n",
            "17:\tlearn: 0.0907193\ttotal: 2.25s\tremaining: 3.99s\n",
            "18:\tlearn: 0.0891779\ttotal: 2.37s\tremaining: 3.86s\n",
            "19:\tlearn: 0.0882741\ttotal: 2.48s\tremaining: 3.73s\n",
            "20:\tlearn: 0.0870030\ttotal: 2.6s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0856082\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0838007\ttotal: 2.86s\tremaining: 3.36s\n",
            "23:\tlearn: 0.0828539\ttotal: 2.98s\tremaining: 3.22s\n",
            "24:\tlearn: 0.0813064\ttotal: 3.1s\tremaining: 3.1s\n",
            "25:\tlearn: 0.0803149\ttotal: 3.22s\tremaining: 2.97s\n",
            "26:\tlearn: 0.0793138\ttotal: 3.34s\tremaining: 2.85s\n",
            "27:\tlearn: 0.0783160\ttotal: 3.46s\tremaining: 2.72s\n",
            "28:\tlearn: 0.0767385\ttotal: 3.58s\tremaining: 2.59s\n",
            "29:\tlearn: 0.0759187\ttotal: 3.69s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0752086\ttotal: 3.83s\tremaining: 2.35s\n",
            "31:\tlearn: 0.0747344\ttotal: 3.94s\tremaining: 2.22s\n",
            "32:\tlearn: 0.0733751\ttotal: 4.07s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0726028\ttotal: 4.18s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0715257\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0704408\ttotal: 4.42s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0695259\ttotal: 4.54s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0686441\ttotal: 4.66s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0675732\ttotal: 4.79s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0665770\ttotal: 4.91s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0658719\ttotal: 5.03s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0649214\ttotal: 5.15s\tremaining: 981ms\n",
            "42:\tlearn: 0.0643943\ttotal: 5.27s\tremaining: 858ms\n",
            "43:\tlearn: 0.0637059\ttotal: 5.38s\tremaining: 734ms\n",
            "44:\tlearn: 0.0631395\ttotal: 5.51s\tremaining: 612ms\n",
            "45:\tlearn: 0.0623104\ttotal: 5.62s\tremaining: 489ms\n",
            "46:\tlearn: 0.0614144\ttotal: 5.74s\tremaining: 367ms\n",
            "47:\tlearn: 0.0606966\ttotal: 5.87s\tremaining: 245ms\n",
            "48:\tlearn: 0.0598202\ttotal: 6s\tremaining: 122ms\n",
            "49:\tlearn: 0.0594531\ttotal: 6.12s\tremaining: 0us\n",
            "0:\tlearn: 0.0338442\ttotal: 176ms\tremaining: 8.6s\n",
            "1:\tlearn: 0.0278865\ttotal: 289ms\tremaining: 6.93s\n",
            "2:\tlearn: 0.0234736\ttotal: 413ms\tremaining: 6.47s\n",
            "3:\tlearn: 0.0200955\ttotal: 524ms\tremaining: 6.03s\n",
            "4:\tlearn: 0.0177513\ttotal: 661ms\tremaining: 5.95s\n",
            "5:\tlearn: 0.0162127\ttotal: 778ms\tremaining: 5.7s\n",
            "6:\tlearn: 0.0150245\ttotal: 903ms\tremaining: 5.54s\n",
            "7:\tlearn: 0.0141659\ttotal: 1.02s\tremaining: 5.34s\n",
            "8:\tlearn: 0.0134658\ttotal: 1.16s\tremaining: 5.27s\n",
            "9:\tlearn: 0.0129477\ttotal: 1.27s\tremaining: 5.08s\n",
            "10:\tlearn: 0.0123690\ttotal: 1.4s\tremaining: 4.95s\n",
            "11:\tlearn: 0.0120243\ttotal: 1.51s\tremaining: 4.79s\n",
            "12:\tlearn: 0.0117361\ttotal: 1.63s\tremaining: 4.65s\n",
            "13:\tlearn: 0.0114447\ttotal: 1.76s\tremaining: 4.53s\n",
            "14:\tlearn: 0.0111761\ttotal: 1.88s\tremaining: 4.4s\n",
            "15:\tlearn: 0.0109298\ttotal: 2s\tremaining: 4.24s\n",
            "16:\tlearn: 0.0108153\ttotal: 2.12s\tremaining: 4.11s\n",
            "17:\tlearn: 0.0106020\ttotal: 2.24s\tremaining: 3.98s\n",
            "18:\tlearn: 0.0104866\ttotal: 2.36s\tremaining: 3.85s\n",
            "19:\tlearn: 0.0103671\ttotal: 2.47s\tremaining: 3.7s\n",
            "20:\tlearn: 0.0102255\ttotal: 2.59s\tremaining: 3.58s\n",
            "21:\tlearn: 0.0100856\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0099609\ttotal: 2.85s\tremaining: 3.34s\n",
            "23:\tlearn: 0.0097970\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0096114\ttotal: 3.08s\tremaining: 3.08s\n",
            "25:\tlearn: 0.0095147\ttotal: 3.2s\tremaining: 2.96s\n",
            "26:\tlearn: 0.0093511\ttotal: 3.33s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0092569\ttotal: 3.44s\tremaining: 2.7s\n",
            "28:\tlearn: 0.0091143\ttotal: 3.57s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0090202\ttotal: 3.68s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0089142\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0088753\ttotal: 3.93s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0088169\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0087074\ttotal: 4.17s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0085898\ttotal: 4.29s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0085255\ttotal: 4.4s\tremaining: 1.71s\n",
            "36:\tlearn: 0.0083873\ttotal: 4.52s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0083101\ttotal: 4.64s\tremaining: 1.46s\n",
            "38:\tlearn: 0.0082176\ttotal: 4.77s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0081287\ttotal: 4.89s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0080477\ttotal: 5.01s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0079346\ttotal: 5.13s\tremaining: 976ms\n",
            "42:\tlearn: 0.0078426\ttotal: 5.25s\tremaining: 855ms\n",
            "43:\tlearn: 0.0077553\ttotal: 5.37s\tremaining: 732ms\n",
            "44:\tlearn: 0.0076971\ttotal: 5.49s\tremaining: 610ms\n",
            "45:\tlearn: 0.0076074\ttotal: 5.6s\tremaining: 487ms\n",
            "46:\tlearn: 0.0075255\ttotal: 5.73s\tremaining: 366ms\n",
            "47:\tlearn: 0.0074596\ttotal: 5.86s\tremaining: 244ms\n",
            "48:\tlearn: 0.0073770\ttotal: 5.98s\tremaining: 122ms\n",
            "49:\tlearn: 0.0072929\ttotal: 6.11s\tremaining: 0us\n",
            "0:\tlearn: 0.0229455\ttotal: 173ms\tremaining: 8.47s\n",
            "1:\tlearn: 0.0206243\ttotal: 285ms\tremaining: 6.84s\n",
            "2:\tlearn: 0.0176702\ttotal: 410ms\tremaining: 6.42s\n",
            "3:\tlearn: 0.0159721\ttotal: 525ms\tremaining: 6.04s\n",
            "4:\tlearn: 0.0139387\ttotal: 658ms\tremaining: 5.92s\n",
            "5:\tlearn: 0.0127185\ttotal: 776ms\tremaining: 5.69s\n",
            "6:\tlearn: 0.0120043\ttotal: 904ms\tremaining: 5.55s\n",
            "7:\tlearn: 0.0113491\ttotal: 1.02s\tremaining: 5.34s\n",
            "8:\tlearn: 0.0108918\ttotal: 1.14s\tremaining: 5.21s\n",
            "9:\tlearn: 0.0105710\ttotal: 1.25s\tremaining: 5.02s\n",
            "10:\tlearn: 0.0102535\ttotal: 1.38s\tremaining: 4.9s\n",
            "11:\tlearn: 0.0100334\ttotal: 1.5s\tremaining: 4.75s\n",
            "12:\tlearn: 0.0097943\ttotal: 1.62s\tremaining: 4.62s\n",
            "13:\tlearn: 0.0094330\ttotal: 1.75s\tremaining: 4.49s\n",
            "14:\tlearn: 0.0088905\ttotal: 1.87s\tremaining: 4.36s\n",
            "15:\tlearn: 0.0086252\ttotal: 1.98s\tremaining: 4.22s\n",
            "16:\tlearn: 0.0084399\ttotal: 2.12s\tremaining: 4.12s\n",
            "17:\tlearn: 0.0082846\ttotal: 2.24s\tremaining: 3.98s\n",
            "18:\tlearn: 0.0081078\ttotal: 2.36s\tremaining: 3.86s\n",
            "19:\tlearn: 0.0078996\ttotal: 2.49s\tremaining: 3.73s\n",
            "20:\tlearn: 0.0078109\ttotal: 2.61s\tremaining: 3.61s\n",
            "21:\tlearn: 0.0077314\ttotal: 2.74s\tremaining: 3.48s\n",
            "22:\tlearn: 0.0076947\ttotal: 2.86s\tremaining: 3.36s\n",
            "23:\tlearn: 0.0074933\ttotal: 2.98s\tremaining: 3.22s\n",
            "24:\tlearn: 0.0073323\ttotal: 3.1s\tremaining: 3.1s\n",
            "25:\tlearn: 0.0072276\ttotal: 3.21s\tremaining: 2.96s\n",
            "26:\tlearn: 0.0072116\ttotal: 3.34s\tremaining: 2.84s\n",
            "27:\tlearn: 0.0070188\ttotal: 3.45s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0069679\ttotal: 3.57s\tremaining: 2.59s\n",
            "29:\tlearn: 0.0069359\ttotal: 3.69s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0067779\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0067099\ttotal: 3.94s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0066543\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0065964\ttotal: 4.18s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0064560\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0063457\ttotal: 4.42s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0061620\ttotal: 4.54s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0060040\ttotal: 4.66s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0059598\ttotal: 4.79s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0058409\ttotal: 4.9s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0057615\ttotal: 5.03s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0057248\ttotal: 5.15s\tremaining: 980ms\n",
            "42:\tlearn: 0.0056573\ttotal: 5.27s\tremaining: 858ms\n",
            "43:\tlearn: 0.0056076\ttotal: 5.39s\tremaining: 735ms\n",
            "44:\tlearn: 0.0055515\ttotal: 5.52s\tremaining: 613ms\n",
            "45:\tlearn: 0.0055116\ttotal: 5.63s\tremaining: 490ms\n",
            "46:\tlearn: 0.0054925\ttotal: 5.77s\tremaining: 368ms\n",
            "47:\tlearn: 0.0054614\ttotal: 5.89s\tremaining: 245ms\n",
            "48:\tlearn: 0.0053998\ttotal: 6.01s\tremaining: 123ms\n",
            "49:\tlearn: 0.0053605\ttotal: 6.12s\tremaining: 0us\n",
            "0:\tlearn: 0.0355755\ttotal: 178ms\tremaining: 8.71s\n",
            "1:\tlearn: 0.0285742\ttotal: 290ms\tremaining: 6.95s\n",
            "2:\tlearn: 0.0239348\ttotal: 427ms\tremaining: 6.69s\n",
            "3:\tlearn: 0.0206503\ttotal: 546ms\tremaining: 6.28s\n",
            "4:\tlearn: 0.0184094\ttotal: 680ms\tremaining: 6.12s\n",
            "5:\tlearn: 0.0168831\ttotal: 794ms\tremaining: 5.82s\n",
            "6:\tlearn: 0.0157845\ttotal: 921ms\tremaining: 5.66s\n",
            "7:\tlearn: 0.0148808\ttotal: 1.03s\tremaining: 5.41s\n",
            "8:\tlearn: 0.0143477\ttotal: 1.15s\tremaining: 5.24s\n",
            "9:\tlearn: 0.0139512\ttotal: 1.26s\tremaining: 5.06s\n",
            "10:\tlearn: 0.0137048\ttotal: 1.38s\tremaining: 4.9s\n",
            "11:\tlearn: 0.0133511\ttotal: 1.5s\tremaining: 4.75s\n",
            "12:\tlearn: 0.0131323\ttotal: 1.63s\tremaining: 4.64s\n",
            "13:\tlearn: 0.0128302\ttotal: 1.75s\tremaining: 4.5s\n",
            "14:\tlearn: 0.0125961\ttotal: 1.87s\tremaining: 4.37s\n",
            "15:\tlearn: 0.0123892\ttotal: 1.98s\tremaining: 4.22s\n",
            "16:\tlearn: 0.0121161\ttotal: 2.11s\tremaining: 4.09s\n",
            "17:\tlearn: 0.0118256\ttotal: 2.23s\tremaining: 3.96s\n",
            "18:\tlearn: 0.0115968\ttotal: 2.35s\tremaining: 3.83s\n",
            "19:\tlearn: 0.0113922\ttotal: 2.46s\tremaining: 3.7s\n",
            "20:\tlearn: 0.0113010\ttotal: 2.6s\tremaining: 3.59s\n",
            "21:\tlearn: 0.0111652\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0109757\ttotal: 2.85s\tremaining: 3.34s\n",
            "23:\tlearn: 0.0108083\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0107177\ttotal: 3.09s\tremaining: 3.09s\n",
            "25:\tlearn: 0.0106377\ttotal: 3.21s\tremaining: 2.96s\n",
            "26:\tlearn: 0.0104889\ttotal: 3.33s\tremaining: 2.84s\n",
            "27:\tlearn: 0.0103194\ttotal: 3.45s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0101938\ttotal: 3.58s\tremaining: 2.59s\n",
            "29:\tlearn: 0.0100780\ttotal: 3.7s\tremaining: 2.47s\n",
            "30:\tlearn: 0.0099870\ttotal: 3.84s\tremaining: 2.35s\n",
            "31:\tlearn: 0.0098470\ttotal: 3.95s\tremaining: 2.22s\n",
            "32:\tlearn: 0.0097694\ttotal: 4.08s\tremaining: 2.1s\n",
            "33:\tlearn: 0.0096363\ttotal: 4.2s\tremaining: 1.98s\n",
            "34:\tlearn: 0.0095369\ttotal: 4.33s\tremaining: 1.85s\n",
            "35:\tlearn: 0.0094489\ttotal: 4.45s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0093387\ttotal: 4.57s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0092461\ttotal: 4.69s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0091257\ttotal: 4.82s\tremaining: 1.36s\n",
            "39:\tlearn: 0.0089923\ttotal: 4.93s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0089157\ttotal: 5.05s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0087849\ttotal: 5.17s\tremaining: 985ms\n",
            "42:\tlearn: 0.0086853\ttotal: 5.3s\tremaining: 863ms\n",
            "43:\tlearn: 0.0085904\ttotal: 5.42s\tremaining: 739ms\n",
            "44:\tlearn: 0.0084868\ttotal: 5.53s\tremaining: 615ms\n",
            "45:\tlearn: 0.0083676\ttotal: 5.65s\tremaining: 491ms\n",
            "46:\tlearn: 0.0082540\ttotal: 5.79s\tremaining: 370ms\n",
            "47:\tlearn: 0.0081479\ttotal: 5.92s\tremaining: 246ms\n",
            "48:\tlearn: 0.0080928\ttotal: 6.04s\tremaining: 123ms\n",
            "49:\tlearn: 0.0080293\ttotal: 6.16s\tremaining: 0us\n",
            "0:\tlearn: 0.2038600\ttotal: 148ms\tremaining: 7.26s\n",
            "1:\tlearn: 0.1854309\ttotal: 247ms\tremaining: 5.92s\n",
            "2:\tlearn: 0.1683833\ttotal: 341ms\tremaining: 5.34s\n",
            "3:\tlearn: 0.1545368\ttotal: 441ms\tremaining: 5.08s\n",
            "4:\tlearn: 0.1436283\ttotal: 546ms\tremaining: 4.92s\n",
            "5:\tlearn: 0.1323538\ttotal: 659ms\tremaining: 4.83s\n",
            "6:\tlearn: 0.1221331\ttotal: 764ms\tremaining: 4.69s\n",
            "7:\tlearn: 0.1144122\ttotal: 858ms\tremaining: 4.5s\n",
            "8:\tlearn: 0.1088305\ttotal: 967ms\tremaining: 4.41s\n",
            "9:\tlearn: 0.1045895\ttotal: 1.06s\tremaining: 4.24s\n",
            "10:\tlearn: 0.1012815\ttotal: 1.15s\tremaining: 4.09s\n",
            "11:\tlearn: 0.0966670\ttotal: 1.26s\tremaining: 4s\n",
            "12:\tlearn: 0.0928210\ttotal: 1.36s\tremaining: 3.87s\n",
            "13:\tlearn: 0.0894193\ttotal: 1.46s\tremaining: 3.74s\n",
            "14:\tlearn: 0.0856977\ttotal: 1.57s\tremaining: 3.67s\n",
            "15:\tlearn: 0.0838641\ttotal: 1.67s\tremaining: 3.54s\n",
            "16:\tlearn: 0.0816273\ttotal: 1.76s\tremaining: 3.41s\n",
            "17:\tlearn: 0.0803401\ttotal: 1.87s\tremaining: 3.32s\n",
            "18:\tlearn: 0.0789132\ttotal: 1.96s\tremaining: 3.2s\n",
            "19:\tlearn: 0.0761568\ttotal: 2.06s\tremaining: 3.08s\n",
            "20:\tlearn: 0.0744211\ttotal: 2.16s\tremaining: 2.98s\n",
            "21:\tlearn: 0.0730067\ttotal: 2.25s\tremaining: 2.87s\n",
            "22:\tlearn: 0.0710293\ttotal: 2.34s\tremaining: 2.75s\n",
            "23:\tlearn: 0.0688329\ttotal: 2.45s\tremaining: 2.65s\n",
            "24:\tlearn: 0.0670002\ttotal: 2.56s\tremaining: 2.56s\n",
            "25:\tlearn: 0.0658841\ttotal: 2.65s\tremaining: 2.45s\n",
            "26:\tlearn: 0.0648227\ttotal: 2.75s\tremaining: 2.34s\n",
            "27:\tlearn: 0.0638348\ttotal: 2.85s\tremaining: 2.24s\n",
            "28:\tlearn: 0.0624141\ttotal: 2.95s\tremaining: 2.14s\n",
            "29:\tlearn: 0.0612694\ttotal: 3.05s\tremaining: 2.03s\n",
            "30:\tlearn: 0.0602518\ttotal: 3.14s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0598807\ttotal: 3.25s\tremaining: 1.82s\n",
            "32:\tlearn: 0.0583353\ttotal: 3.34s\tremaining: 1.72s\n",
            "33:\tlearn: 0.0566441\ttotal: 3.43s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0556478\ttotal: 3.55s\tremaining: 1.52s\n",
            "35:\tlearn: 0.0548842\ttotal: 3.65s\tremaining: 1.42s\n",
            "36:\tlearn: 0.0542154\ttotal: 3.74s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0534327\ttotal: 3.84s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0521349\ttotal: 3.94s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0510057\ttotal: 4.03s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0500655\ttotal: 4.13s\tremaining: 908ms\n",
            "41:\tlearn: 0.0493170\ttotal: 4.23s\tremaining: 806ms\n",
            "42:\tlearn: 0.0482608\ttotal: 4.33s\tremaining: 704ms\n",
            "43:\tlearn: 0.0467119\ttotal: 4.43s\tremaining: 604ms\n",
            "44:\tlearn: 0.0460294\ttotal: 4.53s\tremaining: 503ms\n",
            "45:\tlearn: 0.0453697\ttotal: 4.63s\tremaining: 403ms\n",
            "46:\tlearn: 0.0445718\ttotal: 4.74s\tremaining: 303ms\n",
            "47:\tlearn: 0.0439405\ttotal: 4.83s\tremaining: 201ms\n",
            "48:\tlearn: 0.0431437\ttotal: 4.94s\tremaining: 101ms\n",
            "49:\tlearn: 0.0420848\ttotal: 5.04s\tremaining: 0us\n",
            "0:\tlearn: 0.0334341\ttotal: 149ms\tremaining: 7.28s\n",
            "1:\tlearn: 0.0297619\ttotal: 244ms\tremaining: 5.87s\n",
            "2:\tlearn: 0.0272440\ttotal: 339ms\tremaining: 5.31s\n",
            "3:\tlearn: 0.0246432\ttotal: 443ms\tremaining: 5.1s\n",
            "4:\tlearn: 0.0228851\ttotal: 548ms\tremaining: 4.93s\n",
            "5:\tlearn: 0.0209236\ttotal: 645ms\tremaining: 4.73s\n",
            "6:\tlearn: 0.0194799\ttotal: 755ms\tremaining: 4.63s\n",
            "7:\tlearn: 0.0182356\ttotal: 848ms\tremaining: 4.45s\n",
            "8:\tlearn: 0.0172655\ttotal: 950ms\tremaining: 4.33s\n",
            "9:\tlearn: 0.0168307\ttotal: 1.04s\tremaining: 4.17s\n",
            "10:\tlearn: 0.0161880\ttotal: 1.14s\tremaining: 4.04s\n",
            "11:\tlearn: 0.0154666\ttotal: 1.24s\tremaining: 3.94s\n",
            "12:\tlearn: 0.0147436\ttotal: 1.34s\tremaining: 3.81s\n",
            "13:\tlearn: 0.0140346\ttotal: 1.43s\tremaining: 3.68s\n",
            "14:\tlearn: 0.0133181\ttotal: 1.55s\tremaining: 3.61s\n",
            "15:\tlearn: 0.0126865\ttotal: 1.65s\tremaining: 3.5s\n",
            "16:\tlearn: 0.0120994\ttotal: 1.75s\tremaining: 3.39s\n",
            "17:\tlearn: 0.0117164\ttotal: 1.86s\tremaining: 3.3s\n",
            "18:\tlearn: 0.0114106\ttotal: 1.95s\tremaining: 3.19s\n",
            "19:\tlearn: 0.0109501\ttotal: 2.05s\tremaining: 3.07s\n",
            "20:\tlearn: 0.0107843\ttotal: 2.15s\tremaining: 2.97s\n",
            "21:\tlearn: 0.0104047\ttotal: 2.24s\tremaining: 2.85s\n",
            "22:\tlearn: 0.0102402\ttotal: 2.34s\tremaining: 2.74s\n",
            "23:\tlearn: 0.0099580\ttotal: 2.44s\tremaining: 2.65s\n",
            "24:\tlearn: 0.0096850\ttotal: 2.55s\tremaining: 2.55s\n",
            "25:\tlearn: 0.0094843\ttotal: 2.66s\tremaining: 2.45s\n",
            "26:\tlearn: 0.0092196\ttotal: 2.75s\tremaining: 2.35s\n",
            "27:\tlearn: 0.0089258\ttotal: 2.84s\tremaining: 2.23s\n",
            "28:\tlearn: 0.0087526\ttotal: 2.95s\tremaining: 2.14s\n",
            "29:\tlearn: 0.0085157\ttotal: 3.04s\tremaining: 2.03s\n",
            "30:\tlearn: 0.0083580\ttotal: 3.14s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0082617\ttotal: 3.25s\tremaining: 1.83s\n",
            "32:\tlearn: 0.0081201\ttotal: 3.34s\tremaining: 1.72s\n",
            "33:\tlearn: 0.0079042\ttotal: 3.44s\tremaining: 1.62s\n",
            "34:\tlearn: 0.0077374\ttotal: 3.55s\tremaining: 1.52s\n",
            "35:\tlearn: 0.0075115\ttotal: 3.65s\tremaining: 1.42s\n",
            "36:\tlearn: 0.0073514\ttotal: 3.75s\tremaining: 1.32s\n",
            "37:\tlearn: 0.0072786\ttotal: 3.85s\tremaining: 1.22s\n",
            "38:\tlearn: 0.0070887\ttotal: 3.94s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0069484\ttotal: 4.04s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0068146\ttotal: 4.14s\tremaining: 909ms\n",
            "41:\tlearn: 0.0067381\ttotal: 4.24s\tremaining: 807ms\n",
            "42:\tlearn: 0.0066259\ttotal: 4.33s\tremaining: 705ms\n",
            "43:\tlearn: 0.0064675\ttotal: 4.43s\tremaining: 605ms\n",
            "44:\tlearn: 0.0063649\ttotal: 4.53s\tremaining: 503ms\n",
            "45:\tlearn: 0.0063210\ttotal: 4.63s\tremaining: 403ms\n",
            "46:\tlearn: 0.0062362\ttotal: 4.74s\tremaining: 303ms\n",
            "47:\tlearn: 0.0061423\ttotal: 4.84s\tremaining: 202ms\n",
            "48:\tlearn: 0.0060696\ttotal: 4.93s\tremaining: 101ms\n",
            "49:\tlearn: 0.0059882\ttotal: 5.03s\tremaining: 0us\n",
            "0:\tlearn: 0.0170060\ttotal: 138ms\tremaining: 6.76s\n",
            "1:\tlearn: 0.0163031\ttotal: 241ms\tremaining: 5.78s\n",
            "2:\tlearn: 0.0153463\ttotal: 340ms\tremaining: 5.33s\n",
            "3:\tlearn: 0.0147282\ttotal: 433ms\tremaining: 4.98s\n",
            "4:\tlearn: 0.0140940\ttotal: 546ms\tremaining: 4.91s\n",
            "5:\tlearn: 0.0128087\ttotal: 642ms\tremaining: 4.71s\n",
            "6:\tlearn: 0.0124742\ttotal: 738ms\tremaining: 4.53s\n",
            "7:\tlearn: 0.0117309\ttotal: 842ms\tremaining: 4.42s\n",
            "8:\tlearn: 0.0111665\ttotal: 937ms\tremaining: 4.27s\n",
            "9:\tlearn: 0.0110163\ttotal: 1.03s\tremaining: 4.12s\n",
            "10:\tlearn: 0.0108017\ttotal: 1.14s\tremaining: 4.02s\n",
            "11:\tlearn: 0.0104982\ttotal: 1.23s\tremaining: 3.89s\n",
            "12:\tlearn: 0.0099533\ttotal: 1.32s\tremaining: 3.75s\n",
            "13:\tlearn: 0.0098080\ttotal: 1.42s\tremaining: 3.65s\n",
            "14:\tlearn: 0.0096793\ttotal: 1.52s\tremaining: 3.55s\n",
            "15:\tlearn: 0.0095562\ttotal: 1.62s\tremaining: 3.44s\n",
            "16:\tlearn: 0.0093420\ttotal: 1.73s\tremaining: 3.35s\n",
            "17:\tlearn: 0.0087304\ttotal: 1.82s\tremaining: 3.23s\n",
            "18:\tlearn: 0.0083077\ttotal: 1.91s\tremaining: 3.12s\n",
            "19:\tlearn: 0.0082366\ttotal: 2.01s\tremaining: 3.01s\n",
            "20:\tlearn: 0.0077488\ttotal: 2.1s\tremaining: 2.9s\n",
            "21:\tlearn: 0.0077011\ttotal: 2.2s\tremaining: 2.8s\n",
            "22:\tlearn: 0.0073650\ttotal: 2.32s\tremaining: 2.72s\n",
            "23:\tlearn: 0.0072886\ttotal: 2.41s\tremaining: 2.61s\n",
            "24:\tlearn: 0.0072075\ttotal: 2.5s\tremaining: 2.5s\n",
            "25:\tlearn: 0.0070499\ttotal: 2.62s\tremaining: 2.42s\n",
            "26:\tlearn: 0.0069856\ttotal: 2.72s\tremaining: 2.31s\n",
            "27:\tlearn: 0.0067932\ttotal: 2.81s\tremaining: 2.21s\n",
            "28:\tlearn: 0.0066123\ttotal: 2.92s\tremaining: 2.11s\n",
            "29:\tlearn: 0.0065689\ttotal: 3.01s\tremaining: 2.01s\n",
            "30:\tlearn: 0.0064423\ttotal: 3.11s\tremaining: 1.91s\n",
            "31:\tlearn: 0.0063490\ttotal: 3.22s\tremaining: 1.81s\n",
            "32:\tlearn: 0.0062619\ttotal: 3.32s\tremaining: 1.71s\n",
            "33:\tlearn: 0.0062222\ttotal: 3.42s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0061327\ttotal: 3.53s\tremaining: 1.51s\n",
            "35:\tlearn: 0.0058344\ttotal: 3.63s\tremaining: 1.41s\n",
            "36:\tlearn: 0.0055325\ttotal: 3.74s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0054782\ttotal: 3.84s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0052762\ttotal: 3.94s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0050655\ttotal: 4.05s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0049443\ttotal: 4.14s\tremaining: 909ms\n",
            "41:\tlearn: 0.0048896\ttotal: 4.24s\tremaining: 808ms\n",
            "42:\tlearn: 0.0047949\ttotal: 4.35s\tremaining: 708ms\n",
            "43:\tlearn: 0.0047408\ttotal: 4.45s\tremaining: 607ms\n",
            "44:\tlearn: 0.0047217\ttotal: 4.55s\tremaining: 506ms\n",
            "45:\tlearn: 0.0046812\ttotal: 4.67s\tremaining: 407ms\n",
            "46:\tlearn: 0.0045720\ttotal: 4.78s\tremaining: 305ms\n",
            "47:\tlearn: 0.0043417\ttotal: 4.88s\tremaining: 203ms\n",
            "48:\tlearn: 0.0043201\ttotal: 4.97s\tremaining: 101ms\n",
            "49:\tlearn: 0.0042919\ttotal: 5.08s\tremaining: 0us\n",
            "0:\tlearn: 0.0403376\ttotal: 141ms\tremaining: 6.89s\n",
            "1:\tlearn: 0.0363458\ttotal: 245ms\tremaining: 5.87s\n",
            "2:\tlearn: 0.0328124\ttotal: 337ms\tremaining: 5.28s\n",
            "3:\tlearn: 0.0297877\ttotal: 433ms\tremaining: 4.97s\n",
            "4:\tlearn: 0.0268604\ttotal: 549ms\tremaining: 4.94s\n",
            "5:\tlearn: 0.0247257\ttotal: 643ms\tremaining: 4.72s\n",
            "6:\tlearn: 0.0229866\ttotal: 740ms\tremaining: 4.54s\n",
            "7:\tlearn: 0.0212857\ttotal: 842ms\tremaining: 4.42s\n",
            "8:\tlearn: 0.0197953\ttotal: 938ms\tremaining: 4.27s\n",
            "9:\tlearn: 0.0190224\ttotal: 1.03s\tremaining: 4.13s\n",
            "10:\tlearn: 0.0179391\ttotal: 1.14s\tremaining: 4.04s\n",
            "11:\tlearn: 0.0171822\ttotal: 1.24s\tremaining: 3.91s\n",
            "12:\tlearn: 0.0161552\ttotal: 1.33s\tremaining: 3.79s\n",
            "13:\tlearn: 0.0151951\ttotal: 1.43s\tremaining: 3.69s\n",
            "14:\tlearn: 0.0145757\ttotal: 1.54s\tremaining: 3.59s\n",
            "15:\tlearn: 0.0143397\ttotal: 1.63s\tremaining: 3.47s\n",
            "16:\tlearn: 0.0138286\ttotal: 1.75s\tremaining: 3.39s\n",
            "17:\tlearn: 0.0130425\ttotal: 1.84s\tremaining: 3.28s\n",
            "18:\tlearn: 0.0126645\ttotal: 1.94s\tremaining: 3.17s\n",
            "19:\tlearn: 0.0123192\ttotal: 2.04s\tremaining: 3.06s\n",
            "20:\tlearn: 0.0117109\ttotal: 2.15s\tremaining: 2.96s\n",
            "21:\tlearn: 0.0114297\ttotal: 2.24s\tremaining: 2.85s\n",
            "22:\tlearn: 0.0109475\ttotal: 2.33s\tremaining: 2.74s\n",
            "23:\tlearn: 0.0105266\ttotal: 2.44s\tremaining: 2.64s\n",
            "24:\tlearn: 0.0101752\ttotal: 2.54s\tremaining: 2.54s\n",
            "25:\tlearn: 0.0097896\ttotal: 2.64s\tremaining: 2.44s\n",
            "26:\tlearn: 0.0095309\ttotal: 2.75s\tremaining: 2.34s\n",
            "27:\tlearn: 0.0093425\ttotal: 2.84s\tremaining: 2.23s\n",
            "28:\tlearn: 0.0091633\ttotal: 2.94s\tremaining: 2.13s\n",
            "29:\tlearn: 0.0090139\ttotal: 3.04s\tremaining: 2.03s\n",
            "30:\tlearn: 0.0088024\ttotal: 3.14s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0086394\ttotal: 3.24s\tremaining: 1.82s\n",
            "32:\tlearn: 0.0082985\ttotal: 3.33s\tremaining: 1.72s\n",
            "33:\tlearn: 0.0081437\ttotal: 3.43s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0078832\ttotal: 3.53s\tremaining: 1.51s\n",
            "35:\tlearn: 0.0077157\ttotal: 3.64s\tremaining: 1.42s\n",
            "36:\tlearn: 0.0075784\ttotal: 3.75s\tremaining: 1.32s\n",
            "37:\tlearn: 0.0074204\ttotal: 3.85s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0072594\ttotal: 3.94s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0070011\ttotal: 4.04s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0069088\ttotal: 4.14s\tremaining: 909ms\n",
            "41:\tlearn: 0.0067270\ttotal: 4.23s\tremaining: 807ms\n",
            "42:\tlearn: 0.0066334\ttotal: 4.34s\tremaining: 706ms\n",
            "43:\tlearn: 0.0064941\ttotal: 4.44s\tremaining: 606ms\n",
            "44:\tlearn: 0.0063599\ttotal: 4.55s\tremaining: 505ms\n",
            "45:\tlearn: 0.0062965\ttotal: 4.66s\tremaining: 405ms\n",
            "46:\tlearn: 0.0061858\ttotal: 4.76s\tremaining: 304ms\n",
            "47:\tlearn: 0.0060943\ttotal: 4.86s\tremaining: 202ms\n",
            "48:\tlearn: 0.0059917\ttotal: 4.95s\tremaining: 101ms\n",
            "49:\tlearn: 0.0058195\ttotal: 5.06s\tremaining: 0us\n",
            "0:\tlearn: 0.2198249\ttotal: 164ms\tremaining: 8.06s\n",
            "1:\tlearn: 0.1991513\ttotal: 272ms\tremaining: 6.53s\n",
            "2:\tlearn: 0.1821136\ttotal: 382ms\tremaining: 5.99s\n",
            "3:\tlearn: 0.1662830\ttotal: 497ms\tremaining: 5.71s\n",
            "4:\tlearn: 0.1524523\ttotal: 611ms\tremaining: 5.5s\n",
            "5:\tlearn: 0.1395280\ttotal: 716ms\tremaining: 5.25s\n",
            "6:\tlearn: 0.1300425\ttotal: 834ms\tremaining: 5.12s\n",
            "7:\tlearn: 0.1236047\ttotal: 938ms\tremaining: 4.92s\n",
            "8:\tlearn: 0.1174254\ttotal: 1.05s\tremaining: 4.78s\n",
            "9:\tlearn: 0.1117305\ttotal: 1.16s\tremaining: 4.62s\n",
            "10:\tlearn: 0.1065318\ttotal: 1.26s\tremaining: 4.47s\n",
            "11:\tlearn: 0.1024149\ttotal: 1.37s\tremaining: 4.34s\n",
            "12:\tlearn: 0.0987248\ttotal: 1.49s\tremaining: 4.25s\n",
            "13:\tlearn: 0.0950393\ttotal: 1.6s\tremaining: 4.12s\n",
            "14:\tlearn: 0.0919369\ttotal: 1.71s\tremaining: 3.99s\n",
            "15:\tlearn: 0.0893659\ttotal: 1.82s\tremaining: 3.88s\n",
            "16:\tlearn: 0.0870347\ttotal: 1.94s\tremaining: 3.77s\n",
            "17:\tlearn: 0.0841439\ttotal: 2.05s\tremaining: 3.64s\n",
            "18:\tlearn: 0.0828353\ttotal: 2.16s\tremaining: 3.53s\n",
            "19:\tlearn: 0.0812893\ttotal: 2.27s\tremaining: 3.41s\n",
            "20:\tlearn: 0.0804082\ttotal: 2.39s\tremaining: 3.3s\n",
            "21:\tlearn: 0.0793281\ttotal: 2.5s\tremaining: 3.18s\n",
            "22:\tlearn: 0.0771896\ttotal: 2.62s\tremaining: 3.08s\n",
            "23:\tlearn: 0.0757850\ttotal: 2.73s\tremaining: 2.96s\n",
            "24:\tlearn: 0.0746112\ttotal: 2.84s\tremaining: 2.84s\n",
            "25:\tlearn: 0.0735444\ttotal: 2.95s\tremaining: 2.72s\n",
            "26:\tlearn: 0.0721869\ttotal: 3.06s\tremaining: 2.61s\n",
            "27:\tlearn: 0.0712391\ttotal: 3.17s\tremaining: 2.49s\n",
            "28:\tlearn: 0.0702902\ttotal: 3.28s\tremaining: 2.38s\n",
            "29:\tlearn: 0.0685168\ttotal: 3.38s\tremaining: 2.26s\n",
            "30:\tlearn: 0.0679038\ttotal: 3.49s\tremaining: 2.14s\n",
            "31:\tlearn: 0.0668827\ttotal: 3.61s\tremaining: 2.03s\n",
            "32:\tlearn: 0.0661776\ttotal: 3.73s\tremaining: 1.92s\n",
            "33:\tlearn: 0.0654911\ttotal: 3.84s\tremaining: 1.8s\n",
            "34:\tlearn: 0.0649580\ttotal: 3.95s\tremaining: 1.69s\n",
            "35:\tlearn: 0.0638184\ttotal: 4.06s\tremaining: 1.58s\n",
            "36:\tlearn: 0.0632616\ttotal: 4.17s\tremaining: 1.47s\n",
            "37:\tlearn: 0.0625406\ttotal: 4.28s\tremaining: 1.35s\n",
            "38:\tlearn: 0.0619336\ttotal: 4.39s\tremaining: 1.24s\n",
            "39:\tlearn: 0.0611206\ttotal: 4.49s\tremaining: 1.12s\n",
            "40:\tlearn: 0.0606784\ttotal: 4.62s\tremaining: 1.01s\n",
            "41:\tlearn: 0.0601843\ttotal: 4.72s\tremaining: 900ms\n",
            "42:\tlearn: 0.0593192\ttotal: 4.84s\tremaining: 787ms\n",
            "43:\tlearn: 0.0584957\ttotal: 4.94s\tremaining: 674ms\n",
            "44:\tlearn: 0.0580341\ttotal: 5.05s\tremaining: 562ms\n",
            "45:\tlearn: 0.0576597\ttotal: 5.16s\tremaining: 449ms\n",
            "46:\tlearn: 0.0572386\ttotal: 5.27s\tremaining: 336ms\n",
            "47:\tlearn: 0.0566757\ttotal: 5.38s\tremaining: 224ms\n",
            "48:\tlearn: 0.0557468\ttotal: 5.49s\tremaining: 112ms\n",
            "49:\tlearn: 0.0553434\ttotal: 5.61s\tremaining: 0us\n",
            "0:\tlearn: 0.0370216\ttotal: 172ms\tremaining: 8.41s\n",
            "1:\tlearn: 0.0336744\ttotal: 294ms\tremaining: 7.06s\n",
            "2:\tlearn: 0.0305983\ttotal: 410ms\tremaining: 6.43s\n",
            "3:\tlearn: 0.0280127\ttotal: 516ms\tremaining: 5.93s\n",
            "4:\tlearn: 0.0258144\ttotal: 628ms\tremaining: 5.65s\n",
            "5:\tlearn: 0.0237537\ttotal: 739ms\tremaining: 5.42s\n",
            "6:\tlearn: 0.0221279\ttotal: 854ms\tremaining: 5.25s\n",
            "7:\tlearn: 0.0204472\ttotal: 970ms\tremaining: 5.09s\n",
            "8:\tlearn: 0.0195410\ttotal: 1.09s\tremaining: 4.96s\n",
            "9:\tlearn: 0.0183384\ttotal: 1.19s\tremaining: 4.77s\n",
            "10:\tlearn: 0.0173300\ttotal: 1.31s\tremaining: 4.65s\n",
            "11:\tlearn: 0.0165723\ttotal: 1.43s\tremaining: 4.51s\n",
            "12:\tlearn: 0.0160449\ttotal: 1.54s\tremaining: 4.38s\n",
            "13:\tlearn: 0.0154166\ttotal: 1.64s\tremaining: 4.22s\n",
            "14:\tlearn: 0.0147849\ttotal: 1.76s\tremaining: 4.11s\n",
            "15:\tlearn: 0.0143220\ttotal: 1.87s\tremaining: 3.97s\n",
            "16:\tlearn: 0.0138018\ttotal: 1.99s\tremaining: 3.86s\n",
            "17:\tlearn: 0.0134431\ttotal: 2.1s\tremaining: 3.73s\n",
            "18:\tlearn: 0.0129941\ttotal: 2.21s\tremaining: 3.6s\n",
            "19:\tlearn: 0.0127226\ttotal: 2.31s\tremaining: 3.47s\n",
            "20:\tlearn: 0.0124250\ttotal: 2.43s\tremaining: 3.36s\n",
            "21:\tlearn: 0.0121290\ttotal: 2.53s\tremaining: 3.23s\n",
            "22:\tlearn: 0.0118658\ttotal: 2.65s\tremaining: 3.11s\n",
            "23:\tlearn: 0.0116518\ttotal: 2.77s\tremaining: 3s\n",
            "24:\tlearn: 0.0113805\ttotal: 2.88s\tremaining: 2.88s\n",
            "25:\tlearn: 0.0112020\ttotal: 2.99s\tremaining: 2.76s\n",
            "26:\tlearn: 0.0110176\ttotal: 3.11s\tremaining: 2.65s\n",
            "27:\tlearn: 0.0108492\ttotal: 3.21s\tremaining: 2.52s\n",
            "28:\tlearn: 0.0107003\ttotal: 3.33s\tremaining: 2.41s\n",
            "29:\tlearn: 0.0104524\ttotal: 3.44s\tremaining: 2.29s\n",
            "30:\tlearn: 0.0103404\ttotal: 3.55s\tremaining: 2.18s\n",
            "31:\tlearn: 0.0102519\ttotal: 3.66s\tremaining: 2.06s\n",
            "32:\tlearn: 0.0101129\ttotal: 3.78s\tremaining: 1.95s\n",
            "33:\tlearn: 0.0099200\ttotal: 3.89s\tremaining: 1.83s\n",
            "34:\tlearn: 0.0098087\ttotal: 4s\tremaining: 1.71s\n",
            "35:\tlearn: 0.0096074\ttotal: 4.12s\tremaining: 1.6s\n",
            "36:\tlearn: 0.0094351\ttotal: 4.23s\tremaining: 1.49s\n",
            "37:\tlearn: 0.0092970\ttotal: 4.34s\tremaining: 1.37s\n",
            "38:\tlearn: 0.0091656\ttotal: 4.46s\tremaining: 1.26s\n",
            "39:\tlearn: 0.0090892\ttotal: 4.56s\tremaining: 1.14s\n",
            "40:\tlearn: 0.0089851\ttotal: 4.68s\tremaining: 1.03s\n",
            "41:\tlearn: 0.0089033\ttotal: 4.79s\tremaining: 912ms\n",
            "42:\tlearn: 0.0088094\ttotal: 4.91s\tremaining: 799ms\n",
            "43:\tlearn: 0.0086982\ttotal: 5.01s\tremaining: 683ms\n",
            "44:\tlearn: 0.0086301\ttotal: 5.14s\tremaining: 571ms\n",
            "45:\tlearn: 0.0085666\ttotal: 5.25s\tremaining: 456ms\n",
            "46:\tlearn: 0.0084930\ttotal: 5.36s\tremaining: 342ms\n",
            "47:\tlearn: 0.0084555\ttotal: 5.46s\tremaining: 227ms\n",
            "48:\tlearn: 0.0083621\ttotal: 5.58s\tremaining: 114ms\n",
            "49:\tlearn: 0.0081895\ttotal: 5.69s\tremaining: 0us\n",
            "0:\tlearn: 0.0156864\ttotal: 163ms\tremaining: 8s\n",
            "1:\tlearn: 0.0148714\ttotal: 267ms\tremaining: 6.4s\n",
            "2:\tlearn: 0.0141445\ttotal: 395ms\tremaining: 6.19s\n",
            "3:\tlearn: 0.0133936\ttotal: 501ms\tremaining: 5.76s\n",
            "4:\tlearn: 0.0123513\ttotal: 611ms\tremaining: 5.5s\n",
            "5:\tlearn: 0.0116416\ttotal: 720ms\tremaining: 5.28s\n",
            "6:\tlearn: 0.0113163\ttotal: 832ms\tremaining: 5.11s\n",
            "7:\tlearn: 0.0106805\ttotal: 936ms\tremaining: 4.91s\n",
            "8:\tlearn: 0.0102368\ttotal: 1.05s\tremaining: 4.78s\n",
            "9:\tlearn: 0.0099877\ttotal: 1.18s\tremaining: 4.71s\n",
            "10:\tlearn: 0.0097096\ttotal: 1.29s\tremaining: 4.59s\n",
            "11:\tlearn: 0.0095433\ttotal: 1.41s\tremaining: 4.46s\n",
            "12:\tlearn: 0.0091709\ttotal: 1.52s\tremaining: 4.33s\n",
            "13:\tlearn: 0.0085327\ttotal: 1.63s\tremaining: 4.18s\n",
            "14:\tlearn: 0.0083699\ttotal: 1.74s\tremaining: 4.07s\n",
            "15:\tlearn: 0.0082436\ttotal: 1.85s\tremaining: 3.93s\n",
            "16:\tlearn: 0.0078043\ttotal: 1.96s\tremaining: 3.81s\n",
            "17:\tlearn: 0.0074662\ttotal: 2.07s\tremaining: 3.67s\n",
            "18:\tlearn: 0.0072786\ttotal: 2.19s\tremaining: 3.57s\n",
            "19:\tlearn: 0.0069809\ttotal: 2.29s\tremaining: 3.44s\n",
            "20:\tlearn: 0.0068601\ttotal: 2.42s\tremaining: 3.34s\n",
            "21:\tlearn: 0.0068228\ttotal: 2.53s\tremaining: 3.22s\n",
            "22:\tlearn: 0.0065864\ttotal: 2.65s\tremaining: 3.11s\n",
            "23:\tlearn: 0.0063859\ttotal: 2.75s\tremaining: 2.98s\n",
            "24:\tlearn: 0.0063080\ttotal: 2.86s\tremaining: 2.86s\n",
            "25:\tlearn: 0.0061990\ttotal: 2.97s\tremaining: 2.74s\n",
            "26:\tlearn: 0.0060932\ttotal: 3.09s\tremaining: 2.63s\n",
            "27:\tlearn: 0.0060069\ttotal: 3.19s\tremaining: 2.51s\n",
            "28:\tlearn: 0.0059673\ttotal: 3.31s\tremaining: 2.39s\n",
            "29:\tlearn: 0.0059359\ttotal: 3.43s\tremaining: 2.29s\n",
            "30:\tlearn: 0.0058893\ttotal: 3.55s\tremaining: 2.17s\n",
            "31:\tlearn: 0.0058320\ttotal: 3.66s\tremaining: 2.06s\n",
            "32:\tlearn: 0.0057825\ttotal: 3.77s\tremaining: 1.94s\n",
            "33:\tlearn: 0.0057315\ttotal: 3.88s\tremaining: 1.82s\n",
            "34:\tlearn: 0.0055440\ttotal: 4s\tremaining: 1.71s\n",
            "35:\tlearn: 0.0053768\ttotal: 4.1s\tremaining: 1.59s\n",
            "36:\tlearn: 0.0053593\ttotal: 4.21s\tremaining: 1.48s\n",
            "37:\tlearn: 0.0051787\ttotal: 4.32s\tremaining: 1.36s\n",
            "38:\tlearn: 0.0050047\ttotal: 4.43s\tremaining: 1.25s\n",
            "39:\tlearn: 0.0049668\ttotal: 4.55s\tremaining: 1.14s\n",
            "40:\tlearn: 0.0049174\ttotal: 4.67s\tremaining: 1.02s\n",
            "41:\tlearn: 0.0048109\ttotal: 4.77s\tremaining: 909ms\n",
            "42:\tlearn: 0.0047761\ttotal: 4.88s\tremaining: 795ms\n",
            "43:\tlearn: 0.0047356\ttotal: 4.99s\tremaining: 681ms\n",
            "44:\tlearn: 0.0046507\ttotal: 5.1s\tremaining: 567ms\n",
            "45:\tlearn: 0.0045911\ttotal: 5.21s\tremaining: 453ms\n",
            "46:\tlearn: 0.0045252\ttotal: 5.32s\tremaining: 340ms\n",
            "47:\tlearn: 0.0044486\ttotal: 5.43s\tremaining: 226ms\n",
            "48:\tlearn: 0.0042904\ttotal: 5.55s\tremaining: 113ms\n",
            "49:\tlearn: 0.0042298\ttotal: 5.66s\tremaining: 0us\n",
            "0:\tlearn: 0.0387750\ttotal: 166ms\tremaining: 8.11s\n",
            "1:\tlearn: 0.0343902\ttotal: 274ms\tremaining: 6.58s\n",
            "2:\tlearn: 0.0309531\ttotal: 387ms\tremaining: 6.06s\n",
            "3:\tlearn: 0.0278525\ttotal: 496ms\tremaining: 5.71s\n",
            "4:\tlearn: 0.0251646\ttotal: 617ms\tremaining: 5.55s\n",
            "5:\tlearn: 0.0231057\ttotal: 724ms\tremaining: 5.31s\n",
            "6:\tlearn: 0.0211806\ttotal: 847ms\tremaining: 5.2s\n",
            "7:\tlearn: 0.0195614\ttotal: 952ms\tremaining: 5s\n",
            "8:\tlearn: 0.0182344\ttotal: 1.06s\tremaining: 4.83s\n",
            "9:\tlearn: 0.0174003\ttotal: 1.16s\tremaining: 4.65s\n",
            "10:\tlearn: 0.0166006\ttotal: 1.28s\tremaining: 4.54s\n",
            "11:\tlearn: 0.0158722\ttotal: 1.39s\tremaining: 4.39s\n",
            "12:\tlearn: 0.0151570\ttotal: 1.5s\tremaining: 4.26s\n",
            "13:\tlearn: 0.0144255\ttotal: 1.6s\tremaining: 4.13s\n",
            "14:\tlearn: 0.0138853\ttotal: 1.72s\tremaining: 4.01s\n",
            "15:\tlearn: 0.0135550\ttotal: 1.83s\tremaining: 3.9s\n",
            "16:\tlearn: 0.0130026\ttotal: 1.95s\tremaining: 3.79s\n",
            "17:\tlearn: 0.0126664\ttotal: 2.06s\tremaining: 3.66s\n",
            "18:\tlearn: 0.0123687\ttotal: 2.17s\tremaining: 3.54s\n",
            "19:\tlearn: 0.0119835\ttotal: 2.28s\tremaining: 3.42s\n",
            "20:\tlearn: 0.0117332\ttotal: 2.4s\tremaining: 3.32s\n",
            "21:\tlearn: 0.0114824\ttotal: 2.5s\tremaining: 3.19s\n",
            "22:\tlearn: 0.0112490\ttotal: 2.62s\tremaining: 3.08s\n",
            "23:\tlearn: 0.0109826\ttotal: 2.72s\tremaining: 2.95s\n",
            "24:\tlearn: 0.0106736\ttotal: 2.85s\tremaining: 2.85s\n",
            "25:\tlearn: 0.0104996\ttotal: 2.95s\tremaining: 2.73s\n",
            "26:\tlearn: 0.0104186\ttotal: 3.06s\tremaining: 2.61s\n",
            "27:\tlearn: 0.0103159\ttotal: 3.17s\tremaining: 2.49s\n",
            "28:\tlearn: 0.0100646\ttotal: 3.28s\tremaining: 2.37s\n",
            "29:\tlearn: 0.0099258\ttotal: 3.38s\tremaining: 2.25s\n",
            "30:\tlearn: 0.0097647\ttotal: 3.5s\tremaining: 2.14s\n",
            "31:\tlearn: 0.0095354\ttotal: 3.6s\tremaining: 2.03s\n",
            "32:\tlearn: 0.0093506\ttotal: 3.72s\tremaining: 1.92s\n",
            "33:\tlearn: 0.0092510\ttotal: 3.83s\tremaining: 1.8s\n",
            "34:\tlearn: 0.0090807\ttotal: 3.95s\tremaining: 1.69s\n",
            "35:\tlearn: 0.0089068\ttotal: 4.05s\tremaining: 1.57s\n",
            "36:\tlearn: 0.0088264\ttotal: 4.17s\tremaining: 1.46s\n",
            "37:\tlearn: 0.0086346\ttotal: 4.27s\tremaining: 1.35s\n",
            "38:\tlearn: 0.0085322\ttotal: 4.38s\tremaining: 1.24s\n",
            "39:\tlearn: 0.0084007\ttotal: 4.49s\tremaining: 1.12s\n",
            "40:\tlearn: 0.0083219\ttotal: 4.6s\tremaining: 1.01s\n",
            "41:\tlearn: 0.0082448\ttotal: 4.71s\tremaining: 897ms\n",
            "42:\tlearn: 0.0081664\ttotal: 4.83s\tremaining: 786ms\n",
            "43:\tlearn: 0.0080748\ttotal: 4.94s\tremaining: 674ms\n",
            "44:\tlearn: 0.0079750\ttotal: 5.06s\tremaining: 562ms\n",
            "45:\tlearn: 0.0078555\ttotal: 5.17s\tremaining: 449ms\n",
            "46:\tlearn: 0.0077763\ttotal: 5.28s\tremaining: 337ms\n",
            "47:\tlearn: 0.0076860\ttotal: 5.38s\tremaining: 224ms\n",
            "48:\tlearn: 0.0076272\ttotal: 5.5s\tremaining: 112ms\n",
            "49:\tlearn: 0.0075810\ttotal: 5.6s\tremaining: 0us\n",
            "0:\tlearn: 0.2326062\ttotal: 176ms\tremaining: 8.62s\n",
            "1:\tlearn: 0.2096718\ttotal: 294ms\tremaining: 7.05s\n",
            "2:\tlearn: 0.1924312\ttotal: 412ms\tremaining: 6.46s\n",
            "3:\tlearn: 0.1751025\ttotal: 521ms\tremaining: 5.99s\n",
            "4:\tlearn: 0.1627630\ttotal: 639ms\tremaining: 5.75s\n",
            "5:\tlearn: 0.1514584\ttotal: 750ms\tremaining: 5.5s\n",
            "6:\tlearn: 0.1414049\ttotal: 868ms\tremaining: 5.33s\n",
            "7:\tlearn: 0.1320007\ttotal: 979ms\tremaining: 5.14s\n",
            "8:\tlearn: 0.1248849\ttotal: 1.1s\tremaining: 5.01s\n",
            "9:\tlearn: 0.1209978\ttotal: 1.22s\tremaining: 4.86s\n",
            "10:\tlearn: 0.1160021\ttotal: 1.35s\tremaining: 4.79s\n",
            "11:\tlearn: 0.1110371\ttotal: 1.46s\tremaining: 4.63s\n",
            "12:\tlearn: 0.1063894\ttotal: 1.58s\tremaining: 4.5s\n",
            "13:\tlearn: 0.1020586\ttotal: 1.69s\tremaining: 4.34s\n",
            "14:\tlearn: 0.0987656\ttotal: 1.81s\tremaining: 4.22s\n",
            "15:\tlearn: 0.0959316\ttotal: 1.92s\tremaining: 4.08s\n",
            "16:\tlearn: 0.0934096\ttotal: 2.04s\tremaining: 3.96s\n",
            "17:\tlearn: 0.0918258\ttotal: 2.15s\tremaining: 3.82s\n",
            "18:\tlearn: 0.0901662\ttotal: 2.27s\tremaining: 3.71s\n",
            "19:\tlearn: 0.0889215\ttotal: 2.38s\tremaining: 3.58s\n",
            "20:\tlearn: 0.0878582\ttotal: 2.51s\tremaining: 3.47s\n",
            "21:\tlearn: 0.0868677\ttotal: 2.63s\tremaining: 3.35s\n",
            "22:\tlearn: 0.0846705\ttotal: 2.75s\tremaining: 3.23s\n",
            "23:\tlearn: 0.0832714\ttotal: 2.86s\tremaining: 3.1s\n",
            "24:\tlearn: 0.0822700\ttotal: 2.98s\tremaining: 2.98s\n",
            "25:\tlearn: 0.0813405\ttotal: 3.1s\tremaining: 2.86s\n",
            "26:\tlearn: 0.0798080\ttotal: 3.22s\tremaining: 2.75s\n",
            "27:\tlearn: 0.0791170\ttotal: 3.34s\tremaining: 2.63s\n",
            "28:\tlearn: 0.0775522\ttotal: 3.47s\tremaining: 2.51s\n",
            "29:\tlearn: 0.0765032\ttotal: 3.58s\tremaining: 2.38s\n",
            "30:\tlearn: 0.0757586\ttotal: 3.7s\tremaining: 2.27s\n",
            "31:\tlearn: 0.0749469\ttotal: 3.81s\tremaining: 2.14s\n",
            "32:\tlearn: 0.0739028\ttotal: 3.93s\tremaining: 2.02s\n",
            "33:\tlearn: 0.0729333\ttotal: 4.04s\tremaining: 1.9s\n",
            "34:\tlearn: 0.0715681\ttotal: 4.16s\tremaining: 1.78s\n",
            "35:\tlearn: 0.0708155\ttotal: 4.28s\tremaining: 1.66s\n",
            "36:\tlearn: 0.0702732\ttotal: 4.41s\tremaining: 1.55s\n",
            "37:\tlearn: 0.0698030\ttotal: 4.52s\tremaining: 1.43s\n",
            "38:\tlearn: 0.0684950\ttotal: 4.63s\tremaining: 1.31s\n",
            "39:\tlearn: 0.0675355\ttotal: 4.75s\tremaining: 1.19s\n",
            "40:\tlearn: 0.0668977\ttotal: 4.88s\tremaining: 1.07s\n",
            "41:\tlearn: 0.0664370\ttotal: 4.99s\tremaining: 950ms\n",
            "42:\tlearn: 0.0656751\ttotal: 5.11s\tremaining: 832ms\n",
            "43:\tlearn: 0.0648860\ttotal: 5.22s\tremaining: 712ms\n",
            "44:\tlearn: 0.0641178\ttotal: 5.34s\tremaining: 594ms\n",
            "45:\tlearn: 0.0634511\ttotal: 5.46s\tremaining: 474ms\n",
            "46:\tlearn: 0.0627550\ttotal: 5.57s\tremaining: 356ms\n",
            "47:\tlearn: 0.0622375\ttotal: 5.68s\tremaining: 237ms\n",
            "48:\tlearn: 0.0616385\ttotal: 5.8s\tremaining: 118ms\n",
            "49:\tlearn: 0.0609515\ttotal: 5.91s\tremaining: 0us\n",
            "0:\tlearn: 0.0360831\ttotal: 169ms\tremaining: 8.3s\n",
            "1:\tlearn: 0.0321627\ttotal: 276ms\tremaining: 6.63s\n",
            "2:\tlearn: 0.0291712\ttotal: 402ms\tremaining: 6.29s\n",
            "3:\tlearn: 0.0266063\ttotal: 509ms\tremaining: 5.86s\n",
            "4:\tlearn: 0.0243921\ttotal: 626ms\tremaining: 5.63s\n",
            "5:\tlearn: 0.0223344\ttotal: 742ms\tremaining: 5.44s\n",
            "6:\tlearn: 0.0208307\ttotal: 859ms\tremaining: 5.27s\n",
            "7:\tlearn: 0.0193070\ttotal: 971ms\tremaining: 5.09s\n",
            "8:\tlearn: 0.0183469\ttotal: 1.09s\tremaining: 4.96s\n",
            "9:\tlearn: 0.0175832\ttotal: 1.2s\tremaining: 4.79s\n",
            "10:\tlearn: 0.0168120\ttotal: 1.31s\tremaining: 4.66s\n",
            "11:\tlearn: 0.0159935\ttotal: 1.43s\tremaining: 4.54s\n",
            "12:\tlearn: 0.0152139\ttotal: 1.55s\tremaining: 4.42s\n",
            "13:\tlearn: 0.0145623\ttotal: 1.66s\tremaining: 4.28s\n",
            "14:\tlearn: 0.0140762\ttotal: 1.78s\tremaining: 4.16s\n",
            "15:\tlearn: 0.0135273\ttotal: 1.89s\tremaining: 4.02s\n",
            "16:\tlearn: 0.0130434\ttotal: 2.01s\tremaining: 3.91s\n",
            "17:\tlearn: 0.0126363\ttotal: 2.12s\tremaining: 3.77s\n",
            "18:\tlearn: 0.0122650\ttotal: 2.24s\tremaining: 3.65s\n",
            "19:\tlearn: 0.0120531\ttotal: 2.35s\tremaining: 3.52s\n",
            "20:\tlearn: 0.0117891\ttotal: 2.48s\tremaining: 3.42s\n",
            "21:\tlearn: 0.0115315\ttotal: 2.59s\tremaining: 3.29s\n",
            "22:\tlearn: 0.0112390\ttotal: 2.7s\tremaining: 3.17s\n",
            "23:\tlearn: 0.0110351\ttotal: 2.81s\tremaining: 3.05s\n",
            "24:\tlearn: 0.0109152\ttotal: 2.93s\tremaining: 2.93s\n",
            "25:\tlearn: 0.0107268\ttotal: 3.04s\tremaining: 2.8s\n",
            "26:\tlearn: 0.0106483\ttotal: 3.15s\tremaining: 2.69s\n",
            "27:\tlearn: 0.0105363\ttotal: 3.26s\tremaining: 2.56s\n",
            "28:\tlearn: 0.0103474\ttotal: 3.37s\tremaining: 2.44s\n",
            "29:\tlearn: 0.0102417\ttotal: 3.51s\tremaining: 2.34s\n",
            "30:\tlearn: 0.0100815\ttotal: 3.63s\tremaining: 2.23s\n",
            "31:\tlearn: 0.0099813\ttotal: 3.75s\tremaining: 2.11s\n",
            "32:\tlearn: 0.0098650\ttotal: 3.86s\tremaining: 1.99s\n",
            "33:\tlearn: 0.0096379\ttotal: 3.97s\tremaining: 1.87s\n",
            "34:\tlearn: 0.0095340\ttotal: 4.1s\tremaining: 1.76s\n",
            "35:\tlearn: 0.0094311\ttotal: 4.22s\tremaining: 1.64s\n",
            "36:\tlearn: 0.0093536\ttotal: 4.34s\tremaining: 1.52s\n",
            "37:\tlearn: 0.0092520\ttotal: 4.44s\tremaining: 1.4s\n",
            "38:\tlearn: 0.0090677\ttotal: 4.59s\tremaining: 1.29s\n",
            "39:\tlearn: 0.0089578\ttotal: 4.7s\tremaining: 1.17s\n",
            "40:\tlearn: 0.0088641\ttotal: 4.81s\tremaining: 1.06s\n",
            "41:\tlearn: 0.0087605\ttotal: 4.93s\tremaining: 940ms\n",
            "42:\tlearn: 0.0086920\ttotal: 5.06s\tremaining: 823ms\n",
            "43:\tlearn: 0.0085836\ttotal: 5.17s\tremaining: 705ms\n",
            "44:\tlearn: 0.0084887\ttotal: 5.29s\tremaining: 588ms\n",
            "45:\tlearn: 0.0084189\ttotal: 5.4s\tremaining: 470ms\n",
            "46:\tlearn: 0.0083497\ttotal: 5.52s\tremaining: 353ms\n",
            "47:\tlearn: 0.0083059\ttotal: 5.65s\tremaining: 235ms\n",
            "48:\tlearn: 0.0082165\ttotal: 5.79s\tremaining: 118ms\n",
            "49:\tlearn: 0.0081024\ttotal: 5.9s\tremaining: 0us\n",
            "0:\tlearn: 0.0166232\ttotal: 212ms\tremaining: 10.4s\n",
            "1:\tlearn: 0.0157458\ttotal: 322ms\tremaining: 7.72s\n",
            "2:\tlearn: 0.0148231\ttotal: 441ms\tremaining: 6.91s\n",
            "3:\tlearn: 0.0139092\ttotal: 549ms\tremaining: 6.32s\n",
            "4:\tlearn: 0.0128332\ttotal: 682ms\tremaining: 6.14s\n",
            "5:\tlearn: 0.0122405\ttotal: 791ms\tremaining: 5.8s\n",
            "6:\tlearn: 0.0114836\ttotal: 909ms\tremaining: 5.58s\n",
            "7:\tlearn: 0.0105841\ttotal: 1.02s\tremaining: 5.35s\n",
            "8:\tlearn: 0.0103512\ttotal: 1.14s\tremaining: 5.21s\n",
            "9:\tlearn: 0.0102025\ttotal: 1.25s\tremaining: 5s\n",
            "10:\tlearn: 0.0098166\ttotal: 1.37s\tremaining: 4.85s\n",
            "11:\tlearn: 0.0095203\ttotal: 1.48s\tremaining: 4.67s\n",
            "12:\tlearn: 0.0092305\ttotal: 1.59s\tremaining: 4.54s\n",
            "13:\tlearn: 0.0088317\ttotal: 1.72s\tremaining: 4.42s\n",
            "14:\tlearn: 0.0085204\ttotal: 1.84s\tremaining: 4.3s\n",
            "15:\tlearn: 0.0084091\ttotal: 1.95s\tremaining: 4.15s\n",
            "16:\tlearn: 0.0082043\ttotal: 2.07s\tremaining: 4.02s\n",
            "17:\tlearn: 0.0080049\ttotal: 2.18s\tremaining: 3.88s\n",
            "18:\tlearn: 0.0078782\ttotal: 2.3s\tremaining: 3.76s\n",
            "19:\tlearn: 0.0075791\ttotal: 2.41s\tremaining: 3.62s\n",
            "20:\tlearn: 0.0075354\ttotal: 2.53s\tremaining: 3.5s\n",
            "21:\tlearn: 0.0074986\ttotal: 2.64s\tremaining: 3.36s\n",
            "22:\tlearn: 0.0072092\ttotal: 2.77s\tremaining: 3.25s\n",
            "23:\tlearn: 0.0070486\ttotal: 2.88s\tremaining: 3.12s\n",
            "24:\tlearn: 0.0069904\ttotal: 3.01s\tremaining: 3.01s\n",
            "25:\tlearn: 0.0068848\ttotal: 3.11s\tremaining: 2.87s\n",
            "26:\tlearn: 0.0067662\ttotal: 3.23s\tremaining: 2.76s\n",
            "27:\tlearn: 0.0066123\ttotal: 3.35s\tremaining: 2.63s\n",
            "28:\tlearn: 0.0064947\ttotal: 3.46s\tremaining: 2.51s\n",
            "29:\tlearn: 0.0063338\ttotal: 3.57s\tremaining: 2.38s\n",
            "30:\tlearn: 0.0062863\ttotal: 3.7s\tremaining: 2.27s\n",
            "31:\tlearn: 0.0062151\ttotal: 3.81s\tremaining: 2.15s\n",
            "32:\tlearn: 0.0060988\ttotal: 3.93s\tremaining: 2.02s\n",
            "33:\tlearn: 0.0059338\ttotal: 4.04s\tremaining: 1.9s\n",
            "34:\tlearn: 0.0057925\ttotal: 4.18s\tremaining: 1.79s\n",
            "35:\tlearn: 0.0057738\ttotal: 4.3s\tremaining: 1.67s\n",
            "36:\tlearn: 0.0056067\ttotal: 4.42s\tremaining: 1.55s\n",
            "37:\tlearn: 0.0055683\ttotal: 4.54s\tremaining: 1.43s\n",
            "38:\tlearn: 0.0054344\ttotal: 4.66s\tremaining: 1.31s\n",
            "39:\tlearn: 0.0053749\ttotal: 4.78s\tremaining: 1.19s\n",
            "40:\tlearn: 0.0053466\ttotal: 4.89s\tremaining: 1.07s\n",
            "41:\tlearn: 0.0052153\ttotal: 5s\tremaining: 953ms\n",
            "42:\tlearn: 0.0051228\ttotal: 5.12s\tremaining: 834ms\n",
            "43:\tlearn: 0.0051118\ttotal: 5.23s\tremaining: 713ms\n",
            "44:\tlearn: 0.0049638\ttotal: 5.35s\tremaining: 594ms\n",
            "45:\tlearn: 0.0049224\ttotal: 5.46s\tremaining: 475ms\n",
            "46:\tlearn: 0.0048692\ttotal: 5.58s\tremaining: 356ms\n",
            "47:\tlearn: 0.0047966\ttotal: 5.69s\tremaining: 237ms\n",
            "48:\tlearn: 0.0047400\ttotal: 5.82s\tremaining: 119ms\n",
            "49:\tlearn: 0.0046978\ttotal: 5.93s\tremaining: 0us\n",
            "0:\tlearn: 0.0378612\ttotal: 171ms\tremaining: 8.36s\n",
            "1:\tlearn: 0.0341113\ttotal: 285ms\tremaining: 6.85s\n",
            "2:\tlearn: 0.0305583\ttotal: 407ms\tremaining: 6.37s\n",
            "3:\tlearn: 0.0278239\ttotal: 519ms\tremaining: 5.96s\n",
            "4:\tlearn: 0.0252206\ttotal: 638ms\tremaining: 5.74s\n",
            "5:\tlearn: 0.0231057\ttotal: 755ms\tremaining: 5.54s\n",
            "6:\tlearn: 0.0213842\ttotal: 875ms\tremaining: 5.38s\n",
            "7:\tlearn: 0.0199312\ttotal: 983ms\tremaining: 5.16s\n",
            "8:\tlearn: 0.0187387\ttotal: 1.1s\tremaining: 5.03s\n",
            "9:\tlearn: 0.0180050\ttotal: 1.21s\tremaining: 4.85s\n",
            "10:\tlearn: 0.0171491\ttotal: 1.33s\tremaining: 4.71s\n",
            "11:\tlearn: 0.0164907\ttotal: 1.44s\tremaining: 4.55s\n",
            "12:\tlearn: 0.0158911\ttotal: 1.56s\tremaining: 4.43s\n",
            "13:\tlearn: 0.0152562\ttotal: 1.66s\tremaining: 4.28s\n",
            "14:\tlearn: 0.0147926\ttotal: 1.79s\tremaining: 4.18s\n",
            "15:\tlearn: 0.0145158\ttotal: 1.91s\tremaining: 4.07s\n",
            "16:\tlearn: 0.0142177\ttotal: 2.03s\tremaining: 3.95s\n",
            "17:\tlearn: 0.0138680\ttotal: 2.14s\tremaining: 3.81s\n",
            "18:\tlearn: 0.0136177\ttotal: 2.26s\tremaining: 3.69s\n",
            "19:\tlearn: 0.0132987\ttotal: 2.37s\tremaining: 3.56s\n",
            "20:\tlearn: 0.0130191\ttotal: 2.49s\tremaining: 3.44s\n",
            "21:\tlearn: 0.0127252\ttotal: 2.6s\tremaining: 3.31s\n",
            "22:\tlearn: 0.0125354\ttotal: 2.72s\tremaining: 3.2s\n",
            "23:\tlearn: 0.0124335\ttotal: 2.85s\tremaining: 3.08s\n",
            "24:\tlearn: 0.0121796\ttotal: 2.97s\tremaining: 2.97s\n",
            "25:\tlearn: 0.0120556\ttotal: 3.08s\tremaining: 2.84s\n",
            "26:\tlearn: 0.0119720\ttotal: 3.2s\tremaining: 2.72s\n",
            "27:\tlearn: 0.0117891\ttotal: 3.31s\tremaining: 2.6s\n",
            "28:\tlearn: 0.0116868\ttotal: 3.43s\tremaining: 2.48s\n",
            "29:\tlearn: 0.0115325\ttotal: 3.54s\tremaining: 2.36s\n",
            "30:\tlearn: 0.0112695\ttotal: 3.67s\tremaining: 2.25s\n",
            "31:\tlearn: 0.0110896\ttotal: 3.78s\tremaining: 2.13s\n",
            "32:\tlearn: 0.0108510\ttotal: 3.92s\tremaining: 2.02s\n",
            "33:\tlearn: 0.0107299\ttotal: 4.03s\tremaining: 1.9s\n",
            "34:\tlearn: 0.0106216\ttotal: 4.16s\tremaining: 1.78s\n",
            "35:\tlearn: 0.0104377\ttotal: 4.27s\tremaining: 1.66s\n",
            "36:\tlearn: 0.0103358\ttotal: 4.39s\tremaining: 1.54s\n",
            "37:\tlearn: 0.0101790\ttotal: 4.51s\tremaining: 1.43s\n",
            "38:\tlearn: 0.0100499\ttotal: 4.64s\tremaining: 1.31s\n",
            "39:\tlearn: 0.0099787\ttotal: 4.76s\tremaining: 1.19s\n",
            "40:\tlearn: 0.0098636\ttotal: 4.89s\tremaining: 1.07s\n",
            "41:\tlearn: 0.0097565\ttotal: 5s\tremaining: 952ms\n",
            "42:\tlearn: 0.0096080\ttotal: 5.13s\tremaining: 835ms\n",
            "43:\tlearn: 0.0095240\ttotal: 5.24s\tremaining: 714ms\n",
            "44:\tlearn: 0.0094156\ttotal: 5.36s\tremaining: 596ms\n",
            "45:\tlearn: 0.0093388\ttotal: 5.48s\tremaining: 476ms\n",
            "46:\tlearn: 0.0092182\ttotal: 5.6s\tremaining: 357ms\n",
            "47:\tlearn: 0.0091190\ttotal: 5.71s\tremaining: 238ms\n",
            "48:\tlearn: 0.0090254\ttotal: 5.83s\tremaining: 119ms\n",
            "49:\tlearn: 0.0089047\ttotal: 5.95s\tremaining: 0us\n",
            "0:\tlearn: 0.2458566\ttotal: 170ms\tremaining: 8.32s\n",
            "1:\tlearn: 0.2228357\ttotal: 284ms\tremaining: 6.83s\n",
            "2:\tlearn: 0.2029464\ttotal: 410ms\tremaining: 6.42s\n",
            "3:\tlearn: 0.1867380\ttotal: 528ms\tremaining: 6.07s\n",
            "4:\tlearn: 0.1708241\ttotal: 650ms\tremaining: 5.85s\n",
            "5:\tlearn: 0.1582391\ttotal: 764ms\tremaining: 5.6s\n",
            "6:\tlearn: 0.1483347\ttotal: 904ms\tremaining: 5.55s\n",
            "7:\tlearn: 0.1417043\ttotal: 1.02s\tremaining: 5.35s\n",
            "8:\tlearn: 0.1338119\ttotal: 1.16s\tremaining: 5.27s\n",
            "9:\tlearn: 0.1286706\ttotal: 1.27s\tremaining: 5.08s\n",
            "10:\tlearn: 0.1234601\ttotal: 1.39s\tremaining: 4.93s\n",
            "11:\tlearn: 0.1195018\ttotal: 1.5s\tremaining: 4.76s\n",
            "12:\tlearn: 0.1150063\ttotal: 1.63s\tremaining: 4.63s\n",
            "13:\tlearn: 0.1103487\ttotal: 1.74s\tremaining: 4.47s\n",
            "14:\tlearn: 0.1066968\ttotal: 1.86s\tremaining: 4.34s\n",
            "15:\tlearn: 0.1038002\ttotal: 1.99s\tremaining: 4.22s\n",
            "16:\tlearn: 0.1023229\ttotal: 2.11s\tremaining: 4.1s\n",
            "17:\tlearn: 0.1005704\ttotal: 2.22s\tremaining: 3.95s\n",
            "18:\tlearn: 0.0988856\ttotal: 2.35s\tremaining: 3.84s\n",
            "19:\tlearn: 0.0980131\ttotal: 2.47s\tremaining: 3.7s\n",
            "20:\tlearn: 0.0971765\ttotal: 2.59s\tremaining: 3.58s\n",
            "21:\tlearn: 0.0961425\ttotal: 2.71s\tremaining: 3.44s\n",
            "22:\tlearn: 0.0943659\ttotal: 2.83s\tremaining: 3.33s\n",
            "23:\tlearn: 0.0935342\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0928639\ttotal: 3.09s\tremaining: 3.09s\n",
            "25:\tlearn: 0.0922300\ttotal: 3.2s\tremaining: 2.95s\n",
            "26:\tlearn: 0.0907212\ttotal: 3.32s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0896715\ttotal: 3.44s\tremaining: 2.7s\n",
            "28:\tlearn: 0.0879452\ttotal: 3.56s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0865372\ttotal: 3.68s\tremaining: 2.45s\n",
            "30:\tlearn: 0.0860005\ttotal: 3.8s\tremaining: 2.33s\n",
            "31:\tlearn: 0.0849490\ttotal: 3.92s\tremaining: 2.2s\n",
            "32:\tlearn: 0.0837512\ttotal: 4.05s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0828159\ttotal: 4.17s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0818373\ttotal: 4.29s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0810001\ttotal: 4.4s\tremaining: 1.71s\n",
            "36:\tlearn: 0.0806197\ttotal: 4.52s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0798808\ttotal: 4.64s\tremaining: 1.46s\n",
            "38:\tlearn: 0.0791099\ttotal: 4.76s\tremaining: 1.34s\n",
            "39:\tlearn: 0.0786345\ttotal: 4.9s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0778772\ttotal: 5.04s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0774297\ttotal: 5.15s\tremaining: 981ms\n",
            "42:\tlearn: 0.0765160\ttotal: 5.28s\tremaining: 859ms\n",
            "43:\tlearn: 0.0757068\ttotal: 5.39s\tremaining: 735ms\n",
            "44:\tlearn: 0.0746012\ttotal: 5.51s\tremaining: 613ms\n",
            "45:\tlearn: 0.0739861\ttotal: 5.63s\tremaining: 490ms\n",
            "46:\tlearn: 0.0734522\ttotal: 5.76s\tremaining: 367ms\n",
            "47:\tlearn: 0.0729780\ttotal: 5.87s\tremaining: 245ms\n",
            "48:\tlearn: 0.0721827\ttotal: 6s\tremaining: 122ms\n",
            "49:\tlearn: 0.0713642\ttotal: 6.13s\tremaining: 0us\n",
            "0:\tlearn: 0.0357403\ttotal: 177ms\tremaining: 8.69s\n",
            "1:\tlearn: 0.0325142\ttotal: 295ms\tremaining: 7.07s\n",
            "2:\tlearn: 0.0293682\ttotal: 421ms\tremaining: 6.6s\n",
            "3:\tlearn: 0.0267575\ttotal: 534ms\tremaining: 6.14s\n",
            "4:\tlearn: 0.0244913\ttotal: 658ms\tremaining: 5.92s\n",
            "5:\tlearn: 0.0223653\ttotal: 777ms\tremaining: 5.7s\n",
            "6:\tlearn: 0.0205725\ttotal: 917ms\tremaining: 5.63s\n",
            "7:\tlearn: 0.0190783\ttotal: 1.04s\tremaining: 5.46s\n",
            "8:\tlearn: 0.0180292\ttotal: 1.16s\tremaining: 5.31s\n",
            "9:\tlearn: 0.0169392\ttotal: 1.28s\tremaining: 5.13s\n",
            "10:\tlearn: 0.0161951\ttotal: 1.41s\tremaining: 4.98s\n",
            "11:\tlearn: 0.0153573\ttotal: 1.52s\tremaining: 4.81s\n",
            "12:\tlearn: 0.0146458\ttotal: 1.65s\tremaining: 4.69s\n",
            "13:\tlearn: 0.0140874\ttotal: 1.76s\tremaining: 4.53s\n",
            "14:\tlearn: 0.0136337\ttotal: 1.89s\tremaining: 4.42s\n",
            "15:\tlearn: 0.0131693\ttotal: 2.01s\tremaining: 4.26s\n",
            "16:\tlearn: 0.0127012\ttotal: 2.13s\tremaining: 4.13s\n",
            "17:\tlearn: 0.0123384\ttotal: 2.25s\tremaining: 4.01s\n",
            "18:\tlearn: 0.0120680\ttotal: 2.38s\tremaining: 3.88s\n",
            "19:\tlearn: 0.0118796\ttotal: 2.49s\tremaining: 3.74s\n",
            "20:\tlearn: 0.0116932\ttotal: 2.62s\tremaining: 3.62s\n",
            "21:\tlearn: 0.0113815\ttotal: 2.74s\tremaining: 3.49s\n",
            "22:\tlearn: 0.0111485\ttotal: 2.87s\tremaining: 3.37s\n",
            "23:\tlearn: 0.0109673\ttotal: 2.99s\tremaining: 3.24s\n",
            "24:\tlearn: 0.0108927\ttotal: 3.12s\tremaining: 3.12s\n",
            "25:\tlearn: 0.0107821\ttotal: 3.23s\tremaining: 2.99s\n",
            "26:\tlearn: 0.0107180\ttotal: 3.36s\tremaining: 2.86s\n",
            "27:\tlearn: 0.0105671\ttotal: 3.48s\tremaining: 2.73s\n",
            "28:\tlearn: 0.0104103\ttotal: 3.6s\tremaining: 2.61s\n",
            "29:\tlearn: 0.0102498\ttotal: 3.71s\tremaining: 2.48s\n",
            "30:\tlearn: 0.0100305\ttotal: 3.83s\tremaining: 2.35s\n",
            "31:\tlearn: 0.0099646\ttotal: 3.97s\tremaining: 2.23s\n",
            "32:\tlearn: 0.0098447\ttotal: 4.09s\tremaining: 2.11s\n",
            "33:\tlearn: 0.0097703\ttotal: 4.21s\tremaining: 1.98s\n",
            "34:\tlearn: 0.0095460\ttotal: 4.33s\tremaining: 1.86s\n",
            "35:\tlearn: 0.0094270\ttotal: 4.45s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0093509\ttotal: 4.57s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0092584\ttotal: 4.69s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0091233\ttotal: 4.81s\tremaining: 1.36s\n",
            "39:\tlearn: 0.0089747\ttotal: 4.93s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0089213\ttotal: 5.08s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0088523\ttotal: 5.2s\tremaining: 990ms\n",
            "42:\tlearn: 0.0087651\ttotal: 5.32s\tremaining: 867ms\n",
            "43:\tlearn: 0.0086788\ttotal: 5.44s\tremaining: 742ms\n",
            "44:\tlearn: 0.0085991\ttotal: 5.56s\tremaining: 618ms\n",
            "45:\tlearn: 0.0085191\ttotal: 5.68s\tremaining: 494ms\n",
            "46:\tlearn: 0.0084885\ttotal: 5.8s\tremaining: 370ms\n",
            "47:\tlearn: 0.0084499\ttotal: 5.91s\tremaining: 246ms\n",
            "48:\tlearn: 0.0083816\ttotal: 6.05s\tremaining: 123ms\n",
            "49:\tlearn: 0.0083274\ttotal: 6.16s\tremaining: 0us\n",
            "0:\tlearn: 0.0196061\ttotal: 179ms\tremaining: 8.79s\n",
            "1:\tlearn: 0.0186640\ttotal: 294ms\tremaining: 7.05s\n",
            "2:\tlearn: 0.0173661\ttotal: 423ms\tremaining: 6.62s\n",
            "3:\tlearn: 0.0162514\ttotal: 536ms\tremaining: 6.16s\n",
            "4:\tlearn: 0.0148555\ttotal: 660ms\tremaining: 5.94s\n",
            "5:\tlearn: 0.0137372\ttotal: 785ms\tremaining: 5.76s\n",
            "6:\tlearn: 0.0129096\ttotal: 904ms\tremaining: 5.55s\n",
            "7:\tlearn: 0.0120677\ttotal: 1.02s\tremaining: 5.35s\n",
            "8:\tlearn: 0.0116093\ttotal: 1.14s\tremaining: 5.21s\n",
            "9:\tlearn: 0.0113529\ttotal: 1.25s\tremaining: 5.02s\n",
            "10:\tlearn: 0.0108874\ttotal: 1.38s\tremaining: 4.88s\n",
            "11:\tlearn: 0.0106766\ttotal: 1.49s\tremaining: 4.72s\n",
            "12:\tlearn: 0.0103493\ttotal: 1.61s\tremaining: 4.6s\n",
            "13:\tlearn: 0.0098547\ttotal: 1.73s\tremaining: 4.44s\n",
            "14:\tlearn: 0.0094459\ttotal: 1.86s\tremaining: 4.34s\n",
            "15:\tlearn: 0.0090113\ttotal: 1.97s\tremaining: 4.2s\n",
            "16:\tlearn: 0.0088736\ttotal: 2.1s\tremaining: 4.08s\n",
            "17:\tlearn: 0.0088285\ttotal: 2.22s\tremaining: 3.95s\n",
            "18:\tlearn: 0.0087353\ttotal: 2.35s\tremaining: 3.84s\n",
            "19:\tlearn: 0.0084612\ttotal: 2.47s\tremaining: 3.71s\n",
            "20:\tlearn: 0.0084034\ttotal: 2.61s\tremaining: 3.61s\n",
            "21:\tlearn: 0.0083784\ttotal: 2.74s\tremaining: 3.48s\n",
            "22:\tlearn: 0.0081753\ttotal: 2.87s\tremaining: 3.37s\n",
            "23:\tlearn: 0.0080613\ttotal: 2.98s\tremaining: 3.23s\n",
            "24:\tlearn: 0.0080235\ttotal: 3.11s\tremaining: 3.11s\n",
            "25:\tlearn: 0.0078174\ttotal: 3.23s\tremaining: 2.98s\n",
            "26:\tlearn: 0.0076938\ttotal: 3.34s\tremaining: 2.85s\n",
            "27:\tlearn: 0.0075281\ttotal: 3.46s\tremaining: 2.72s\n",
            "28:\tlearn: 0.0073559\ttotal: 3.59s\tremaining: 2.6s\n",
            "29:\tlearn: 0.0071866\ttotal: 3.7s\tremaining: 2.47s\n",
            "30:\tlearn: 0.0071687\ttotal: 3.83s\tremaining: 2.35s\n",
            "31:\tlearn: 0.0070239\ttotal: 3.96s\tremaining: 2.23s\n",
            "32:\tlearn: 0.0069583\ttotal: 4.08s\tremaining: 2.1s\n",
            "33:\tlearn: 0.0068359\ttotal: 4.2s\tremaining: 1.98s\n",
            "34:\tlearn: 0.0066890\ttotal: 4.33s\tremaining: 1.85s\n",
            "35:\tlearn: 0.0066109\ttotal: 4.45s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0065248\ttotal: 4.57s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0065008\ttotal: 4.68s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0064004\ttotal: 4.81s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0062822\ttotal: 4.93s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0062055\ttotal: 5.06s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0060919\ttotal: 5.18s\tremaining: 987ms\n",
            "42:\tlearn: 0.0060596\ttotal: 5.32s\tremaining: 865ms\n",
            "43:\tlearn: 0.0060398\ttotal: 5.43s\tremaining: 741ms\n",
            "44:\tlearn: 0.0059386\ttotal: 5.55s\tremaining: 617ms\n",
            "45:\tlearn: 0.0058242\ttotal: 5.66s\tremaining: 493ms\n",
            "46:\tlearn: 0.0057053\ttotal: 5.79s\tremaining: 370ms\n",
            "47:\tlearn: 0.0056232\ttotal: 5.92s\tremaining: 247ms\n",
            "48:\tlearn: 0.0055885\ttotal: 6.04s\tremaining: 123ms\n",
            "49:\tlearn: 0.0055117\ttotal: 6.16s\tremaining: 0us\n",
            "0:\tlearn: 0.0369947\ttotal: 177ms\tremaining: 8.68s\n",
            "1:\tlearn: 0.0333747\ttotal: 291ms\tremaining: 6.97s\n",
            "2:\tlearn: 0.0302941\ttotal: 411ms\tremaining: 6.45s\n",
            "3:\tlearn: 0.0276850\ttotal: 523ms\tremaining: 6.02s\n",
            "4:\tlearn: 0.0253754\ttotal: 649ms\tremaining: 5.84s\n",
            "5:\tlearn: 0.0232808\ttotal: 776ms\tremaining: 5.69s\n",
            "6:\tlearn: 0.0216360\ttotal: 897ms\tremaining: 5.51s\n",
            "7:\tlearn: 0.0201999\ttotal: 1.01s\tremaining: 5.32s\n",
            "8:\tlearn: 0.0189114\ttotal: 1.14s\tremaining: 5.2s\n",
            "9:\tlearn: 0.0180892\ttotal: 1.25s\tremaining: 5.02s\n",
            "10:\tlearn: 0.0172242\ttotal: 1.38s\tremaining: 4.89s\n",
            "11:\tlearn: 0.0165082\ttotal: 1.49s\tremaining: 4.73s\n",
            "12:\tlearn: 0.0159550\ttotal: 1.61s\tremaining: 4.6s\n",
            "13:\tlearn: 0.0154948\ttotal: 1.74s\tremaining: 4.48s\n",
            "14:\tlearn: 0.0150099\ttotal: 1.87s\tremaining: 4.35s\n",
            "15:\tlearn: 0.0146147\ttotal: 1.98s\tremaining: 4.21s\n",
            "16:\tlearn: 0.0142377\ttotal: 2.1s\tremaining: 4.08s\n",
            "17:\tlearn: 0.0139442\ttotal: 2.22s\tremaining: 3.94s\n",
            "18:\tlearn: 0.0137851\ttotal: 2.34s\tremaining: 3.81s\n",
            "19:\tlearn: 0.0135009\ttotal: 2.45s\tremaining: 3.67s\n",
            "20:\tlearn: 0.0133345\ttotal: 2.57s\tremaining: 3.55s\n",
            "21:\tlearn: 0.0131659\ttotal: 2.69s\tremaining: 3.42s\n",
            "22:\tlearn: 0.0128947\ttotal: 2.82s\tremaining: 3.31s\n",
            "23:\tlearn: 0.0127388\ttotal: 2.93s\tremaining: 3.18s\n",
            "24:\tlearn: 0.0125753\ttotal: 3.06s\tremaining: 3.06s\n",
            "25:\tlearn: 0.0124625\ttotal: 3.17s\tremaining: 2.93s\n",
            "26:\tlearn: 0.0123637\ttotal: 3.29s\tremaining: 2.81s\n",
            "27:\tlearn: 0.0122688\ttotal: 3.41s\tremaining: 2.68s\n",
            "28:\tlearn: 0.0121194\ttotal: 3.54s\tremaining: 2.56s\n",
            "29:\tlearn: 0.0119598\ttotal: 3.66s\tremaining: 2.44s\n",
            "30:\tlearn: 0.0117578\ttotal: 3.79s\tremaining: 2.33s\n",
            "31:\tlearn: 0.0115578\ttotal: 3.91s\tremaining: 2.2s\n",
            "32:\tlearn: 0.0114495\ttotal: 4.04s\tremaining: 2.08s\n",
            "33:\tlearn: 0.0113126\ttotal: 4.15s\tremaining: 1.95s\n",
            "34:\tlearn: 0.0112052\ttotal: 4.27s\tremaining: 1.83s\n",
            "35:\tlearn: 0.0110422\ttotal: 4.38s\tremaining: 1.71s\n",
            "36:\tlearn: 0.0109830\ttotal: 4.51s\tremaining: 1.58s\n",
            "37:\tlearn: 0.0108319\ttotal: 4.62s\tremaining: 1.46s\n",
            "38:\tlearn: 0.0107668\ttotal: 4.75s\tremaining: 1.34s\n",
            "39:\tlearn: 0.0106096\ttotal: 4.87s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0105125\ttotal: 5s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0104519\ttotal: 5.12s\tremaining: 976ms\n",
            "42:\tlearn: 0.0103662\ttotal: 5.24s\tremaining: 853ms\n",
            "43:\tlearn: 0.0102428\ttotal: 5.36s\tremaining: 731ms\n",
            "44:\tlearn: 0.0101562\ttotal: 5.48s\tremaining: 609ms\n",
            "45:\tlearn: 0.0100790\ttotal: 5.6s\tremaining: 487ms\n",
            "46:\tlearn: 0.0100069\ttotal: 5.72s\tremaining: 365ms\n",
            "47:\tlearn: 0.0098959\ttotal: 5.85s\tremaining: 244ms\n",
            "48:\tlearn: 0.0098065\ttotal: 5.98s\tremaining: 122ms\n",
            "49:\tlearn: 0.0097472\ttotal: 6.09s\tremaining: 0us\n",
            "0:\tlearn: 0.2704597\ttotal: 167ms\tremaining: 8.17s\n",
            "1:\tlearn: 0.2429763\ttotal: 282ms\tremaining: 6.77s\n",
            "2:\tlearn: 0.2204516\ttotal: 403ms\tremaining: 6.31s\n",
            "3:\tlearn: 0.1987365\ttotal: 517ms\tremaining: 5.95s\n",
            "4:\tlearn: 0.1834733\ttotal: 648ms\tremaining: 5.83s\n",
            "5:\tlearn: 0.1690331\ttotal: 772ms\tremaining: 5.66s\n",
            "6:\tlearn: 0.1583485\ttotal: 901ms\tremaining: 5.54s\n",
            "7:\tlearn: 0.1490789\ttotal: 1.02s\tremaining: 5.34s\n",
            "8:\tlearn: 0.1420858\ttotal: 1.14s\tremaining: 5.21s\n",
            "9:\tlearn: 0.1366213\ttotal: 1.26s\tremaining: 5.03s\n",
            "10:\tlearn: 0.1299062\ttotal: 1.38s\tremaining: 4.89s\n",
            "11:\tlearn: 0.1258207\ttotal: 1.5s\tremaining: 4.73s\n",
            "12:\tlearn: 0.1211572\ttotal: 1.62s\tremaining: 4.61s\n",
            "13:\tlearn: 0.1162544\ttotal: 1.75s\tremaining: 4.5s\n",
            "14:\tlearn: 0.1121401\ttotal: 1.88s\tremaining: 4.38s\n",
            "15:\tlearn: 0.1090958\ttotal: 1.99s\tremaining: 4.24s\n",
            "16:\tlearn: 0.1072274\ttotal: 2.12s\tremaining: 4.11s\n",
            "17:\tlearn: 0.1052158\ttotal: 2.24s\tremaining: 3.98s\n",
            "18:\tlearn: 0.1032241\ttotal: 2.36s\tremaining: 3.86s\n",
            "19:\tlearn: 0.1019550\ttotal: 2.48s\tremaining: 3.72s\n",
            "20:\tlearn: 0.1009169\ttotal: 2.6s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0998962\ttotal: 2.73s\tremaining: 3.48s\n",
            "22:\tlearn: 0.0981113\ttotal: 2.86s\tremaining: 3.36s\n",
            "23:\tlearn: 0.0959436\ttotal: 2.99s\tremaining: 3.24s\n",
            "24:\tlearn: 0.0947874\ttotal: 3.13s\tremaining: 3.13s\n",
            "25:\tlearn: 0.0939001\ttotal: 3.25s\tremaining: 3s\n",
            "26:\tlearn: 0.0933541\ttotal: 3.38s\tremaining: 2.88s\n",
            "27:\tlearn: 0.0926435\ttotal: 3.5s\tremaining: 2.75s\n",
            "28:\tlearn: 0.0916778\ttotal: 3.63s\tremaining: 2.63s\n",
            "29:\tlearn: 0.0902814\ttotal: 3.76s\tremaining: 2.51s\n",
            "30:\tlearn: 0.0894034\ttotal: 3.89s\tremaining: 2.39s\n",
            "31:\tlearn: 0.0886934\ttotal: 4.01s\tremaining: 2.26s\n",
            "32:\tlearn: 0.0874322\ttotal: 4.14s\tremaining: 2.13s\n",
            "33:\tlearn: 0.0861019\ttotal: 4.26s\tremaining: 2s\n",
            "34:\tlearn: 0.0853065\ttotal: 4.39s\tremaining: 1.88s\n",
            "35:\tlearn: 0.0839327\ttotal: 4.51s\tremaining: 1.75s\n",
            "36:\tlearn: 0.0833945\ttotal: 4.63s\tremaining: 1.63s\n",
            "37:\tlearn: 0.0827244\ttotal: 4.75s\tremaining: 1.5s\n",
            "38:\tlearn: 0.0819798\ttotal: 4.89s\tremaining: 1.38s\n",
            "39:\tlearn: 0.0814944\ttotal: 5s\tremaining: 1.25s\n",
            "40:\tlearn: 0.0805695\ttotal: 5.13s\tremaining: 1.13s\n",
            "41:\tlearn: 0.0802139\ttotal: 5.24s\tremaining: 999ms\n",
            "42:\tlearn: 0.0794722\ttotal: 5.37s\tremaining: 874ms\n",
            "43:\tlearn: 0.0787127\ttotal: 5.48s\tremaining: 748ms\n",
            "44:\tlearn: 0.0780514\ttotal: 5.61s\tremaining: 623ms\n",
            "45:\tlearn: 0.0775156\ttotal: 5.72s\tremaining: 498ms\n",
            "46:\tlearn: 0.0768571\ttotal: 5.86s\tremaining: 374ms\n",
            "47:\tlearn: 0.0764322\ttotal: 5.98s\tremaining: 249ms\n",
            "48:\tlearn: 0.0757585\ttotal: 6.11s\tremaining: 125ms\n",
            "49:\tlearn: 0.0752294\ttotal: 6.22s\tremaining: 0us\n",
            "0:\tlearn: 0.0378613\ttotal: 180ms\tremaining: 8.83s\n",
            "1:\tlearn: 0.0341923\ttotal: 296ms\tremaining: 7.09s\n",
            "2:\tlearn: 0.0308520\ttotal: 415ms\tremaining: 6.5s\n",
            "3:\tlearn: 0.0281649\ttotal: 540ms\tremaining: 6.21s\n",
            "4:\tlearn: 0.0256943\ttotal: 684ms\tremaining: 6.16s\n",
            "5:\tlearn: 0.0235765\ttotal: 800ms\tremaining: 5.87s\n",
            "6:\tlearn: 0.0218177\ttotal: 927ms\tremaining: 5.7s\n",
            "7:\tlearn: 0.0202010\ttotal: 1.04s\tremaining: 5.48s\n",
            "8:\tlearn: 0.0188517\ttotal: 1.17s\tremaining: 5.33s\n",
            "9:\tlearn: 0.0178027\ttotal: 1.29s\tremaining: 5.15s\n",
            "10:\tlearn: 0.0170085\ttotal: 1.41s\tremaining: 5s\n",
            "11:\tlearn: 0.0161716\ttotal: 1.53s\tremaining: 4.85s\n",
            "12:\tlearn: 0.0154243\ttotal: 1.67s\tremaining: 4.75s\n",
            "13:\tlearn: 0.0148183\ttotal: 1.78s\tremaining: 4.59s\n",
            "14:\tlearn: 0.0142800\ttotal: 1.91s\tremaining: 4.47s\n",
            "15:\tlearn: 0.0137730\ttotal: 2.03s\tremaining: 4.31s\n",
            "16:\tlearn: 0.0133006\ttotal: 2.15s\tremaining: 4.18s\n",
            "17:\tlearn: 0.0128220\ttotal: 2.27s\tremaining: 4.04s\n",
            "18:\tlearn: 0.0124639\ttotal: 2.4s\tremaining: 3.92s\n",
            "19:\tlearn: 0.0121721\ttotal: 2.52s\tremaining: 3.78s\n",
            "20:\tlearn: 0.0119486\ttotal: 2.65s\tremaining: 3.67s\n",
            "21:\tlearn: 0.0117677\ttotal: 2.77s\tremaining: 3.52s\n",
            "22:\tlearn: 0.0115817\ttotal: 2.89s\tremaining: 3.4s\n",
            "23:\tlearn: 0.0114374\ttotal: 3s\tremaining: 3.26s\n",
            "24:\tlearn: 0.0112964\ttotal: 3.13s\tremaining: 3.13s\n",
            "25:\tlearn: 0.0111493\ttotal: 3.25s\tremaining: 3s\n",
            "26:\tlearn: 0.0110861\ttotal: 3.38s\tremaining: 2.88s\n",
            "27:\tlearn: 0.0109919\ttotal: 3.49s\tremaining: 2.74s\n",
            "28:\tlearn: 0.0108596\ttotal: 3.62s\tremaining: 2.62s\n",
            "29:\tlearn: 0.0107342\ttotal: 3.74s\tremaining: 2.49s\n",
            "30:\tlearn: 0.0106395\ttotal: 3.87s\tremaining: 2.37s\n",
            "31:\tlearn: 0.0105423\ttotal: 3.99s\tremaining: 2.25s\n",
            "32:\tlearn: 0.0104557\ttotal: 4.11s\tremaining: 2.12s\n",
            "33:\tlearn: 0.0103679\ttotal: 4.23s\tremaining: 1.99s\n",
            "34:\tlearn: 0.0102761\ttotal: 4.36s\tremaining: 1.87s\n",
            "35:\tlearn: 0.0101078\ttotal: 4.47s\tremaining: 1.74s\n",
            "36:\tlearn: 0.0100195\ttotal: 4.6s\tremaining: 1.62s\n",
            "37:\tlearn: 0.0099371\ttotal: 4.73s\tremaining: 1.49s\n",
            "38:\tlearn: 0.0098422\ttotal: 4.86s\tremaining: 1.37s\n",
            "39:\tlearn: 0.0097534\ttotal: 4.97s\tremaining: 1.24s\n",
            "40:\tlearn: 0.0096961\ttotal: 5.09s\tremaining: 1.12s\n",
            "41:\tlearn: 0.0096353\ttotal: 5.21s\tremaining: 992ms\n",
            "42:\tlearn: 0.0095577\ttotal: 5.33s\tremaining: 869ms\n",
            "43:\tlearn: 0.0095099\ttotal: 5.45s\tremaining: 743ms\n",
            "44:\tlearn: 0.0094294\ttotal: 5.57s\tremaining: 619ms\n",
            "45:\tlearn: 0.0093769\ttotal: 5.7s\tremaining: 495ms\n",
            "46:\tlearn: 0.0093105\ttotal: 5.82s\tremaining: 372ms\n",
            "47:\tlearn: 0.0092267\ttotal: 5.94s\tremaining: 247ms\n",
            "48:\tlearn: 0.0092009\ttotal: 6.06s\tremaining: 124ms\n",
            "49:\tlearn: 0.0090974\ttotal: 6.18s\tremaining: 0us\n",
            "0:\tlearn: 0.0245400\ttotal: 177ms\tremaining: 8.68s\n",
            "1:\tlearn: 0.0231751\ttotal: 297ms\tremaining: 7.13s\n",
            "2:\tlearn: 0.0215958\ttotal: 436ms\tremaining: 6.82s\n",
            "3:\tlearn: 0.0200714\ttotal: 555ms\tremaining: 6.38s\n",
            "4:\tlearn: 0.0181836\ttotal: 675ms\tremaining: 6.07s\n",
            "5:\tlearn: 0.0175793\ttotal: 791ms\tremaining: 5.8s\n",
            "6:\tlearn: 0.0165459\ttotal: 919ms\tremaining: 5.64s\n",
            "7:\tlearn: 0.0153536\ttotal: 1.03s\tremaining: 5.43s\n",
            "8:\tlearn: 0.0144502\ttotal: 1.15s\tremaining: 5.26s\n",
            "9:\tlearn: 0.0135734\ttotal: 1.26s\tremaining: 5.06s\n",
            "10:\tlearn: 0.0129802\ttotal: 1.39s\tremaining: 4.92s\n",
            "11:\tlearn: 0.0127493\ttotal: 1.51s\tremaining: 4.79s\n",
            "12:\tlearn: 0.0121889\ttotal: 1.64s\tremaining: 4.66s\n",
            "13:\tlearn: 0.0117622\ttotal: 1.75s\tremaining: 4.5s\n",
            "14:\tlearn: 0.0114068\ttotal: 1.87s\tremaining: 4.37s\n",
            "15:\tlearn: 0.0111462\ttotal: 1.99s\tremaining: 4.23s\n",
            "16:\tlearn: 0.0109004\ttotal: 2.11s\tremaining: 4.1s\n",
            "17:\tlearn: 0.0107775\ttotal: 2.23s\tremaining: 3.97s\n",
            "18:\tlearn: 0.0105664\ttotal: 2.35s\tremaining: 3.84s\n",
            "19:\tlearn: 0.0102681\ttotal: 2.47s\tremaining: 3.7s\n",
            "20:\tlearn: 0.0101778\ttotal: 2.61s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0100509\ttotal: 2.72s\tremaining: 3.47s\n",
            "22:\tlearn: 0.0098092\ttotal: 2.85s\tremaining: 3.34s\n",
            "23:\tlearn: 0.0096936\ttotal: 2.96s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0096468\ttotal: 3.09s\tremaining: 3.09s\n",
            "25:\tlearn: 0.0094274\ttotal: 3.2s\tremaining: 2.95s\n",
            "26:\tlearn: 0.0093369\ttotal: 3.33s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0092089\ttotal: 3.44s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0091204\ttotal: 3.58s\tremaining: 2.59s\n",
            "29:\tlearn: 0.0090161\ttotal: 3.69s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0088844\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0086887\ttotal: 3.94s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0085513\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0084109\ttotal: 4.17s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0082592\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0081691\ttotal: 4.42s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0080417\ttotal: 4.56s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0079375\ttotal: 4.68s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0078885\ttotal: 4.8s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0077629\ttotal: 4.92s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0077370\ttotal: 5.04s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0076208\ttotal: 5.16s\tremaining: 982ms\n",
            "42:\tlearn: 0.0076022\ttotal: 5.29s\tremaining: 861ms\n",
            "43:\tlearn: 0.0075913\ttotal: 5.41s\tremaining: 738ms\n",
            "44:\tlearn: 0.0075454\ttotal: 5.53s\tremaining: 614ms\n",
            "45:\tlearn: 0.0074311\ttotal: 5.65s\tremaining: 491ms\n",
            "46:\tlearn: 0.0073387\ttotal: 5.77s\tremaining: 368ms\n",
            "47:\tlearn: 0.0072543\ttotal: 5.88s\tremaining: 245ms\n",
            "48:\tlearn: 0.0071695\ttotal: 6.01s\tremaining: 123ms\n",
            "49:\tlearn: 0.0071380\ttotal: 6.12s\tremaining: 0us\n",
            "0:\tlearn: 0.0401077\ttotal: 177ms\tremaining: 8.68s\n",
            "1:\tlearn: 0.0360017\ttotal: 293ms\tremaining: 7.03s\n",
            "2:\tlearn: 0.0324197\ttotal: 427ms\tremaining: 6.68s\n",
            "3:\tlearn: 0.0294480\ttotal: 542ms\tremaining: 6.24s\n",
            "4:\tlearn: 0.0267113\ttotal: 668ms\tremaining: 6.01s\n",
            "5:\tlearn: 0.0246575\ttotal: 786ms\tremaining: 5.76s\n",
            "6:\tlearn: 0.0229115\ttotal: 910ms\tremaining: 5.59s\n",
            "7:\tlearn: 0.0212260\ttotal: 1.02s\tremaining: 5.38s\n",
            "8:\tlearn: 0.0199535\ttotal: 1.15s\tremaining: 5.23s\n",
            "9:\tlearn: 0.0190523\ttotal: 1.26s\tremaining: 5.05s\n",
            "10:\tlearn: 0.0181050\ttotal: 1.39s\tremaining: 4.92s\n",
            "11:\tlearn: 0.0173781\ttotal: 1.53s\tremaining: 4.83s\n",
            "12:\tlearn: 0.0166883\ttotal: 1.67s\tremaining: 4.74s\n",
            "13:\tlearn: 0.0160236\ttotal: 1.79s\tremaining: 4.59s\n",
            "14:\tlearn: 0.0155817\ttotal: 1.91s\tremaining: 4.46s\n",
            "15:\tlearn: 0.0152055\ttotal: 2.03s\tremaining: 4.31s\n",
            "16:\tlearn: 0.0148459\ttotal: 2.15s\tremaining: 4.18s\n",
            "17:\tlearn: 0.0143876\ttotal: 2.27s\tremaining: 4.03s\n",
            "18:\tlearn: 0.0140897\ttotal: 2.39s\tremaining: 3.9s\n",
            "19:\tlearn: 0.0137691\ttotal: 2.52s\tremaining: 3.78s\n",
            "20:\tlearn: 0.0135973\ttotal: 2.64s\tremaining: 3.65s\n",
            "21:\tlearn: 0.0133887\ttotal: 2.76s\tremaining: 3.51s\n",
            "22:\tlearn: 0.0131796\ttotal: 2.89s\tremaining: 3.39s\n",
            "23:\tlearn: 0.0130339\ttotal: 3.01s\tremaining: 3.26s\n",
            "24:\tlearn: 0.0128853\ttotal: 3.13s\tremaining: 3.13s\n",
            "25:\tlearn: 0.0127623\ttotal: 3.24s\tremaining: 2.99s\n",
            "26:\tlearn: 0.0126195\ttotal: 3.37s\tremaining: 2.87s\n",
            "27:\tlearn: 0.0125452\ttotal: 3.49s\tremaining: 2.74s\n",
            "28:\tlearn: 0.0123316\ttotal: 3.61s\tremaining: 2.61s\n",
            "29:\tlearn: 0.0122041\ttotal: 3.72s\tremaining: 2.48s\n",
            "30:\tlearn: 0.0121174\ttotal: 3.85s\tremaining: 2.36s\n",
            "31:\tlearn: 0.0120061\ttotal: 3.97s\tremaining: 2.23s\n",
            "32:\tlearn: 0.0118412\ttotal: 4.09s\tremaining: 2.11s\n",
            "33:\tlearn: 0.0116898\ttotal: 4.21s\tremaining: 1.98s\n",
            "34:\tlearn: 0.0115871\ttotal: 4.33s\tremaining: 1.86s\n",
            "35:\tlearn: 0.0114945\ttotal: 4.45s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0114041\ttotal: 4.58s\tremaining: 1.61s\n",
            "37:\tlearn: 0.0112422\ttotal: 4.7s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0111521\ttotal: 4.82s\tremaining: 1.36s\n",
            "39:\tlearn: 0.0110551\ttotal: 4.93s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0109316\ttotal: 5.06s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0108510\ttotal: 5.17s\tremaining: 986ms\n",
            "42:\tlearn: 0.0107732\ttotal: 5.29s\tremaining: 862ms\n",
            "43:\tlearn: 0.0107020\ttotal: 5.41s\tremaining: 738ms\n",
            "44:\tlearn: 0.0106297\ttotal: 5.55s\tremaining: 617ms\n",
            "45:\tlearn: 0.0105555\ttotal: 5.67s\tremaining: 493ms\n",
            "46:\tlearn: 0.0104773\ttotal: 5.79s\tremaining: 370ms\n",
            "47:\tlearn: 0.0104170\ttotal: 5.91s\tremaining: 246ms\n",
            "48:\tlearn: 0.0103179\ttotal: 6.03s\tremaining: 123ms\n",
            "49:\tlearn: 0.0102390\ttotal: 6.15s\tremaining: 0us\n",
            "0:\tlearn: 0.2101943\ttotal: 143ms\tremaining: 7.01s\n",
            "1:\tlearn: 0.1951658\ttotal: 238ms\tremaining: 5.71s\n",
            "2:\tlearn: 0.1819107\ttotal: 348ms\tremaining: 5.45s\n",
            "3:\tlearn: 0.1700092\ttotal: 449ms\tremaining: 5.16s\n",
            "4:\tlearn: 0.1597972\ttotal: 544ms\tremaining: 4.9s\n",
            "5:\tlearn: 0.1507550\ttotal: 649ms\tremaining: 4.76s\n",
            "6:\tlearn: 0.1418500\ttotal: 745ms\tremaining: 4.57s\n",
            "7:\tlearn: 0.1333564\ttotal: 838ms\tremaining: 4.4s\n",
            "8:\tlearn: 0.1267376\ttotal: 941ms\tremaining: 4.29s\n",
            "9:\tlearn: 0.1226424\ttotal: 1.03s\tremaining: 4.14s\n",
            "10:\tlearn: 0.1170833\ttotal: 1.13s\tremaining: 4s\n",
            "11:\tlearn: 0.1122793\ttotal: 1.23s\tremaining: 3.9s\n",
            "12:\tlearn: 0.1082003\ttotal: 1.33s\tremaining: 3.78s\n",
            "13:\tlearn: 0.1044683\ttotal: 1.44s\tremaining: 3.69s\n",
            "14:\tlearn: 0.1001022\ttotal: 1.54s\tremaining: 3.59s\n",
            "15:\tlearn: 0.0974965\ttotal: 1.63s\tremaining: 3.47s\n",
            "16:\tlearn: 0.0937507\ttotal: 1.74s\tremaining: 3.37s\n",
            "17:\tlearn: 0.0915635\ttotal: 1.83s\tremaining: 3.26s\n",
            "18:\tlearn: 0.0895578\ttotal: 1.95s\tremaining: 3.18s\n",
            "19:\tlearn: 0.0862063\ttotal: 2.06s\tremaining: 3.09s\n",
            "20:\tlearn: 0.0850209\ttotal: 2.16s\tremaining: 2.98s\n",
            "21:\tlearn: 0.0835137\ttotal: 2.26s\tremaining: 2.88s\n",
            "22:\tlearn: 0.0814319\ttotal: 2.36s\tremaining: 2.77s\n",
            "23:\tlearn: 0.0799123\ttotal: 2.47s\tremaining: 2.68s\n",
            "24:\tlearn: 0.0782408\ttotal: 2.57s\tremaining: 2.57s\n",
            "25:\tlearn: 0.0757835\ttotal: 2.67s\tremaining: 2.46s\n",
            "26:\tlearn: 0.0746634\ttotal: 2.76s\tremaining: 2.35s\n",
            "27:\tlearn: 0.0735574\ttotal: 2.87s\tremaining: 2.25s\n",
            "28:\tlearn: 0.0719476\ttotal: 2.96s\tremaining: 2.14s\n",
            "29:\tlearn: 0.0704405\ttotal: 3.05s\tremaining: 2.03s\n",
            "30:\tlearn: 0.0693159\ttotal: 3.15s\tremaining: 1.93s\n",
            "31:\tlearn: 0.0683405\ttotal: 3.25s\tremaining: 1.83s\n",
            "32:\tlearn: 0.0671877\ttotal: 3.34s\tremaining: 1.72s\n",
            "33:\tlearn: 0.0660976\ttotal: 3.45s\tremaining: 1.62s\n",
            "34:\tlearn: 0.0653033\ttotal: 3.55s\tremaining: 1.52s\n",
            "35:\tlearn: 0.0643179\ttotal: 3.64s\tremaining: 1.42s\n",
            "36:\tlearn: 0.0635586\ttotal: 3.75s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0630176\ttotal: 3.84s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0619829\ttotal: 3.93s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0613468\ttotal: 4.03s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0603425\ttotal: 4.13s\tremaining: 907ms\n",
            "41:\tlearn: 0.0597202\ttotal: 4.22s\tremaining: 804ms\n",
            "42:\tlearn: 0.0587488\ttotal: 4.33s\tremaining: 705ms\n",
            "43:\tlearn: 0.0583536\ttotal: 4.44s\tremaining: 605ms\n",
            "44:\tlearn: 0.0579808\ttotal: 4.53s\tremaining: 503ms\n",
            "45:\tlearn: 0.0572812\ttotal: 4.63s\tremaining: 403ms\n",
            "46:\tlearn: 0.0565884\ttotal: 4.72s\tremaining: 301ms\n",
            "47:\tlearn: 0.0562096\ttotal: 4.83s\tremaining: 201ms\n",
            "48:\tlearn: 0.0557914\ttotal: 4.92s\tremaining: 100ms\n",
            "49:\tlearn: 0.0550850\ttotal: 5.01s\tremaining: 0us\n",
            "0:\tlearn: 0.0344960\ttotal: 139ms\tremaining: 6.8s\n",
            "1:\tlearn: 0.0321550\ttotal: 233ms\tremaining: 5.59s\n",
            "2:\tlearn: 0.0300417\ttotal: 328ms\tremaining: 5.14s\n",
            "3:\tlearn: 0.0280331\ttotal: 444ms\tremaining: 5.11s\n",
            "4:\tlearn: 0.0263562\ttotal: 538ms\tremaining: 4.84s\n",
            "5:\tlearn: 0.0245289\ttotal: 633ms\tremaining: 4.64s\n",
            "6:\tlearn: 0.0229247\ttotal: 734ms\tremaining: 4.51s\n",
            "7:\tlearn: 0.0214618\ttotal: 828ms\tremaining: 4.35s\n",
            "8:\tlearn: 0.0204680\ttotal: 923ms\tremaining: 4.21s\n",
            "9:\tlearn: 0.0198391\ttotal: 1.02s\tremaining: 4.1s\n",
            "10:\tlearn: 0.0191107\ttotal: 1.12s\tremaining: 3.97s\n",
            "11:\tlearn: 0.0181733\ttotal: 1.21s\tremaining: 3.84s\n",
            "12:\tlearn: 0.0174924\ttotal: 1.31s\tremaining: 3.73s\n",
            "13:\tlearn: 0.0168803\ttotal: 1.42s\tremaining: 3.64s\n",
            "14:\tlearn: 0.0161767\ttotal: 1.51s\tremaining: 3.52s\n",
            "15:\tlearn: 0.0154329\ttotal: 1.61s\tremaining: 3.43s\n",
            "16:\tlearn: 0.0150113\ttotal: 1.71s\tremaining: 3.31s\n",
            "17:\tlearn: 0.0144549\ttotal: 1.8s\tremaining: 3.2s\n",
            "18:\tlearn: 0.0140003\ttotal: 1.9s\tremaining: 3.11s\n",
            "19:\tlearn: 0.0136680\ttotal: 2s\tremaining: 3s\n",
            "20:\tlearn: 0.0132728\ttotal: 2.09s\tremaining: 2.89s\n",
            "21:\tlearn: 0.0129230\ttotal: 2.2s\tremaining: 2.8s\n",
            "22:\tlearn: 0.0126000\ttotal: 2.3s\tremaining: 2.7s\n",
            "23:\tlearn: 0.0122873\ttotal: 2.39s\tremaining: 2.59s\n",
            "24:\tlearn: 0.0119394\ttotal: 2.51s\tremaining: 2.51s\n",
            "25:\tlearn: 0.0116107\ttotal: 2.61s\tremaining: 2.41s\n",
            "26:\tlearn: 0.0113261\ttotal: 2.7s\tremaining: 2.3s\n",
            "27:\tlearn: 0.0110804\ttotal: 2.81s\tremaining: 2.21s\n",
            "28:\tlearn: 0.0108772\ttotal: 2.9s\tremaining: 2.1s\n",
            "29:\tlearn: 0.0106249\ttotal: 2.99s\tremaining: 2s\n",
            "30:\tlearn: 0.0104015\ttotal: 3.1s\tremaining: 1.9s\n",
            "31:\tlearn: 0.0102718\ttotal: 3.19s\tremaining: 1.8s\n",
            "32:\tlearn: 0.0101096\ttotal: 3.29s\tremaining: 1.69s\n",
            "33:\tlearn: 0.0098361\ttotal: 3.39s\tremaining: 1.59s\n",
            "34:\tlearn: 0.0096990\ttotal: 3.5s\tremaining: 1.5s\n",
            "35:\tlearn: 0.0094681\ttotal: 3.59s\tremaining: 1.4s\n",
            "36:\tlearn: 0.0093185\ttotal: 3.7s\tremaining: 1.3s\n",
            "37:\tlearn: 0.0090651\ttotal: 3.81s\tremaining: 1.2s\n",
            "38:\tlearn: 0.0088746\ttotal: 3.91s\tremaining: 1.1s\n",
            "39:\tlearn: 0.0086189\ttotal: 4s\tremaining: 1s\n",
            "40:\tlearn: 0.0085045\ttotal: 4.1s\tremaining: 901ms\n",
            "41:\tlearn: 0.0083678\ttotal: 4.2s\tremaining: 799ms\n",
            "42:\tlearn: 0.0082568\ttotal: 4.29s\tremaining: 699ms\n",
            "43:\tlearn: 0.0082086\ttotal: 4.4s\tremaining: 600ms\n",
            "44:\tlearn: 0.0081104\ttotal: 4.5s\tremaining: 500ms\n",
            "45:\tlearn: 0.0079863\ttotal: 4.6s\tremaining: 400ms\n",
            "46:\tlearn: 0.0078707\ttotal: 4.7s\tremaining: 300ms\n",
            "47:\tlearn: 0.0077634\ttotal: 4.79s\tremaining: 200ms\n",
            "48:\tlearn: 0.0076733\ttotal: 4.9s\tremaining: 100ms\n",
            "49:\tlearn: 0.0075559\ttotal: 4.99s\tremaining: 0us\n",
            "0:\tlearn: 0.0172643\ttotal: 147ms\tremaining: 7.22s\n",
            "1:\tlearn: 0.0167002\ttotal: 241ms\tremaining: 5.78s\n",
            "2:\tlearn: 0.0159745\ttotal: 334ms\tremaining: 5.24s\n",
            "3:\tlearn: 0.0154714\ttotal: 439ms\tremaining: 5.05s\n",
            "4:\tlearn: 0.0148627\ttotal: 545ms\tremaining: 4.9s\n",
            "5:\tlearn: 0.0140404\ttotal: 639ms\tremaining: 4.68s\n",
            "6:\tlearn: 0.0137588\ttotal: 745ms\tremaining: 4.58s\n",
            "7:\tlearn: 0.0135372\ttotal: 839ms\tremaining: 4.4s\n",
            "8:\tlearn: 0.0132375\ttotal: 940ms\tremaining: 4.28s\n",
            "9:\tlearn: 0.0131678\ttotal: 1.03s\tremaining: 4.13s\n",
            "10:\tlearn: 0.0129759\ttotal: 1.14s\tremaining: 4.04s\n",
            "11:\tlearn: 0.0127770\ttotal: 1.23s\tremaining: 3.9s\n",
            "12:\tlearn: 0.0123606\ttotal: 1.32s\tremaining: 3.77s\n",
            "13:\tlearn: 0.0120388\ttotal: 1.43s\tremaining: 3.67s\n",
            "14:\tlearn: 0.0118320\ttotal: 1.53s\tremaining: 3.58s\n",
            "15:\tlearn: 0.0115784\ttotal: 1.63s\tremaining: 3.47s\n",
            "16:\tlearn: 0.0112058\ttotal: 1.74s\tremaining: 3.37s\n",
            "17:\tlearn: 0.0106837\ttotal: 1.83s\tremaining: 3.26s\n",
            "18:\tlearn: 0.0104816\ttotal: 1.94s\tremaining: 3.17s\n",
            "19:\tlearn: 0.0103056\ttotal: 2.03s\tremaining: 3.05s\n",
            "20:\tlearn: 0.0096784\ttotal: 2.13s\tremaining: 2.94s\n",
            "21:\tlearn: 0.0095263\ttotal: 2.23s\tremaining: 2.84s\n",
            "22:\tlearn: 0.0090980\ttotal: 2.33s\tremaining: 2.73s\n",
            "23:\tlearn: 0.0086321\ttotal: 2.42s\tremaining: 2.62s\n",
            "24:\tlearn: 0.0085321\ttotal: 2.54s\tremaining: 2.54s\n",
            "25:\tlearn: 0.0083832\ttotal: 2.64s\tremaining: 2.44s\n",
            "26:\tlearn: 0.0082504\ttotal: 2.74s\tremaining: 2.33s\n",
            "27:\tlearn: 0.0080190\ttotal: 2.85s\tremaining: 2.23s\n",
            "28:\tlearn: 0.0078995\ttotal: 2.94s\tremaining: 2.13s\n",
            "29:\tlearn: 0.0075887\ttotal: 3.03s\tremaining: 2.02s\n",
            "30:\tlearn: 0.0075570\ttotal: 3.14s\tremaining: 1.92s\n",
            "31:\tlearn: 0.0075105\ttotal: 3.24s\tremaining: 1.82s\n",
            "32:\tlearn: 0.0074170\ttotal: 3.33s\tremaining: 1.72s\n",
            "33:\tlearn: 0.0072136\ttotal: 3.43s\tremaining: 1.61s\n",
            "34:\tlearn: 0.0071527\ttotal: 3.54s\tremaining: 1.51s\n",
            "35:\tlearn: 0.0070633\ttotal: 3.64s\tremaining: 1.42s\n",
            "36:\tlearn: 0.0066728\ttotal: 3.74s\tremaining: 1.31s\n",
            "37:\tlearn: 0.0065971\ttotal: 3.83s\tremaining: 1.21s\n",
            "38:\tlearn: 0.0064171\ttotal: 3.94s\tremaining: 1.11s\n",
            "39:\tlearn: 0.0061129\ttotal: 4.03s\tremaining: 1.01s\n",
            "40:\tlearn: 0.0061028\ttotal: 4.13s\tremaining: 906ms\n",
            "41:\tlearn: 0.0060774\ttotal: 4.24s\tremaining: 807ms\n",
            "42:\tlearn: 0.0059918\ttotal: 4.33s\tremaining: 705ms\n",
            "43:\tlearn: 0.0059036\ttotal: 4.42s\tremaining: 603ms\n",
            "44:\tlearn: 0.0057637\ttotal: 4.53s\tremaining: 504ms\n",
            "45:\tlearn: 0.0056992\ttotal: 4.64s\tremaining: 403ms\n",
            "46:\tlearn: 0.0056735\ttotal: 4.74s\tremaining: 302ms\n",
            "47:\tlearn: 0.0054334\ttotal: 4.84s\tremaining: 202ms\n",
            "48:\tlearn: 0.0053486\ttotal: 4.95s\tremaining: 101ms\n",
            "49:\tlearn: 0.0052945\ttotal: 5.05s\tremaining: 0us\n",
            "0:\tlearn: 0.0417391\ttotal: 151ms\tremaining: 7.41s\n",
            "1:\tlearn: 0.0386016\ttotal: 262ms\tremaining: 6.3s\n",
            "2:\tlearn: 0.0356459\ttotal: 366ms\tremaining: 5.74s\n",
            "3:\tlearn: 0.0331151\ttotal: 470ms\tremaining: 5.4s\n",
            "4:\tlearn: 0.0306579\ttotal: 575ms\tremaining: 5.17s\n",
            "5:\tlearn: 0.0287244\ttotal: 672ms\tremaining: 4.93s\n",
            "6:\tlearn: 0.0270329\ttotal: 767ms\tremaining: 4.71s\n",
            "7:\tlearn: 0.0253414\ttotal: 870ms\tremaining: 4.57s\n",
            "8:\tlearn: 0.0236623\ttotal: 965ms\tremaining: 4.4s\n",
            "9:\tlearn: 0.0224815\ttotal: 1.06s\tremaining: 4.24s\n",
            "10:\tlearn: 0.0214237\ttotal: 1.16s\tremaining: 4.11s\n",
            "11:\tlearn: 0.0205334\ttotal: 1.26s\tremaining: 3.98s\n",
            "12:\tlearn: 0.0196351\ttotal: 1.35s\tremaining: 3.84s\n",
            "13:\tlearn: 0.0186251\ttotal: 1.46s\tremaining: 3.75s\n",
            "14:\tlearn: 0.0179210\ttotal: 1.57s\tremaining: 3.67s\n",
            "15:\tlearn: 0.0173507\ttotal: 1.68s\tremaining: 3.56s\n",
            "16:\tlearn: 0.0167207\ttotal: 1.78s\tremaining: 3.46s\n",
            "17:\tlearn: 0.0159220\ttotal: 1.88s\tremaining: 3.33s\n",
            "18:\tlearn: 0.0155126\ttotal: 1.99s\tremaining: 3.25s\n",
            "19:\tlearn: 0.0149056\ttotal: 2.09s\tremaining: 3.14s\n",
            "20:\tlearn: 0.0146491\ttotal: 2.19s\tremaining: 3.02s\n",
            "21:\tlearn: 0.0142224\ttotal: 2.29s\tremaining: 2.92s\n",
            "22:\tlearn: 0.0136953\ttotal: 2.39s\tremaining: 2.8s\n",
            "23:\tlearn: 0.0133828\ttotal: 2.49s\tremaining: 2.69s\n",
            "24:\tlearn: 0.0130182\ttotal: 2.6s\tremaining: 2.6s\n",
            "25:\tlearn: 0.0125300\ttotal: 2.7s\tremaining: 2.5s\n",
            "26:\tlearn: 0.0122285\ttotal: 2.8s\tremaining: 2.38s\n",
            "27:\tlearn: 0.0120110\ttotal: 2.9s\tremaining: 2.28s\n",
            "28:\tlearn: 0.0117474\ttotal: 3s\tremaining: 2.17s\n",
            "29:\tlearn: 0.0115408\ttotal: 3.09s\tremaining: 2.06s\n",
            "30:\tlearn: 0.0112295\ttotal: 3.21s\tremaining: 1.97s\n",
            "31:\tlearn: 0.0108980\ttotal: 3.3s\tremaining: 1.86s\n",
            "32:\tlearn: 0.0105320\ttotal: 3.4s\tremaining: 1.75s\n",
            "33:\tlearn: 0.0102802\ttotal: 3.5s\tremaining: 1.65s\n",
            "34:\tlearn: 0.0099886\ttotal: 3.61s\tremaining: 1.55s\n",
            "35:\tlearn: 0.0097568\ttotal: 3.71s\tremaining: 1.44s\n",
            "36:\tlearn: 0.0095449\ttotal: 3.81s\tremaining: 1.34s\n",
            "37:\tlearn: 0.0093741\ttotal: 3.91s\tremaining: 1.23s\n",
            "38:\tlearn: 0.0092567\ttotal: 4.01s\tremaining: 1.13s\n",
            "39:\tlearn: 0.0090588\ttotal: 4.11s\tremaining: 1.03s\n",
            "40:\tlearn: 0.0088793\ttotal: 4.21s\tremaining: 924ms\n",
            "41:\tlearn: 0.0087671\ttotal: 4.31s\tremaining: 822ms\n",
            "42:\tlearn: 0.0086077\ttotal: 4.41s\tremaining: 718ms\n",
            "43:\tlearn: 0.0084540\ttotal: 4.51s\tremaining: 615ms\n",
            "44:\tlearn: 0.0082948\ttotal: 4.63s\tremaining: 515ms\n",
            "45:\tlearn: 0.0082207\ttotal: 4.73s\tremaining: 411ms\n",
            "46:\tlearn: 0.0080914\ttotal: 4.82s\tremaining: 308ms\n",
            "47:\tlearn: 0.0079674\ttotal: 4.93s\tremaining: 205ms\n",
            "48:\tlearn: 0.0078819\ttotal: 5.03s\tremaining: 103ms\n",
            "49:\tlearn: 0.0076804\ttotal: 5.12s\tremaining: 0us\n",
            "0:\tlearn: 0.2268955\ttotal: 154ms\tremaining: 7.53s\n",
            "1:\tlearn: 0.2104676\ttotal: 261ms\tremaining: 6.26s\n",
            "2:\tlearn: 0.1956113\ttotal: 373ms\tremaining: 5.84s\n",
            "3:\tlearn: 0.1821721\ttotal: 488ms\tremaining: 5.61s\n",
            "4:\tlearn: 0.1714828\ttotal: 600ms\tremaining: 5.39s\n",
            "5:\tlearn: 0.1603341\ttotal: 704ms\tremaining: 5.16s\n",
            "6:\tlearn: 0.1498130\ttotal: 819ms\tremaining: 5.03s\n",
            "7:\tlearn: 0.1405715\ttotal: 926ms\tremaining: 4.86s\n",
            "8:\tlearn: 0.1336639\ttotal: 1.04s\tremaining: 4.73s\n",
            "9:\tlearn: 0.1277792\ttotal: 1.15s\tremaining: 4.58s\n",
            "10:\tlearn: 0.1220748\ttotal: 1.26s\tremaining: 4.47s\n",
            "11:\tlearn: 0.1174899\ttotal: 1.36s\tremaining: 4.32s\n",
            "12:\tlearn: 0.1136585\ttotal: 1.49s\tremaining: 4.24s\n",
            "13:\tlearn: 0.1094365\ttotal: 1.6s\tremaining: 4.12s\n",
            "14:\tlearn: 0.1060116\ttotal: 1.73s\tremaining: 4.03s\n",
            "15:\tlearn: 0.1020082\ttotal: 1.83s\tremaining: 3.89s\n",
            "16:\tlearn: 0.0991286\ttotal: 1.94s\tremaining: 3.77s\n",
            "17:\tlearn: 0.0960951\ttotal: 2.05s\tremaining: 3.65s\n",
            "18:\tlearn: 0.0934588\ttotal: 2.17s\tremaining: 3.54s\n",
            "19:\tlearn: 0.0915307\ttotal: 2.27s\tremaining: 3.41s\n",
            "20:\tlearn: 0.0900992\ttotal: 2.39s\tremaining: 3.3s\n",
            "21:\tlearn: 0.0877080\ttotal: 2.5s\tremaining: 3.19s\n",
            "22:\tlearn: 0.0856568\ttotal: 2.62s\tremaining: 3.07s\n",
            "23:\tlearn: 0.0844284\ttotal: 2.72s\tremaining: 2.95s\n",
            "24:\tlearn: 0.0833254\ttotal: 2.83s\tremaining: 2.83s\n",
            "25:\tlearn: 0.0821297\ttotal: 2.94s\tremaining: 2.72s\n",
            "26:\tlearn: 0.0807129\ttotal: 3.06s\tremaining: 2.6s\n",
            "27:\tlearn: 0.0799635\ttotal: 3.16s\tremaining: 2.48s\n",
            "28:\tlearn: 0.0791091\ttotal: 3.27s\tremaining: 2.37s\n",
            "29:\tlearn: 0.0776963\ttotal: 3.38s\tremaining: 2.25s\n",
            "30:\tlearn: 0.0765855\ttotal: 3.5s\tremaining: 2.15s\n",
            "31:\tlearn: 0.0756082\ttotal: 3.61s\tremaining: 2.03s\n",
            "32:\tlearn: 0.0741785\ttotal: 3.71s\tremaining: 1.91s\n",
            "33:\tlearn: 0.0733371\ttotal: 3.83s\tremaining: 1.8s\n",
            "34:\tlearn: 0.0727385\ttotal: 3.95s\tremaining: 1.69s\n",
            "35:\tlearn: 0.0719390\ttotal: 4.06s\tremaining: 1.58s\n",
            "36:\tlearn: 0.0715442\ttotal: 4.17s\tremaining: 1.47s\n",
            "37:\tlearn: 0.0705053\ttotal: 4.28s\tremaining: 1.35s\n",
            "38:\tlearn: 0.0698670\ttotal: 4.41s\tremaining: 1.24s\n",
            "39:\tlearn: 0.0691990\ttotal: 4.53s\tremaining: 1.13s\n",
            "40:\tlearn: 0.0687658\ttotal: 4.64s\tremaining: 1.02s\n",
            "41:\tlearn: 0.0683537\ttotal: 4.75s\tremaining: 906ms\n",
            "42:\tlearn: 0.0674362\ttotal: 4.87s\tremaining: 793ms\n",
            "43:\tlearn: 0.0666068\ttotal: 4.98s\tremaining: 679ms\n",
            "44:\tlearn: 0.0661815\ttotal: 5.09s\tremaining: 566ms\n",
            "45:\tlearn: 0.0659431\ttotal: 5.2s\tremaining: 452ms\n",
            "46:\tlearn: 0.0654038\ttotal: 5.32s\tremaining: 339ms\n",
            "47:\tlearn: 0.0648200\ttotal: 5.42s\tremaining: 226ms\n",
            "48:\tlearn: 0.0641735\ttotal: 5.55s\tremaining: 113ms\n",
            "49:\tlearn: 0.0635951\ttotal: 5.66s\tremaining: 0us\n",
            "0:\tlearn: 0.0381765\ttotal: 164ms\tremaining: 8.01s\n",
            "1:\tlearn: 0.0354953\ttotal: 272ms\tremaining: 6.53s\n",
            "2:\tlearn: 0.0329687\ttotal: 386ms\tremaining: 6.05s\n",
            "3:\tlearn: 0.0306886\ttotal: 494ms\tremaining: 5.68s\n",
            "4:\tlearn: 0.0286821\ttotal: 609ms\tremaining: 5.48s\n",
            "5:\tlearn: 0.0268251\ttotal: 725ms\tremaining: 5.32s\n",
            "6:\tlearn: 0.0251163\ttotal: 848ms\tremaining: 5.21s\n",
            "7:\tlearn: 0.0237349\ttotal: 954ms\tremaining: 5.01s\n",
            "8:\tlearn: 0.0229442\ttotal: 1.07s\tremaining: 4.87s\n",
            "9:\tlearn: 0.0220689\ttotal: 1.18s\tremaining: 4.7s\n",
            "10:\tlearn: 0.0210404\ttotal: 1.29s\tremaining: 4.58s\n",
            "11:\tlearn: 0.0201147\ttotal: 1.4s\tremaining: 4.42s\n",
            "12:\tlearn: 0.0193530\ttotal: 1.51s\tremaining: 4.3s\n",
            "13:\tlearn: 0.0187205\ttotal: 1.62s\tremaining: 4.16s\n",
            "14:\tlearn: 0.0178947\ttotal: 1.73s\tremaining: 4.05s\n",
            "15:\tlearn: 0.0171956\ttotal: 1.84s\tremaining: 3.91s\n",
            "16:\tlearn: 0.0166622\ttotal: 1.96s\tremaining: 3.81s\n",
            "17:\tlearn: 0.0160287\ttotal: 2.07s\tremaining: 3.68s\n",
            "18:\tlearn: 0.0154899\ttotal: 2.18s\tremaining: 3.56s\n",
            "19:\tlearn: 0.0150593\ttotal: 2.29s\tremaining: 3.43s\n",
            "20:\tlearn: 0.0146248\ttotal: 2.41s\tremaining: 3.33s\n",
            "21:\tlearn: 0.0142975\ttotal: 2.52s\tremaining: 3.2s\n",
            "22:\tlearn: 0.0139092\ttotal: 2.63s\tremaining: 3.09s\n",
            "23:\tlearn: 0.0135247\ttotal: 2.73s\tremaining: 2.96s\n",
            "24:\tlearn: 0.0132991\ttotal: 2.85s\tremaining: 2.85s\n",
            "25:\tlearn: 0.0130539\ttotal: 2.96s\tremaining: 2.73s\n",
            "26:\tlearn: 0.0127551\ttotal: 3.08s\tremaining: 2.63s\n",
            "27:\tlearn: 0.0125042\ttotal: 3.21s\tremaining: 2.52s\n",
            "28:\tlearn: 0.0122791\ttotal: 3.33s\tremaining: 2.41s\n",
            "29:\tlearn: 0.0119599\ttotal: 3.44s\tremaining: 2.29s\n",
            "30:\tlearn: 0.0117632\ttotal: 3.56s\tremaining: 2.18s\n",
            "31:\tlearn: 0.0116492\ttotal: 3.69s\tremaining: 2.08s\n",
            "32:\tlearn: 0.0115110\ttotal: 3.82s\tremaining: 1.97s\n",
            "33:\tlearn: 0.0113766\ttotal: 3.95s\tremaining: 1.86s\n",
            "34:\tlearn: 0.0112351\ttotal: 4.06s\tremaining: 1.74s\n",
            "35:\tlearn: 0.0111050\ttotal: 4.17s\tremaining: 1.62s\n",
            "36:\tlearn: 0.0109556\ttotal: 4.29s\tremaining: 1.51s\n",
            "37:\tlearn: 0.0108316\ttotal: 4.4s\tremaining: 1.39s\n",
            "38:\tlearn: 0.0106528\ttotal: 4.51s\tremaining: 1.27s\n",
            "39:\tlearn: 0.0105432\ttotal: 4.62s\tremaining: 1.15s\n",
            "40:\tlearn: 0.0104385\ttotal: 4.73s\tremaining: 1.04s\n",
            "41:\tlearn: 0.0103404\ttotal: 4.83s\tremaining: 921ms\n",
            "42:\tlearn: 0.0101952\ttotal: 4.95s\tremaining: 807ms\n",
            "43:\tlearn: 0.0100662\ttotal: 5.07s\tremaining: 691ms\n",
            "44:\tlearn: 0.0099667\ttotal: 5.18s\tremaining: 576ms\n",
            "45:\tlearn: 0.0098083\ttotal: 5.29s\tremaining: 460ms\n",
            "46:\tlearn: 0.0097049\ttotal: 5.4s\tremaining: 345ms\n",
            "47:\tlearn: 0.0095717\ttotal: 5.51s\tremaining: 230ms\n",
            "48:\tlearn: 0.0094953\ttotal: 5.63s\tremaining: 115ms\n",
            "49:\tlearn: 0.0093614\ttotal: 5.74s\tremaining: 0us\n",
            "0:\tlearn: 0.0160091\ttotal: 177ms\tremaining: 8.65s\n",
            "1:\tlearn: 0.0154489\ttotal: 285ms\tremaining: 6.85s\n",
            "2:\tlearn: 0.0147596\ttotal: 405ms\tremaining: 6.35s\n",
            "3:\tlearn: 0.0141350\ttotal: 513ms\tremaining: 5.9s\n",
            "4:\tlearn: 0.0131072\ttotal: 624ms\tremaining: 5.62s\n",
            "5:\tlearn: 0.0126192\ttotal: 728ms\tremaining: 5.34s\n",
            "6:\tlearn: 0.0121453\ttotal: 840ms\tremaining: 5.16s\n",
            "7:\tlearn: 0.0116113\ttotal: 952ms\tremaining: 5s\n",
            "8:\tlearn: 0.0113778\ttotal: 1.07s\tremaining: 4.87s\n",
            "9:\tlearn: 0.0110194\ttotal: 1.17s\tremaining: 4.69s\n",
            "10:\tlearn: 0.0106365\ttotal: 1.3s\tremaining: 4.61s\n",
            "11:\tlearn: 0.0104219\ttotal: 1.4s\tremaining: 4.44s\n",
            "12:\tlearn: 0.0100532\ttotal: 1.51s\tremaining: 4.31s\n",
            "13:\tlearn: 0.0095342\ttotal: 1.62s\tremaining: 4.17s\n",
            "14:\tlearn: 0.0093226\ttotal: 1.74s\tremaining: 4.06s\n",
            "15:\tlearn: 0.0091863\ttotal: 1.84s\tremaining: 3.92s\n",
            "16:\tlearn: 0.0088661\ttotal: 1.95s\tremaining: 3.79s\n",
            "17:\tlearn: 0.0085034\ttotal: 2.06s\tremaining: 3.66s\n",
            "18:\tlearn: 0.0082592\ttotal: 2.18s\tremaining: 3.55s\n",
            "19:\tlearn: 0.0079681\ttotal: 2.29s\tremaining: 3.44s\n",
            "20:\tlearn: 0.0078184\ttotal: 2.4s\tremaining: 3.32s\n",
            "21:\tlearn: 0.0076472\ttotal: 2.51s\tremaining: 3.2s\n",
            "22:\tlearn: 0.0073821\ttotal: 2.63s\tremaining: 3.09s\n",
            "23:\tlearn: 0.0071870\ttotal: 2.74s\tremaining: 2.97s\n",
            "24:\tlearn: 0.0071064\ttotal: 2.85s\tremaining: 2.85s\n",
            "25:\tlearn: 0.0069870\ttotal: 2.95s\tremaining: 2.73s\n",
            "26:\tlearn: 0.0069003\ttotal: 3.07s\tremaining: 2.61s\n",
            "27:\tlearn: 0.0067631\ttotal: 3.18s\tremaining: 2.5s\n",
            "28:\tlearn: 0.0067072\ttotal: 3.3s\tremaining: 2.39s\n",
            "29:\tlearn: 0.0066684\ttotal: 3.4s\tremaining: 2.27s\n",
            "30:\tlearn: 0.0066364\ttotal: 3.52s\tremaining: 2.16s\n",
            "31:\tlearn: 0.0065824\ttotal: 3.63s\tremaining: 2.04s\n",
            "32:\tlearn: 0.0065538\ttotal: 3.75s\tremaining: 1.93s\n",
            "33:\tlearn: 0.0064247\ttotal: 3.85s\tremaining: 1.81s\n",
            "34:\tlearn: 0.0062903\ttotal: 3.96s\tremaining: 1.7s\n",
            "35:\tlearn: 0.0062337\ttotal: 4.07s\tremaining: 1.58s\n",
            "36:\tlearn: 0.0060309\ttotal: 4.18s\tremaining: 1.47s\n",
            "37:\tlearn: 0.0059417\ttotal: 4.31s\tremaining: 1.36s\n",
            "38:\tlearn: 0.0059018\ttotal: 4.43s\tremaining: 1.25s\n",
            "39:\tlearn: 0.0058747\ttotal: 4.54s\tremaining: 1.13s\n",
            "40:\tlearn: 0.0058636\ttotal: 4.65s\tremaining: 1.02s\n",
            "41:\tlearn: 0.0057282\ttotal: 4.76s\tremaining: 907ms\n",
            "42:\tlearn: 0.0056980\ttotal: 4.87s\tremaining: 793ms\n",
            "43:\tlearn: 0.0056226\ttotal: 4.98s\tremaining: 679ms\n",
            "44:\tlearn: 0.0055423\ttotal: 5.1s\tremaining: 566ms\n",
            "45:\tlearn: 0.0055169\ttotal: 5.2s\tremaining: 452ms\n",
            "46:\tlearn: 0.0053880\ttotal: 5.33s\tremaining: 340ms\n",
            "47:\tlearn: 0.0053043\ttotal: 5.44s\tremaining: 227ms\n",
            "48:\tlearn: 0.0051796\ttotal: 5.55s\tremaining: 113ms\n",
            "49:\tlearn: 0.0051364\ttotal: 5.66s\tremaining: 0us\n",
            "0:\tlearn: 0.0401227\ttotal: 166ms\tremaining: 8.16s\n",
            "1:\tlearn: 0.0367320\ttotal: 272ms\tremaining: 6.52s\n",
            "2:\tlearn: 0.0339586\ttotal: 381ms\tremaining: 5.98s\n",
            "3:\tlearn: 0.0313455\ttotal: 486ms\tremaining: 5.58s\n",
            "4:\tlearn: 0.0292361\ttotal: 610ms\tremaining: 5.49s\n",
            "5:\tlearn: 0.0270779\ttotal: 719ms\tremaining: 5.27s\n",
            "6:\tlearn: 0.0254774\ttotal: 833ms\tremaining: 5.12s\n",
            "7:\tlearn: 0.0239300\ttotal: 938ms\tremaining: 4.92s\n",
            "8:\tlearn: 0.0224933\ttotal: 1.05s\tremaining: 4.78s\n",
            "9:\tlearn: 0.0214057\ttotal: 1.16s\tremaining: 4.64s\n",
            "10:\tlearn: 0.0203675\ttotal: 1.27s\tremaining: 4.5s\n",
            "11:\tlearn: 0.0194745\ttotal: 1.37s\tremaining: 4.34s\n",
            "12:\tlearn: 0.0185555\ttotal: 1.48s\tremaining: 4.21s\n",
            "13:\tlearn: 0.0179276\ttotal: 1.59s\tremaining: 4.09s\n",
            "14:\tlearn: 0.0173025\ttotal: 1.72s\tremaining: 4.01s\n",
            "15:\tlearn: 0.0167885\ttotal: 1.82s\tremaining: 3.87s\n",
            "16:\tlearn: 0.0161767\ttotal: 1.94s\tremaining: 3.76s\n",
            "17:\tlearn: 0.0156502\ttotal: 2.04s\tremaining: 3.63s\n",
            "18:\tlearn: 0.0151090\ttotal: 2.16s\tremaining: 3.52s\n",
            "19:\tlearn: 0.0146820\ttotal: 2.26s\tremaining: 3.39s\n",
            "20:\tlearn: 0.0143684\ttotal: 2.37s\tremaining: 3.28s\n",
            "21:\tlearn: 0.0139714\ttotal: 2.48s\tremaining: 3.16s\n",
            "22:\tlearn: 0.0135818\ttotal: 2.6s\tremaining: 3.05s\n",
            "23:\tlearn: 0.0132131\ttotal: 2.71s\tremaining: 2.94s\n",
            "24:\tlearn: 0.0130365\ttotal: 2.83s\tremaining: 2.83s\n",
            "25:\tlearn: 0.0127691\ttotal: 2.94s\tremaining: 2.71s\n",
            "26:\tlearn: 0.0124731\ttotal: 3.06s\tremaining: 2.6s\n",
            "27:\tlearn: 0.0122719\ttotal: 3.16s\tremaining: 2.48s\n",
            "28:\tlearn: 0.0120952\ttotal: 3.27s\tremaining: 2.37s\n",
            "29:\tlearn: 0.0118996\ttotal: 3.38s\tremaining: 2.25s\n",
            "30:\tlearn: 0.0115864\ttotal: 3.49s\tremaining: 2.14s\n",
            "31:\tlearn: 0.0113498\ttotal: 3.6s\tremaining: 2.02s\n",
            "32:\tlearn: 0.0111680\ttotal: 3.73s\tremaining: 1.92s\n",
            "33:\tlearn: 0.0110011\ttotal: 3.83s\tremaining: 1.8s\n",
            "34:\tlearn: 0.0108725\ttotal: 3.94s\tremaining: 1.69s\n",
            "35:\tlearn: 0.0107322\ttotal: 4.05s\tremaining: 1.58s\n",
            "36:\tlearn: 0.0104849\ttotal: 4.17s\tremaining: 1.46s\n",
            "37:\tlearn: 0.0102931\ttotal: 4.27s\tremaining: 1.35s\n",
            "38:\tlearn: 0.0101851\ttotal: 4.38s\tremaining: 1.24s\n",
            "39:\tlearn: 0.0100150\ttotal: 4.49s\tremaining: 1.12s\n",
            "40:\tlearn: 0.0099208\ttotal: 4.6s\tremaining: 1.01s\n",
            "41:\tlearn: 0.0097455\ttotal: 4.71s\tremaining: 897ms\n",
            "42:\tlearn: 0.0096455\ttotal: 4.83s\tremaining: 786ms\n",
            "43:\tlearn: 0.0095651\ttotal: 4.93s\tremaining: 672ms\n",
            "44:\tlearn: 0.0094189\ttotal: 5.04s\tremaining: 560ms\n",
            "45:\tlearn: 0.0093618\ttotal: 5.15s\tremaining: 448ms\n",
            "46:\tlearn: 0.0092574\ttotal: 5.26s\tremaining: 336ms\n",
            "47:\tlearn: 0.0091240\ttotal: 5.36s\tremaining: 224ms\n",
            "48:\tlearn: 0.0090595\ttotal: 5.47s\tremaining: 112ms\n",
            "49:\tlearn: 0.0089802\ttotal: 5.58s\tremaining: 0us\n",
            "0:\tlearn: 0.2401489\ttotal: 176ms\tremaining: 8.6s\n",
            "1:\tlearn: 0.2220591\ttotal: 288ms\tremaining: 6.91s\n",
            "2:\tlearn: 0.2078809\ttotal: 407ms\tremaining: 6.38s\n",
            "3:\tlearn: 0.1948578\ttotal: 516ms\tremaining: 5.94s\n",
            "4:\tlearn: 0.1832113\ttotal: 637ms\tremaining: 5.74s\n",
            "5:\tlearn: 0.1721633\ttotal: 748ms\tremaining: 5.49s\n",
            "6:\tlearn: 0.1632169\ttotal: 865ms\tremaining: 5.32s\n",
            "7:\tlearn: 0.1547849\ttotal: 976ms\tremaining: 5.12s\n",
            "8:\tlearn: 0.1466318\ttotal: 1.11s\tremaining: 5.06s\n",
            "9:\tlearn: 0.1402201\ttotal: 1.22s\tremaining: 4.88s\n",
            "10:\tlearn: 0.1339503\ttotal: 1.34s\tremaining: 4.76s\n",
            "11:\tlearn: 0.1292762\ttotal: 1.45s\tremaining: 4.6s\n",
            "12:\tlearn: 0.1242452\ttotal: 1.58s\tremaining: 4.49s\n",
            "13:\tlearn: 0.1197019\ttotal: 1.69s\tremaining: 4.36s\n",
            "14:\tlearn: 0.1158627\ttotal: 1.81s\tremaining: 4.23s\n",
            "15:\tlearn: 0.1121432\ttotal: 1.93s\tremaining: 4.1s\n",
            "16:\tlearn: 0.1091674\ttotal: 2.07s\tremaining: 4.02s\n",
            "17:\tlearn: 0.1057113\ttotal: 2.18s\tremaining: 3.88s\n",
            "18:\tlearn: 0.1030088\ttotal: 2.3s\tremaining: 3.75s\n",
            "19:\tlearn: 0.1005598\ttotal: 2.41s\tremaining: 3.61s\n",
            "20:\tlearn: 0.0989928\ttotal: 2.53s\tremaining: 3.49s\n",
            "21:\tlearn: 0.0976445\ttotal: 2.64s\tremaining: 3.36s\n",
            "22:\tlearn: 0.0953605\ttotal: 2.77s\tremaining: 3.25s\n",
            "23:\tlearn: 0.0936582\ttotal: 2.89s\tremaining: 3.14s\n",
            "24:\tlearn: 0.0924417\ttotal: 3.02s\tremaining: 3.02s\n",
            "25:\tlearn: 0.0904761\ttotal: 3.14s\tremaining: 2.9s\n",
            "26:\tlearn: 0.0887629\ttotal: 3.26s\tremaining: 2.78s\n",
            "27:\tlearn: 0.0876886\ttotal: 3.38s\tremaining: 2.65s\n",
            "28:\tlearn: 0.0862754\ttotal: 3.5s\tremaining: 2.53s\n",
            "29:\tlearn: 0.0849973\ttotal: 3.61s\tremaining: 2.4s\n",
            "30:\tlearn: 0.0840727\ttotal: 3.73s\tremaining: 2.29s\n",
            "31:\tlearn: 0.0835551\ttotal: 3.85s\tremaining: 2.16s\n",
            "32:\tlearn: 0.0824387\ttotal: 3.97s\tremaining: 2.04s\n",
            "33:\tlearn: 0.0813001\ttotal: 4.09s\tremaining: 1.92s\n",
            "34:\tlearn: 0.0807953\ttotal: 4.22s\tremaining: 1.81s\n",
            "35:\tlearn: 0.0799866\ttotal: 4.33s\tremaining: 1.68s\n",
            "36:\tlearn: 0.0790170\ttotal: 4.45s\tremaining: 1.56s\n",
            "37:\tlearn: 0.0784903\ttotal: 4.56s\tremaining: 1.44s\n",
            "38:\tlearn: 0.0779189\ttotal: 4.68s\tremaining: 1.32s\n",
            "39:\tlearn: 0.0773155\ttotal: 4.79s\tremaining: 1.2s\n",
            "40:\tlearn: 0.0766720\ttotal: 4.92s\tremaining: 1.08s\n",
            "41:\tlearn: 0.0758120\ttotal: 5.03s\tremaining: 958ms\n",
            "42:\tlearn: 0.0748388\ttotal: 5.16s\tremaining: 840ms\n",
            "43:\tlearn: 0.0739062\ttotal: 5.27s\tremaining: 718ms\n",
            "44:\tlearn: 0.0727718\ttotal: 5.39s\tremaining: 599ms\n",
            "45:\tlearn: 0.0722947\ttotal: 5.5s\tremaining: 478ms\n",
            "46:\tlearn: 0.0715095\ttotal: 5.61s\tremaining: 358ms\n",
            "47:\tlearn: 0.0709961\ttotal: 5.72s\tremaining: 238ms\n",
            "48:\tlearn: 0.0703920\ttotal: 5.84s\tremaining: 119ms\n",
            "49:\tlearn: 0.0698719\ttotal: 5.96s\tremaining: 0us\n",
            "0:\tlearn: 0.0371071\ttotal: 183ms\tremaining: 8.98s\n",
            "1:\tlearn: 0.0344683\ttotal: 294ms\tremaining: 7.06s\n",
            "2:\tlearn: 0.0320121\ttotal: 415ms\tremaining: 6.5s\n",
            "3:\tlearn: 0.0300042\ttotal: 524ms\tremaining: 6.03s\n",
            "4:\tlearn: 0.0278921\ttotal: 644ms\tremaining: 5.79s\n",
            "5:\tlearn: 0.0259982\ttotal: 758ms\tremaining: 5.56s\n",
            "6:\tlearn: 0.0243012\ttotal: 876ms\tremaining: 5.38s\n",
            "7:\tlearn: 0.0229443\ttotal: 991ms\tremaining: 5.2s\n",
            "8:\tlearn: 0.0217899\ttotal: 1.11s\tremaining: 5.08s\n",
            "9:\tlearn: 0.0209369\ttotal: 1.24s\tremaining: 4.95s\n",
            "10:\tlearn: 0.0199772\ttotal: 1.36s\tremaining: 4.81s\n",
            "11:\tlearn: 0.0188877\ttotal: 1.47s\tremaining: 4.65s\n",
            "12:\tlearn: 0.0180001\ttotal: 1.6s\tremaining: 4.54s\n",
            "13:\tlearn: 0.0172694\ttotal: 1.71s\tremaining: 4.39s\n",
            "14:\tlearn: 0.0166219\ttotal: 1.83s\tremaining: 4.27s\n",
            "15:\tlearn: 0.0161050\ttotal: 1.94s\tremaining: 4.12s\n",
            "16:\tlearn: 0.0156455\ttotal: 2.06s\tremaining: 3.99s\n",
            "17:\tlearn: 0.0151659\ttotal: 2.16s\tremaining: 3.85s\n",
            "18:\tlearn: 0.0147176\ttotal: 2.29s\tremaining: 3.74s\n",
            "19:\tlearn: 0.0144164\ttotal: 2.41s\tremaining: 3.61s\n",
            "20:\tlearn: 0.0140157\ttotal: 2.52s\tremaining: 3.49s\n",
            "21:\tlearn: 0.0136254\ttotal: 2.63s\tremaining: 3.35s\n",
            "22:\tlearn: 0.0132410\ttotal: 2.75s\tremaining: 3.23s\n",
            "23:\tlearn: 0.0130093\ttotal: 2.86s\tremaining: 3.1s\n",
            "24:\tlearn: 0.0126715\ttotal: 2.98s\tremaining: 2.98s\n",
            "25:\tlearn: 0.0124166\ttotal: 3.08s\tremaining: 2.85s\n",
            "26:\tlearn: 0.0121983\ttotal: 3.21s\tremaining: 2.73s\n",
            "27:\tlearn: 0.0119922\ttotal: 3.33s\tremaining: 2.62s\n",
            "28:\tlearn: 0.0117810\ttotal: 3.44s\tremaining: 2.49s\n",
            "29:\tlearn: 0.0115605\ttotal: 3.56s\tremaining: 2.37s\n",
            "30:\tlearn: 0.0113415\ttotal: 3.67s\tremaining: 2.25s\n",
            "31:\tlearn: 0.0111607\ttotal: 3.79s\tremaining: 2.13s\n",
            "32:\tlearn: 0.0110244\ttotal: 3.91s\tremaining: 2.01s\n",
            "33:\tlearn: 0.0108830\ttotal: 4.02s\tremaining: 1.89s\n",
            "34:\tlearn: 0.0107796\ttotal: 4.14s\tremaining: 1.77s\n",
            "35:\tlearn: 0.0106385\ttotal: 4.26s\tremaining: 1.66s\n",
            "36:\tlearn: 0.0105294\ttotal: 4.38s\tremaining: 1.54s\n",
            "37:\tlearn: 0.0104243\ttotal: 4.49s\tremaining: 1.42s\n",
            "38:\tlearn: 0.0103746\ttotal: 4.61s\tremaining: 1.3s\n",
            "39:\tlearn: 0.0102987\ttotal: 4.72s\tremaining: 1.18s\n",
            "40:\tlearn: 0.0101985\ttotal: 4.84s\tremaining: 1.06s\n",
            "41:\tlearn: 0.0101264\ttotal: 4.95s\tremaining: 943ms\n",
            "42:\tlearn: 0.0100411\ttotal: 5.07s\tremaining: 826ms\n",
            "43:\tlearn: 0.0099703\ttotal: 5.18s\tremaining: 707ms\n",
            "44:\tlearn: 0.0098669\ttotal: 5.31s\tremaining: 590ms\n",
            "45:\tlearn: 0.0098117\ttotal: 5.42s\tremaining: 471ms\n",
            "46:\tlearn: 0.0097466\ttotal: 5.54s\tremaining: 353ms\n",
            "47:\tlearn: 0.0096414\ttotal: 5.65s\tremaining: 235ms\n",
            "48:\tlearn: 0.0095728\ttotal: 5.76s\tremaining: 118ms\n",
            "49:\tlearn: 0.0094826\ttotal: 5.88s\tremaining: 0us\n",
            "0:\tlearn: 0.0169436\ttotal: 177ms\tremaining: 8.66s\n",
            "1:\tlearn: 0.0162529\ttotal: 287ms\tremaining: 6.9s\n",
            "2:\tlearn: 0.0154402\ttotal: 418ms\tremaining: 6.54s\n",
            "3:\tlearn: 0.0146163\ttotal: 535ms\tremaining: 6.15s\n",
            "4:\tlearn: 0.0137550\ttotal: 659ms\tremaining: 5.93s\n",
            "5:\tlearn: 0.0131811\ttotal: 770ms\tremaining: 5.64s\n",
            "6:\tlearn: 0.0127877\ttotal: 890ms\tremaining: 5.47s\n",
            "7:\tlearn: 0.0120784\ttotal: 1000ms\tremaining: 5.25s\n",
            "8:\tlearn: 0.0114685\ttotal: 1.12s\tremaining: 5.1s\n",
            "9:\tlearn: 0.0111124\ttotal: 1.23s\tremaining: 4.92s\n",
            "10:\tlearn: 0.0107345\ttotal: 1.35s\tremaining: 4.79s\n",
            "11:\tlearn: 0.0104783\ttotal: 1.47s\tremaining: 4.65s\n",
            "12:\tlearn: 0.0101305\ttotal: 1.59s\tremaining: 4.52s\n",
            "13:\tlearn: 0.0097422\ttotal: 1.7s\tremaining: 4.37s\n",
            "14:\tlearn: 0.0094242\ttotal: 1.82s\tremaining: 4.24s\n",
            "15:\tlearn: 0.0093152\ttotal: 1.93s\tremaining: 4.1s\n",
            "16:\tlearn: 0.0090905\ttotal: 2.04s\tremaining: 3.97s\n",
            "17:\tlearn: 0.0088718\ttotal: 2.16s\tremaining: 3.83s\n",
            "18:\tlearn: 0.0086146\ttotal: 2.27s\tremaining: 3.71s\n",
            "19:\tlearn: 0.0085471\ttotal: 2.4s\tremaining: 3.59s\n",
            "20:\tlearn: 0.0083626\ttotal: 2.51s\tremaining: 3.47s\n",
            "21:\tlearn: 0.0080860\ttotal: 2.62s\tremaining: 3.33s\n",
            "22:\tlearn: 0.0077792\ttotal: 2.74s\tremaining: 3.21s\n",
            "23:\tlearn: 0.0076030\ttotal: 2.85s\tremaining: 3.09s\n",
            "24:\tlearn: 0.0075569\ttotal: 2.97s\tremaining: 2.97s\n",
            "25:\tlearn: 0.0074341\ttotal: 3.08s\tremaining: 2.84s\n",
            "26:\tlearn: 0.0073313\ttotal: 3.2s\tremaining: 2.73s\n",
            "27:\tlearn: 0.0071841\ttotal: 3.31s\tremaining: 2.6s\n",
            "28:\tlearn: 0.0070672\ttotal: 3.44s\tremaining: 2.49s\n",
            "29:\tlearn: 0.0070339\ttotal: 3.55s\tremaining: 2.37s\n",
            "30:\tlearn: 0.0070152\ttotal: 3.66s\tremaining: 2.24s\n",
            "31:\tlearn: 0.0069847\ttotal: 3.77s\tremaining: 2.12s\n",
            "32:\tlearn: 0.0069595\ttotal: 3.89s\tremaining: 2s\n",
            "33:\tlearn: 0.0069259\ttotal: 4s\tremaining: 1.88s\n",
            "34:\tlearn: 0.0068248\ttotal: 4.12s\tremaining: 1.77s\n",
            "35:\tlearn: 0.0067950\ttotal: 4.24s\tremaining: 1.65s\n",
            "36:\tlearn: 0.0066527\ttotal: 4.36s\tremaining: 1.53s\n",
            "37:\tlearn: 0.0066275\ttotal: 4.48s\tremaining: 1.42s\n",
            "38:\tlearn: 0.0065259\ttotal: 4.6s\tremaining: 1.3s\n",
            "39:\tlearn: 0.0064869\ttotal: 4.71s\tremaining: 1.18s\n",
            "40:\tlearn: 0.0063749\ttotal: 4.83s\tremaining: 1.06s\n",
            "41:\tlearn: 0.0063635\ttotal: 4.94s\tremaining: 942ms\n",
            "42:\tlearn: 0.0062834\ttotal: 5.07s\tremaining: 825ms\n",
            "43:\tlearn: 0.0061743\ttotal: 5.18s\tremaining: 706ms\n",
            "44:\tlearn: 0.0060994\ttotal: 5.3s\tremaining: 589ms\n",
            "45:\tlearn: 0.0060175\ttotal: 5.41s\tremaining: 470ms\n",
            "46:\tlearn: 0.0059996\ttotal: 5.53s\tremaining: 353ms\n",
            "47:\tlearn: 0.0059104\ttotal: 5.65s\tremaining: 235ms\n",
            "48:\tlearn: 0.0058209\ttotal: 5.77s\tremaining: 118ms\n",
            "49:\tlearn: 0.0057966\ttotal: 5.88s\tremaining: 0us\n",
            "0:\tlearn: 0.0391256\ttotal: 169ms\tremaining: 8.3s\n",
            "1:\tlearn: 0.0363107\ttotal: 282ms\tremaining: 6.77s\n",
            "2:\tlearn: 0.0335693\ttotal: 405ms\tremaining: 6.35s\n",
            "3:\tlearn: 0.0310701\ttotal: 519ms\tremaining: 5.96s\n",
            "4:\tlearn: 0.0287844\ttotal: 651ms\tremaining: 5.86s\n",
            "5:\tlearn: 0.0267482\ttotal: 764ms\tremaining: 5.6s\n",
            "6:\tlearn: 0.0249324\ttotal: 883ms\tremaining: 5.43s\n",
            "7:\tlearn: 0.0233651\ttotal: 995ms\tremaining: 5.22s\n",
            "8:\tlearn: 0.0219404\ttotal: 1.12s\tremaining: 5.09s\n",
            "9:\tlearn: 0.0209733\ttotal: 1.23s\tremaining: 4.91s\n",
            "10:\tlearn: 0.0201009\ttotal: 1.35s\tremaining: 4.78s\n",
            "11:\tlearn: 0.0192099\ttotal: 1.46s\tremaining: 4.62s\n",
            "12:\tlearn: 0.0184652\ttotal: 1.58s\tremaining: 4.5s\n",
            "13:\tlearn: 0.0177150\ttotal: 1.7s\tremaining: 4.38s\n",
            "14:\tlearn: 0.0171644\ttotal: 1.82s\tremaining: 4.25s\n",
            "15:\tlearn: 0.0167244\ttotal: 1.93s\tremaining: 4.11s\n",
            "16:\tlearn: 0.0162064\ttotal: 2.05s\tremaining: 3.99s\n",
            "17:\tlearn: 0.0157212\ttotal: 2.16s\tremaining: 3.84s\n",
            "18:\tlearn: 0.0153209\ttotal: 2.28s\tremaining: 3.72s\n",
            "19:\tlearn: 0.0149021\ttotal: 2.39s\tremaining: 3.58s\n",
            "20:\tlearn: 0.0145368\ttotal: 2.51s\tremaining: 3.46s\n",
            "21:\tlearn: 0.0142165\ttotal: 2.63s\tremaining: 3.34s\n",
            "22:\tlearn: 0.0138926\ttotal: 2.74s\tremaining: 3.22s\n",
            "23:\tlearn: 0.0135742\ttotal: 2.85s\tremaining: 3.09s\n",
            "24:\tlearn: 0.0133582\ttotal: 2.97s\tremaining: 2.97s\n",
            "25:\tlearn: 0.0131526\ttotal: 3.08s\tremaining: 2.85s\n",
            "26:\tlearn: 0.0130074\ttotal: 3.2s\tremaining: 2.73s\n",
            "27:\tlearn: 0.0128226\ttotal: 3.31s\tremaining: 2.6s\n",
            "28:\tlearn: 0.0126213\ttotal: 3.43s\tremaining: 2.48s\n",
            "29:\tlearn: 0.0124610\ttotal: 3.53s\tremaining: 2.36s\n",
            "30:\tlearn: 0.0122807\ttotal: 3.66s\tremaining: 2.25s\n",
            "31:\tlearn: 0.0121417\ttotal: 3.77s\tremaining: 2.12s\n",
            "32:\tlearn: 0.0120181\ttotal: 3.89s\tremaining: 2s\n",
            "33:\tlearn: 0.0118804\ttotal: 4s\tremaining: 1.88s\n",
            "34:\tlearn: 0.0117513\ttotal: 4.12s\tremaining: 1.76s\n",
            "35:\tlearn: 0.0115916\ttotal: 4.23s\tremaining: 1.64s\n",
            "36:\tlearn: 0.0115075\ttotal: 4.34s\tremaining: 1.52s\n",
            "37:\tlearn: 0.0114042\ttotal: 4.45s\tremaining: 1.41s\n",
            "38:\tlearn: 0.0113181\ttotal: 4.57s\tremaining: 1.29s\n",
            "39:\tlearn: 0.0111952\ttotal: 4.69s\tremaining: 1.17s\n",
            "40:\tlearn: 0.0110299\ttotal: 4.8s\tremaining: 1.05s\n",
            "41:\tlearn: 0.0109474\ttotal: 4.92s\tremaining: 936ms\n",
            "42:\tlearn: 0.0108168\ttotal: 5.04s\tremaining: 821ms\n",
            "43:\tlearn: 0.0106714\ttotal: 5.15s\tremaining: 702ms\n",
            "44:\tlearn: 0.0105766\ttotal: 5.26s\tremaining: 585ms\n",
            "45:\tlearn: 0.0105385\ttotal: 5.38s\tremaining: 468ms\n",
            "46:\tlearn: 0.0104421\ttotal: 5.5s\tremaining: 351ms\n",
            "47:\tlearn: 0.0103442\ttotal: 5.61s\tremaining: 234ms\n",
            "48:\tlearn: 0.0102717\ttotal: 5.74s\tremaining: 117ms\n",
            "49:\tlearn: 0.0101974\ttotal: 5.85s\tremaining: 0us\n",
            "0:\tlearn: 0.2535344\ttotal: 173ms\tremaining: 8.46s\n",
            "1:\tlearn: 0.2352221\ttotal: 290ms\tremaining: 6.95s\n",
            "2:\tlearn: 0.2186643\ttotal: 414ms\tremaining: 6.48s\n",
            "3:\tlearn: 0.2055902\ttotal: 527ms\tremaining: 6.06s\n",
            "4:\tlearn: 0.1921676\ttotal: 650ms\tremaining: 5.85s\n",
            "5:\tlearn: 0.1805815\ttotal: 761ms\tremaining: 5.58s\n",
            "6:\tlearn: 0.1696996\ttotal: 894ms\tremaining: 5.49s\n",
            "7:\tlearn: 0.1607799\ttotal: 1.01s\tremaining: 5.28s\n",
            "8:\tlearn: 0.1539441\ttotal: 1.13s\tremaining: 5.15s\n",
            "9:\tlearn: 0.1474302\ttotal: 1.24s\tremaining: 4.97s\n",
            "10:\tlearn: 0.1417366\ttotal: 1.37s\tremaining: 4.85s\n",
            "11:\tlearn: 0.1363976\ttotal: 1.48s\tremaining: 4.68s\n",
            "12:\tlearn: 0.1311097\ttotal: 1.6s\tremaining: 4.55s\n",
            "13:\tlearn: 0.1266205\ttotal: 1.71s\tremaining: 4.41s\n",
            "14:\tlearn: 0.1222736\ttotal: 1.85s\tremaining: 4.32s\n",
            "15:\tlearn: 0.1190372\ttotal: 1.97s\tremaining: 4.18s\n",
            "16:\tlearn: 0.1157125\ttotal: 2.09s\tremaining: 4.07s\n",
            "17:\tlearn: 0.1126065\ttotal: 2.21s\tremaining: 3.93s\n",
            "18:\tlearn: 0.1105368\ttotal: 2.33s\tremaining: 3.8s\n",
            "19:\tlearn: 0.1075910\ttotal: 2.44s\tremaining: 3.67s\n",
            "20:\tlearn: 0.1062788\ttotal: 2.57s\tremaining: 3.55s\n",
            "21:\tlearn: 0.1044603\ttotal: 2.68s\tremaining: 3.41s\n",
            "22:\tlearn: 0.1016994\ttotal: 2.8s\tremaining: 3.29s\n",
            "23:\tlearn: 0.1003640\ttotal: 2.93s\tremaining: 3.17s\n",
            "24:\tlearn: 0.0994249\ttotal: 3.05s\tremaining: 3.05s\n",
            "25:\tlearn: 0.0979524\ttotal: 3.17s\tremaining: 2.92s\n",
            "26:\tlearn: 0.0963606\ttotal: 3.29s\tremaining: 2.8s\n",
            "27:\tlearn: 0.0955505\ttotal: 3.4s\tremaining: 2.67s\n",
            "28:\tlearn: 0.0943766\ttotal: 3.53s\tremaining: 2.56s\n",
            "29:\tlearn: 0.0929296\ttotal: 3.64s\tremaining: 2.43s\n",
            "30:\tlearn: 0.0920314\ttotal: 3.77s\tremaining: 2.31s\n",
            "31:\tlearn: 0.0912400\ttotal: 3.89s\tremaining: 2.19s\n",
            "32:\tlearn: 0.0895293\ttotal: 4.02s\tremaining: 2.07s\n",
            "33:\tlearn: 0.0884869\ttotal: 4.13s\tremaining: 1.94s\n",
            "34:\tlearn: 0.0877010\ttotal: 4.25s\tremaining: 1.82s\n",
            "35:\tlearn: 0.0868938\ttotal: 4.37s\tremaining: 1.7s\n",
            "36:\tlearn: 0.0861096\ttotal: 4.49s\tremaining: 1.58s\n",
            "37:\tlearn: 0.0853805\ttotal: 4.6s\tremaining: 1.45s\n",
            "38:\tlearn: 0.0849096\ttotal: 4.72s\tremaining: 1.33s\n",
            "39:\tlearn: 0.0838618\ttotal: 4.84s\tremaining: 1.21s\n",
            "40:\tlearn: 0.0831227\ttotal: 4.97s\tremaining: 1.09s\n",
            "41:\tlearn: 0.0823378\ttotal: 5.08s\tremaining: 968ms\n",
            "42:\tlearn: 0.0815096\ttotal: 5.21s\tremaining: 848ms\n",
            "43:\tlearn: 0.0805685\ttotal: 5.32s\tremaining: 726ms\n",
            "44:\tlearn: 0.0797405\ttotal: 5.45s\tremaining: 605ms\n",
            "45:\tlearn: 0.0793249\ttotal: 5.56s\tremaining: 484ms\n",
            "46:\tlearn: 0.0790478\ttotal: 5.69s\tremaining: 363ms\n",
            "47:\tlearn: 0.0782336\ttotal: 5.8s\tremaining: 242ms\n",
            "48:\tlearn: 0.0778375\ttotal: 5.93s\tremaining: 121ms\n",
            "49:\tlearn: 0.0773476\ttotal: 6.05s\tremaining: 0us\n",
            "0:\tlearn: 0.0368787\ttotal: 173ms\tremaining: 8.48s\n",
            "1:\tlearn: 0.0342710\ttotal: 285ms\tremaining: 6.85s\n",
            "2:\tlearn: 0.0317014\ttotal: 408ms\tremaining: 6.4s\n",
            "3:\tlearn: 0.0295265\ttotal: 521ms\tremaining: 5.99s\n",
            "4:\tlearn: 0.0274071\ttotal: 642ms\tremaining: 5.78s\n",
            "5:\tlearn: 0.0255870\ttotal: 758ms\tremaining: 5.56s\n",
            "6:\tlearn: 0.0239112\ttotal: 894ms\tremaining: 5.49s\n",
            "7:\tlearn: 0.0225648\ttotal: 1.01s\tremaining: 5.31s\n",
            "8:\tlearn: 0.0212466\ttotal: 1.13s\tremaining: 5.15s\n",
            "9:\tlearn: 0.0200732\ttotal: 1.25s\tremaining: 4.99s\n",
            "10:\tlearn: 0.0191228\ttotal: 1.37s\tremaining: 4.86s\n",
            "11:\tlearn: 0.0182907\ttotal: 1.48s\tremaining: 4.69s\n",
            "12:\tlearn: 0.0174251\ttotal: 1.61s\tremaining: 4.58s\n",
            "13:\tlearn: 0.0166332\ttotal: 1.72s\tremaining: 4.43s\n",
            "14:\tlearn: 0.0160181\ttotal: 1.85s\tremaining: 4.32s\n",
            "15:\tlearn: 0.0153965\ttotal: 1.98s\tremaining: 4.2s\n",
            "16:\tlearn: 0.0149815\ttotal: 2.1s\tremaining: 4.08s\n",
            "17:\tlearn: 0.0144798\ttotal: 2.22s\tremaining: 3.95s\n",
            "18:\tlearn: 0.0140180\ttotal: 2.34s\tremaining: 3.82s\n",
            "19:\tlearn: 0.0136281\ttotal: 2.46s\tremaining: 3.69s\n",
            "20:\tlearn: 0.0132607\ttotal: 2.59s\tremaining: 3.58s\n",
            "21:\tlearn: 0.0129147\ttotal: 2.7s\tremaining: 3.44s\n",
            "22:\tlearn: 0.0126099\ttotal: 2.83s\tremaining: 3.32s\n",
            "23:\tlearn: 0.0123436\ttotal: 2.96s\tremaining: 3.2s\n",
            "24:\tlearn: 0.0120555\ttotal: 3.08s\tremaining: 3.08s\n",
            "25:\tlearn: 0.0118921\ttotal: 3.2s\tremaining: 2.95s\n",
            "26:\tlearn: 0.0117927\ttotal: 3.32s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0116291\ttotal: 3.44s\tremaining: 2.7s\n",
            "28:\tlearn: 0.0114711\ttotal: 3.56s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0112616\ttotal: 3.68s\tremaining: 2.45s\n",
            "30:\tlearn: 0.0111278\ttotal: 3.81s\tremaining: 2.33s\n",
            "31:\tlearn: 0.0110291\ttotal: 3.94s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0108885\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0107581\ttotal: 4.18s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0106777\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0105925\ttotal: 4.41s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0104933\ttotal: 4.53s\tremaining: 1.59s\n",
            "37:\tlearn: 0.0103897\ttotal: 4.64s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0102391\ttotal: 4.77s\tremaining: 1.34s\n",
            "39:\tlearn: 0.0101157\ttotal: 4.88s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0100331\ttotal: 5.02s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0098981\ttotal: 5.14s\tremaining: 979ms\n",
            "42:\tlearn: 0.0097855\ttotal: 5.26s\tremaining: 857ms\n",
            "43:\tlearn: 0.0096810\ttotal: 5.38s\tremaining: 733ms\n",
            "44:\tlearn: 0.0096326\ttotal: 5.5s\tremaining: 611ms\n",
            "45:\tlearn: 0.0095543\ttotal: 5.61s\tremaining: 488ms\n",
            "46:\tlearn: 0.0095084\ttotal: 5.74s\tremaining: 366ms\n",
            "47:\tlearn: 0.0094149\ttotal: 5.85s\tremaining: 244ms\n",
            "48:\tlearn: 0.0093533\ttotal: 5.99s\tremaining: 122ms\n",
            "49:\tlearn: 0.0092797\ttotal: 6.12s\tremaining: 0us\n",
            "0:\tlearn: 0.0200075\ttotal: 173ms\tremaining: 8.46s\n",
            "1:\tlearn: 0.0192598\ttotal: 286ms\tremaining: 6.86s\n",
            "2:\tlearn: 0.0182292\ttotal: 409ms\tremaining: 6.4s\n",
            "3:\tlearn: 0.0171826\ttotal: 524ms\tremaining: 6.02s\n",
            "4:\tlearn: 0.0160527\ttotal: 652ms\tremaining: 5.87s\n",
            "5:\tlearn: 0.0154266\ttotal: 769ms\tremaining: 5.64s\n",
            "6:\tlearn: 0.0150372\ttotal: 906ms\tremaining: 5.56s\n",
            "7:\tlearn: 0.0141345\ttotal: 1.02s\tremaining: 5.35s\n",
            "8:\tlearn: 0.0135571\ttotal: 1.14s\tremaining: 5.21s\n",
            "9:\tlearn: 0.0129857\ttotal: 1.25s\tremaining: 5.02s\n",
            "10:\tlearn: 0.0125006\ttotal: 1.37s\tremaining: 4.87s\n",
            "11:\tlearn: 0.0122614\ttotal: 1.49s\tremaining: 4.71s\n",
            "12:\tlearn: 0.0118722\ttotal: 1.61s\tremaining: 4.59s\n",
            "13:\tlearn: 0.0113571\ttotal: 1.73s\tremaining: 4.44s\n",
            "14:\tlearn: 0.0109582\ttotal: 1.87s\tremaining: 4.36s\n",
            "15:\tlearn: 0.0105405\ttotal: 1.99s\tremaining: 4.22s\n",
            "16:\tlearn: 0.0104447\ttotal: 2.11s\tremaining: 4.1s\n",
            "17:\tlearn: 0.0100542\ttotal: 2.23s\tremaining: 3.97s\n",
            "18:\tlearn: 0.0097873\ttotal: 2.36s\tremaining: 3.85s\n",
            "19:\tlearn: 0.0095309\ttotal: 2.48s\tremaining: 3.72s\n",
            "20:\tlearn: 0.0093199\ttotal: 2.6s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0091463\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0088996\ttotal: 2.85s\tremaining: 3.34s\n",
            "23:\tlearn: 0.0088587\ttotal: 2.97s\tremaining: 3.22s\n",
            "24:\tlearn: 0.0087464\ttotal: 3.1s\tremaining: 3.1s\n",
            "25:\tlearn: 0.0086719\ttotal: 3.21s\tremaining: 2.97s\n",
            "26:\tlearn: 0.0085652\ttotal: 3.34s\tremaining: 2.84s\n",
            "27:\tlearn: 0.0084433\ttotal: 3.45s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0082681\ttotal: 3.58s\tremaining: 2.59s\n",
            "29:\tlearn: 0.0082454\ttotal: 3.7s\tremaining: 2.46s\n",
            "30:\tlearn: 0.0081804\ttotal: 3.83s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0081487\ttotal: 3.96s\tremaining: 2.23s\n",
            "32:\tlearn: 0.0079854\ttotal: 4.08s\tremaining: 2.1s\n",
            "33:\tlearn: 0.0078520\ttotal: 4.19s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0076964\ttotal: 4.31s\tremaining: 1.85s\n",
            "35:\tlearn: 0.0075853\ttotal: 4.43s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0074430\ttotal: 4.56s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0074153\ttotal: 4.67s\tremaining: 1.48s\n",
            "38:\tlearn: 0.0072817\ttotal: 4.79s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0072496\ttotal: 4.92s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0072260\ttotal: 5.04s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0071949\ttotal: 5.17s\tremaining: 984ms\n",
            "42:\tlearn: 0.0071209\ttotal: 5.29s\tremaining: 862ms\n",
            "43:\tlearn: 0.0070540\ttotal: 5.41s\tremaining: 738ms\n",
            "44:\tlearn: 0.0069778\ttotal: 5.53s\tremaining: 615ms\n",
            "45:\tlearn: 0.0068919\ttotal: 5.65s\tremaining: 491ms\n",
            "46:\tlearn: 0.0068073\ttotal: 5.77s\tremaining: 368ms\n",
            "47:\tlearn: 0.0067615\ttotal: 5.89s\tremaining: 245ms\n",
            "48:\tlearn: 0.0066906\ttotal: 6.03s\tremaining: 123ms\n",
            "49:\tlearn: 0.0066076\ttotal: 6.14s\tremaining: 0us\n",
            "0:\tlearn: 0.0382044\ttotal: 179ms\tremaining: 8.77s\n",
            "1:\tlearn: 0.0353773\ttotal: 294ms\tremaining: 7.06s\n",
            "2:\tlearn: 0.0328912\ttotal: 418ms\tremaining: 6.55s\n",
            "3:\tlearn: 0.0306068\ttotal: 537ms\tremaining: 6.18s\n",
            "4:\tlearn: 0.0284369\ttotal: 664ms\tremaining: 5.98s\n",
            "5:\tlearn: 0.0265463\ttotal: 796ms\tremaining: 5.83s\n",
            "6:\tlearn: 0.0248352\ttotal: 921ms\tremaining: 5.66s\n",
            "7:\tlearn: 0.0233149\ttotal: 1.03s\tremaining: 5.44s\n",
            "8:\tlearn: 0.0218961\ttotal: 1.16s\tremaining: 5.28s\n",
            "9:\tlearn: 0.0209815\ttotal: 1.27s\tremaining: 5.09s\n",
            "10:\tlearn: 0.0200546\ttotal: 1.4s\tremaining: 4.96s\n",
            "11:\tlearn: 0.0191685\ttotal: 1.51s\tremaining: 4.79s\n",
            "12:\tlearn: 0.0184576\ttotal: 1.64s\tremaining: 4.67s\n",
            "13:\tlearn: 0.0178450\ttotal: 1.77s\tremaining: 4.55s\n",
            "14:\tlearn: 0.0171665\ttotal: 1.89s\tremaining: 4.42s\n",
            "15:\tlearn: 0.0166920\ttotal: 2s\tremaining: 4.26s\n",
            "16:\tlearn: 0.0161467\ttotal: 2.13s\tremaining: 4.13s\n",
            "17:\tlearn: 0.0156371\ttotal: 2.25s\tremaining: 3.99s\n",
            "18:\tlearn: 0.0152676\ttotal: 2.37s\tremaining: 3.86s\n",
            "19:\tlearn: 0.0148503\ttotal: 2.48s\tremaining: 3.72s\n",
            "20:\tlearn: 0.0145202\ttotal: 2.61s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0142600\ttotal: 2.72s\tremaining: 3.46s\n",
            "22:\tlearn: 0.0139883\ttotal: 2.85s\tremaining: 3.35s\n",
            "23:\tlearn: 0.0137627\ttotal: 2.97s\tremaining: 3.21s\n",
            "24:\tlearn: 0.0135454\ttotal: 3.09s\tremaining: 3.09s\n",
            "25:\tlearn: 0.0134059\ttotal: 3.2s\tremaining: 2.96s\n",
            "26:\tlearn: 0.0132766\ttotal: 3.32s\tremaining: 2.83s\n",
            "27:\tlearn: 0.0130583\ttotal: 3.44s\tremaining: 2.7s\n",
            "28:\tlearn: 0.0128985\ttotal: 3.56s\tremaining: 2.58s\n",
            "29:\tlearn: 0.0127431\ttotal: 3.68s\tremaining: 2.45s\n",
            "30:\tlearn: 0.0125788\ttotal: 3.82s\tremaining: 2.34s\n",
            "31:\tlearn: 0.0124524\ttotal: 3.93s\tremaining: 2.21s\n",
            "32:\tlearn: 0.0123321\ttotal: 4.06s\tremaining: 2.09s\n",
            "33:\tlearn: 0.0122398\ttotal: 4.17s\tremaining: 1.96s\n",
            "34:\tlearn: 0.0121386\ttotal: 4.3s\tremaining: 1.84s\n",
            "35:\tlearn: 0.0120285\ttotal: 4.42s\tremaining: 1.72s\n",
            "36:\tlearn: 0.0119189\ttotal: 4.54s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0118110\ttotal: 4.66s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0117092\ttotal: 4.79s\tremaining: 1.35s\n",
            "39:\tlearn: 0.0115969\ttotal: 4.91s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0114922\ttotal: 5.03s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0113938\ttotal: 5.14s\tremaining: 980ms\n",
            "42:\tlearn: 0.0112724\ttotal: 5.27s\tremaining: 857ms\n",
            "43:\tlearn: 0.0111909\ttotal: 5.38s\tremaining: 733ms\n",
            "44:\tlearn: 0.0110660\ttotal: 5.5s\tremaining: 611ms\n",
            "45:\tlearn: 0.0109734\ttotal: 5.62s\tremaining: 488ms\n",
            "46:\tlearn: 0.0108989\ttotal: 5.74s\tremaining: 366ms\n",
            "47:\tlearn: 0.0107978\ttotal: 5.87s\tremaining: 245ms\n",
            "48:\tlearn: 0.0107188\ttotal: 5.99s\tremaining: 122ms\n",
            "49:\tlearn: 0.0106330\ttotal: 6.11s\tremaining: 0us\n",
            "0:\tlearn: 0.2797820\ttotal: 167ms\tremaining: 8.19s\n",
            "1:\tlearn: 0.2598295\ttotal: 281ms\tremaining: 6.74s\n",
            "2:\tlearn: 0.2404136\ttotal: 402ms\tremaining: 6.29s\n",
            "3:\tlearn: 0.2248995\ttotal: 514ms\tremaining: 5.91s\n",
            "4:\tlearn: 0.2090687\ttotal: 644ms\tremaining: 5.79s\n",
            "5:\tlearn: 0.1952774\ttotal: 763ms\tremaining: 5.6s\n",
            "6:\tlearn: 0.1839256\ttotal: 886ms\tremaining: 5.44s\n",
            "7:\tlearn: 0.1737126\ttotal: 1s\tremaining: 5.26s\n",
            "8:\tlearn: 0.1657401\ttotal: 1.13s\tremaining: 5.15s\n",
            "9:\tlearn: 0.1578873\ttotal: 1.24s\tremaining: 4.97s\n",
            "10:\tlearn: 0.1509448\ttotal: 1.37s\tremaining: 4.84s\n",
            "11:\tlearn: 0.1437567\ttotal: 1.48s\tremaining: 4.69s\n",
            "12:\tlearn: 0.1386029\ttotal: 1.61s\tremaining: 4.57s\n",
            "13:\tlearn: 0.1342312\ttotal: 1.74s\tremaining: 4.46s\n",
            "14:\tlearn: 0.1293356\ttotal: 1.86s\tremaining: 4.34s\n",
            "15:\tlearn: 0.1248526\ttotal: 1.98s\tremaining: 4.2s\n",
            "16:\tlearn: 0.1218652\ttotal: 2.1s\tremaining: 4.07s\n",
            "17:\tlearn: 0.1187871\ttotal: 2.21s\tremaining: 3.93s\n",
            "18:\tlearn: 0.1156556\ttotal: 2.34s\tremaining: 3.81s\n",
            "19:\tlearn: 0.1135429\ttotal: 2.45s\tremaining: 3.68s\n",
            "20:\tlearn: 0.1119719\ttotal: 2.57s\tremaining: 3.55s\n",
            "21:\tlearn: 0.1098525\ttotal: 2.7s\tremaining: 3.43s\n",
            "22:\tlearn: 0.1071306\ttotal: 2.82s\tremaining: 3.31s\n",
            "23:\tlearn: 0.1060031\ttotal: 2.93s\tremaining: 3.18s\n",
            "24:\tlearn: 0.1052246\ttotal: 3.05s\tremaining: 3.05s\n",
            "25:\tlearn: 0.1036817\ttotal: 3.17s\tremaining: 2.92s\n",
            "26:\tlearn: 0.1020860\ttotal: 3.29s\tremaining: 2.81s\n",
            "27:\tlearn: 0.1004742\ttotal: 3.41s\tremaining: 2.68s\n",
            "28:\tlearn: 0.0995442\ttotal: 3.53s\tremaining: 2.56s\n",
            "29:\tlearn: 0.0982045\ttotal: 3.64s\tremaining: 2.43s\n",
            "30:\tlearn: 0.0973645\ttotal: 3.78s\tremaining: 2.32s\n",
            "31:\tlearn: 0.0961396\ttotal: 3.9s\tremaining: 2.19s\n",
            "32:\tlearn: 0.0949169\ttotal: 4.02s\tremaining: 2.07s\n",
            "33:\tlearn: 0.0941712\ttotal: 4.14s\tremaining: 1.95s\n",
            "34:\tlearn: 0.0937216\ttotal: 4.26s\tremaining: 1.83s\n",
            "35:\tlearn: 0.0924890\ttotal: 4.38s\tremaining: 1.7s\n",
            "36:\tlearn: 0.0920634\ttotal: 4.5s\tremaining: 1.58s\n",
            "37:\tlearn: 0.0913861\ttotal: 4.62s\tremaining: 1.46s\n",
            "38:\tlearn: 0.0904014\ttotal: 4.76s\tremaining: 1.34s\n",
            "39:\tlearn: 0.0898517\ttotal: 4.88s\tremaining: 1.22s\n",
            "40:\tlearn: 0.0892789\ttotal: 5.01s\tremaining: 1.1s\n",
            "41:\tlearn: 0.0884993\ttotal: 5.13s\tremaining: 976ms\n",
            "42:\tlearn: 0.0878111\ttotal: 5.25s\tremaining: 855ms\n",
            "43:\tlearn: 0.0870391\ttotal: 5.37s\tremaining: 732ms\n",
            "44:\tlearn: 0.0865065\ttotal: 5.49s\tremaining: 610ms\n",
            "45:\tlearn: 0.0859381\ttotal: 5.61s\tremaining: 488ms\n",
            "46:\tlearn: 0.0855160\ttotal: 5.73s\tremaining: 366ms\n",
            "47:\tlearn: 0.0848588\ttotal: 5.86s\tremaining: 244ms\n",
            "48:\tlearn: 0.0842704\ttotal: 5.99s\tremaining: 122ms\n",
            "49:\tlearn: 0.0838228\ttotal: 6.1s\tremaining: 0us\n",
            "0:\tlearn: 0.0390151\ttotal: 175ms\tremaining: 8.55s\n",
            "1:\tlearn: 0.0360959\ttotal: 291ms\tremaining: 6.98s\n",
            "2:\tlearn: 0.0334754\ttotal: 419ms\tremaining: 6.56s\n",
            "3:\tlearn: 0.0312487\ttotal: 541ms\tremaining: 6.22s\n",
            "4:\tlearn: 0.0289513\ttotal: 675ms\tremaining: 6.08s\n",
            "5:\tlearn: 0.0271080\ttotal: 790ms\tremaining: 5.79s\n",
            "6:\tlearn: 0.0253396\ttotal: 919ms\tremaining: 5.64s\n",
            "7:\tlearn: 0.0239088\ttotal: 1.03s\tremaining: 5.43s\n",
            "8:\tlearn: 0.0225628\ttotal: 1.16s\tremaining: 5.27s\n",
            "9:\tlearn: 0.0213033\ttotal: 1.27s\tremaining: 5.09s\n",
            "10:\tlearn: 0.0201933\ttotal: 1.4s\tremaining: 4.96s\n",
            "11:\tlearn: 0.0193927\ttotal: 1.51s\tremaining: 4.79s\n",
            "12:\tlearn: 0.0183993\ttotal: 1.65s\tremaining: 4.7s\n",
            "13:\tlearn: 0.0175585\ttotal: 1.76s\tremaining: 4.54s\n",
            "14:\tlearn: 0.0169051\ttotal: 1.89s\tremaining: 4.41s\n",
            "15:\tlearn: 0.0162657\ttotal: 2.01s\tremaining: 4.27s\n",
            "16:\tlearn: 0.0155873\ttotal: 2.13s\tremaining: 4.14s\n",
            "17:\tlearn: 0.0151132\ttotal: 2.25s\tremaining: 4s\n",
            "18:\tlearn: 0.0146165\ttotal: 2.37s\tremaining: 3.87s\n",
            "19:\tlearn: 0.0142255\ttotal: 2.49s\tremaining: 3.73s\n",
            "20:\tlearn: 0.0139205\ttotal: 2.61s\tremaining: 3.6s\n",
            "21:\tlearn: 0.0135895\ttotal: 2.74s\tremaining: 3.48s\n",
            "22:\tlearn: 0.0132369\ttotal: 2.86s\tremaining: 3.36s\n",
            "23:\tlearn: 0.0129350\ttotal: 2.98s\tremaining: 3.23s\n",
            "24:\tlearn: 0.0126613\ttotal: 3.1s\tremaining: 3.1s\n",
            "25:\tlearn: 0.0124781\ttotal: 3.21s\tremaining: 2.97s\n",
            "26:\tlearn: 0.0122780\ttotal: 3.34s\tremaining: 2.84s\n",
            "27:\tlearn: 0.0121102\ttotal: 3.46s\tremaining: 2.71s\n",
            "28:\tlearn: 0.0119506\ttotal: 3.58s\tremaining: 2.6s\n",
            "29:\tlearn: 0.0117710\ttotal: 3.71s\tremaining: 2.48s\n",
            "30:\tlearn: 0.0116552\ttotal: 3.83s\tremaining: 2.35s\n",
            "31:\tlearn: 0.0115102\ttotal: 3.95s\tremaining: 2.22s\n",
            "32:\tlearn: 0.0114099\ttotal: 4.08s\tremaining: 2.1s\n",
            "33:\tlearn: 0.0112898\ttotal: 4.19s\tremaining: 1.97s\n",
            "34:\tlearn: 0.0112034\ttotal: 4.32s\tremaining: 1.85s\n",
            "35:\tlearn: 0.0110965\ttotal: 4.43s\tremaining: 1.73s\n",
            "36:\tlearn: 0.0109702\ttotal: 4.56s\tremaining: 1.6s\n",
            "37:\tlearn: 0.0108671\ttotal: 4.67s\tremaining: 1.47s\n",
            "38:\tlearn: 0.0107758\ttotal: 4.81s\tremaining: 1.36s\n",
            "39:\tlearn: 0.0106999\ttotal: 4.92s\tremaining: 1.23s\n",
            "40:\tlearn: 0.0105804\ttotal: 5.04s\tremaining: 1.11s\n",
            "41:\tlearn: 0.0104678\ttotal: 5.16s\tremaining: 984ms\n",
            "42:\tlearn: 0.0103888\ttotal: 5.29s\tremaining: 861ms\n",
            "43:\tlearn: 0.0103355\ttotal: 5.41s\tremaining: 737ms\n",
            "44:\tlearn: 0.0102526\ttotal: 5.53s\tremaining: 614ms\n",
            "45:\tlearn: 0.0101659\ttotal: 5.64s\tremaining: 490ms\n",
            "46:\tlearn: 0.0101171\ttotal: 5.77s\tremaining: 368ms\n",
            "47:\tlearn: 0.0100381\ttotal: 5.89s\tremaining: 245ms\n",
            "48:\tlearn: 0.0099841\ttotal: 6.01s\tremaining: 123ms\n",
            "49:\tlearn: 0.0099206\ttotal: 6.12s\tremaining: 0us\n",
            "0:\tlearn: 0.0250193\ttotal: 180ms\tremaining: 8.8s\n",
            "1:\tlearn: 0.0236138\ttotal: 292ms\tremaining: 7.01s\n",
            "2:\tlearn: 0.0222383\ttotal: 412ms\tremaining: 6.45s\n",
            "3:\tlearn: 0.0215573\ttotal: 446ms\tremaining: 5.13s\n",
            "4:\tlearn: 0.0201201\ttotal: 570ms\tremaining: 5.13s\n",
            "5:\tlearn: 0.0190181\ttotal: 702ms\tremaining: 5.15s\n",
            "6:\tlearn: 0.0178203\ttotal: 814ms\tremaining: 5s\n",
            "7:\tlearn: 0.0168162\ttotal: 939ms\tremaining: 4.93s\n",
            "8:\tlearn: 0.0159848\ttotal: 1.05s\tremaining: 4.8s\n",
            "9:\tlearn: 0.0151401\ttotal: 1.18s\tremaining: 4.71s\n",
            "10:\tlearn: 0.0145996\ttotal: 1.29s\tremaining: 4.58s\n",
            "11:\tlearn: 0.0140612\ttotal: 1.41s\tremaining: 4.47s\n",
            "12:\tlearn: 0.0136281\ttotal: 1.53s\tremaining: 4.35s\n",
            "13:\tlearn: 0.0132616\ttotal: 1.66s\tremaining: 4.28s\n",
            "14:\tlearn: 0.0127525\ttotal: 1.78s\tremaining: 4.15s\n",
            "15:\tlearn: 0.0123462\ttotal: 1.9s\tremaining: 4.04s\n",
            "16:\tlearn: 0.0119091\ttotal: 2.02s\tremaining: 3.91s\n",
            "17:\tlearn: 0.0115833\ttotal: 2.14s\tremaining: 3.81s\n",
            "18:\tlearn: 0.0111750\ttotal: 2.26s\tremaining: 3.68s\n",
            "19:\tlearn: 0.0109383\ttotal: 2.38s\tremaining: 3.57s\n",
            "20:\tlearn: 0.0107933\ttotal: 2.5s\tremaining: 3.45s\n",
            "21:\tlearn: 0.0106460\ttotal: 2.63s\tremaining: 3.35s\n",
            "22:\tlearn: 0.0105619\ttotal: 2.75s\tremaining: 3.23s\n",
            "23:\tlearn: 0.0104493\ttotal: 2.88s\tremaining: 3.12s\n",
            "24:\tlearn: 0.0102162\ttotal: 2.99s\tremaining: 2.99s\n",
            "25:\tlearn: 0.0100689\ttotal: 3.11s\tremaining: 2.87s\n",
            "26:\tlearn: 0.0098575\ttotal: 3.23s\tremaining: 2.75s\n",
            "27:\tlearn: 0.0098357\ttotal: 3.35s\tremaining: 2.63s\n",
            "28:\tlearn: 0.0096728\ttotal: 3.47s\tremaining: 2.51s\n",
            "29:\tlearn: 0.0095690\ttotal: 3.59s\tremaining: 2.4s\n",
            "30:\tlearn: 0.0094273\ttotal: 3.72s\tremaining: 2.28s\n",
            "31:\tlearn: 0.0093333\ttotal: 3.85s\tremaining: 2.16s\n",
            "32:\tlearn: 0.0093086\ttotal: 3.97s\tremaining: 2.04s\n",
            "33:\tlearn: 0.0092262\ttotal: 4.09s\tremaining: 1.93s\n",
            "34:\tlearn: 0.0091738\ttotal: 4.21s\tremaining: 1.8s\n",
            "35:\tlearn: 0.0091332\ttotal: 4.33s\tremaining: 1.69s\n",
            "36:\tlearn: 0.0090277\ttotal: 4.45s\tremaining: 1.56s\n",
            "37:\tlearn: 0.0089439\ttotal: 4.58s\tremaining: 1.45s\n",
            "38:\tlearn: 0.0088681\ttotal: 4.71s\tremaining: 1.33s\n",
            "39:\tlearn: 0.0087374\ttotal: 4.84s\tremaining: 1.21s\n",
            "40:\tlearn: 0.0085787\ttotal: 4.95s\tremaining: 1.09s\n",
            "41:\tlearn: 0.0084970\ttotal: 5.08s\tremaining: 968ms\n",
            "42:\tlearn: 0.0084630\ttotal: 5.2s\tremaining: 846ms\n",
            "43:\tlearn: 0.0084113\ttotal: 5.32s\tremaining: 726ms\n",
            "44:\tlearn: 0.0083322\ttotal: 5.44s\tremaining: 604ms\n",
            "45:\tlearn: 0.0082286\ttotal: 5.56s\tremaining: 484ms\n",
            "46:\tlearn: 0.0081774\ttotal: 5.68s\tremaining: 363ms\n",
            "47:\tlearn: 0.0080971\ttotal: 5.81s\tremaining: 242ms\n",
            "48:\tlearn: 0.0080135\ttotal: 5.93s\tremaining: 121ms\n",
            "49:\tlearn: 0.0079047\ttotal: 6.06s\tremaining: 0us\n",
            "0:\tlearn: 0.0414119\ttotal: 176ms\tremaining: 8.6s\n",
            "1:\tlearn: 0.0383561\ttotal: 291ms\tremaining: 6.99s\n",
            "2:\tlearn: 0.0354381\ttotal: 420ms\tremaining: 6.58s\n",
            "3:\tlearn: 0.0329408\ttotal: 535ms\tremaining: 6.15s\n",
            "4:\tlearn: 0.0304457\ttotal: 668ms\tremaining: 6.01s\n",
            "5:\tlearn: 0.0284345\ttotal: 784ms\tremaining: 5.75s\n",
            "6:\tlearn: 0.0266505\ttotal: 912ms\tremaining: 5.6s\n",
            "7:\tlearn: 0.0250350\ttotal: 1.03s\tremaining: 5.41s\n",
            "8:\tlearn: 0.0235255\ttotal: 1.15s\tremaining: 5.26s\n",
            "9:\tlearn: 0.0224865\ttotal: 1.27s\tremaining: 5.09s\n",
            "10:\tlearn: 0.0215117\ttotal: 1.4s\tremaining: 4.97s\n",
            "11:\tlearn: 0.0205178\ttotal: 1.52s\tremaining: 4.8s\n",
            "12:\tlearn: 0.0197295\ttotal: 1.66s\tremaining: 4.72s\n",
            "13:\tlearn: 0.0190328\ttotal: 1.77s\tremaining: 4.57s\n",
            "14:\tlearn: 0.0182909\ttotal: 1.9s\tremaining: 4.44s\n",
            "15:\tlearn: 0.0177592\ttotal: 2.02s\tremaining: 4.29s\n",
            "16:\tlearn: 0.0171225\ttotal: 2.14s\tremaining: 4.16s\n",
            "17:\tlearn: 0.0166140\ttotal: 2.26s\tremaining: 4.02s\n",
            "18:\tlearn: 0.0161659\ttotal: 2.38s\tremaining: 3.88s\n",
            "19:\tlearn: 0.0157856\ttotal: 2.49s\tremaining: 3.74s\n",
            "20:\tlearn: 0.0154708\ttotal: 2.62s\tremaining: 3.61s\n",
            "21:\tlearn: 0.0151576\ttotal: 2.75s\tremaining: 3.49s\n",
            "22:\tlearn: 0.0148724\ttotal: 2.87s\tremaining: 3.37s\n",
            "23:\tlearn: 0.0146030\ttotal: 2.98s\tremaining: 3.23s\n",
            "24:\tlearn: 0.0143110\ttotal: 3.11s\tremaining: 3.11s\n",
            "25:\tlearn: 0.0141400\ttotal: 3.23s\tremaining: 2.98s\n",
            "26:\tlearn: 0.0139539\ttotal: 3.36s\tremaining: 2.86s\n",
            "27:\tlearn: 0.0137574\ttotal: 3.48s\tremaining: 2.73s\n",
            "28:\tlearn: 0.0135232\ttotal: 3.6s\tremaining: 2.6s\n",
            "29:\tlearn: 0.0133406\ttotal: 3.73s\tremaining: 2.49s\n",
            "30:\tlearn: 0.0131753\ttotal: 3.86s\tremaining: 2.37s\n",
            "31:\tlearn: 0.0129932\ttotal: 3.98s\tremaining: 2.24s\n",
            "32:\tlearn: 0.0128804\ttotal: 4.11s\tremaining: 2.12s\n",
            "33:\tlearn: 0.0127538\ttotal: 4.22s\tremaining: 1.99s\n",
            "34:\tlearn: 0.0126555\ttotal: 4.35s\tremaining: 1.86s\n",
            "35:\tlearn: 0.0125542\ttotal: 4.47s\tremaining: 1.74s\n",
            "36:\tlearn: 0.0124802\ttotal: 4.6s\tremaining: 1.61s\n",
            "37:\tlearn: 0.0123244\ttotal: 4.73s\tremaining: 1.49s\n",
            "38:\tlearn: 0.0122394\ttotal: 4.86s\tremaining: 1.37s\n",
            "39:\tlearn: 0.0121073\ttotal: 4.98s\tremaining: 1.24s\n",
            "40:\tlearn: 0.0120131\ttotal: 5.1s\tremaining: 1.12s\n",
            "41:\tlearn: 0.0119562\ttotal: 5.22s\tremaining: 995ms\n",
            "42:\tlearn: 0.0118796\ttotal: 5.34s\tremaining: 870ms\n",
            "43:\tlearn: 0.0117681\ttotal: 5.46s\tremaining: 745ms\n",
            "44:\tlearn: 0.0116934\ttotal: 5.58s\tremaining: 620ms\n",
            "45:\tlearn: 0.0116217\ttotal: 5.71s\tremaining: 497ms\n",
            "46:\tlearn: 0.0115256\ttotal: 5.85s\tremaining: 373ms\n",
            "47:\tlearn: 0.0114489\ttotal: 5.96s\tremaining: 249ms\n",
            "48:\tlearn: 0.0113495\ttotal: 6.08s\tremaining: 124ms\n",
            "49:\tlearn: 0.0112551\ttotal: 6.2s\tremaining: 0us\n",
            "0:\tlearn: 0.2536060\ttotal: 7.03ms\tremaining: 344ms\n",
            "1:\tlearn: 0.2154386\ttotal: 14.3ms\tremaining: 343ms\n",
            "2:\tlearn: 0.1873790\ttotal: 20.6ms\tremaining: 323ms\n",
            "3:\tlearn: 0.1684712\ttotal: 27.1ms\tremaining: 312ms\n",
            "4:\tlearn: 0.1514423\ttotal: 33.2ms\tremaining: 299ms\n",
            "5:\tlearn: 0.1379407\ttotal: 39.7ms\tremaining: 291ms\n",
            "6:\tlearn: 0.1293642\ttotal: 47.2ms\tremaining: 290ms\n",
            "7:\tlearn: 0.1240476\ttotal: 55.6ms\tremaining: 292ms\n",
            "8:\tlearn: 0.1219615\ttotal: 62.2ms\tremaining: 283ms\n",
            "9:\tlearn: 0.1203860\ttotal: 68.1ms\tremaining: 273ms\n",
            "10:\tlearn: 0.1188225\ttotal: 74.5ms\tremaining: 264ms\n",
            "11:\tlearn: 0.1178764\ttotal: 80.9ms\tremaining: 256ms\n",
            "12:\tlearn: 0.1172106\ttotal: 87.3ms\tremaining: 248ms\n",
            "13:\tlearn: 0.1167123\ttotal: 93.7ms\tremaining: 241ms\n",
            "14:\tlearn: 0.1156554\ttotal: 100ms\tremaining: 234ms\n",
            "15:\tlearn: 0.1153819\ttotal: 107ms\tremaining: 227ms\n",
            "16:\tlearn: 0.1152040\ttotal: 113ms\tremaining: 219ms\n",
            "17:\tlearn: 0.1140939\ttotal: 119ms\tremaining: 211ms\n",
            "18:\tlearn: 0.1137466\ttotal: 125ms\tremaining: 203ms\n",
            "19:\tlearn: 0.1136228\ttotal: 131ms\tremaining: 196ms\n",
            "20:\tlearn: 0.1133251\ttotal: 137ms\tremaining: 189ms\n",
            "21:\tlearn: 0.1131283\ttotal: 143ms\tremaining: 182ms\n",
            "22:\tlearn: 0.1107682\ttotal: 149ms\tremaining: 175ms\n",
            "23:\tlearn: 0.1101738\ttotal: 156ms\tremaining: 169ms\n",
            "24:\tlearn: 0.1099148\ttotal: 162ms\tremaining: 162ms\n",
            "25:\tlearn: 0.1083490\ttotal: 168ms\tremaining: 155ms\n",
            "26:\tlearn: 0.1076894\ttotal: 174ms\tremaining: 148ms\n",
            "27:\tlearn: 0.1074048\ttotal: 180ms\tremaining: 142ms\n",
            "28:\tlearn: 0.1071931\ttotal: 186ms\tremaining: 135ms\n",
            "29:\tlearn: 0.1070849\ttotal: 192ms\tremaining: 128ms\n",
            "30:\tlearn: 0.1069363\ttotal: 199ms\tremaining: 122ms\n",
            "31:\tlearn: 0.1066743\ttotal: 215ms\tremaining: 121ms\n",
            "32:\tlearn: 0.1060963\ttotal: 222ms\tremaining: 115ms\n",
            "33:\tlearn: 0.1058284\ttotal: 229ms\tremaining: 108ms\n",
            "34:\tlearn: 0.1053033\ttotal: 235ms\tremaining: 101ms\n",
            "35:\tlearn: 0.1051113\ttotal: 242ms\tremaining: 94.1ms\n",
            "36:\tlearn: 0.1048735\ttotal: 248ms\tremaining: 87.2ms\n",
            "37:\tlearn: 0.1046488\ttotal: 254ms\tremaining: 80.3ms\n",
            "38:\tlearn: 0.1037090\ttotal: 261ms\tremaining: 73.5ms\n",
            "39:\tlearn: 0.1034482\ttotal: 267ms\tremaining: 66.7ms\n",
            "40:\tlearn: 0.1029145\ttotal: 273ms\tremaining: 59.9ms\n",
            "41:\tlearn: 0.1021093\ttotal: 279ms\tremaining: 53.1ms\n",
            "42:\tlearn: 0.1019364\ttotal: 285ms\tremaining: 46.4ms\n",
            "43:\tlearn: 0.1017717\ttotal: 291ms\tremaining: 39.7ms\n",
            "44:\tlearn: 0.1013227\ttotal: 298ms\tremaining: 33.1ms\n",
            "45:\tlearn: 0.1009888\ttotal: 304ms\tremaining: 26.5ms\n",
            "46:\tlearn: 0.1008751\ttotal: 311ms\tremaining: 19.8ms\n",
            "47:\tlearn: 0.1006781\ttotal: 317ms\tremaining: 13.2ms\n",
            "48:\tlearn: 0.1005648\ttotal: 323ms\tremaining: 6.59ms\n",
            "49:\tlearn: 0.1003563\ttotal: 330ms\tremaining: 0us\n",
            "0:\tlearn: 0.0355704\ttotal: 16.1ms\tremaining: 789ms\n",
            "1:\tlearn: 0.0292946\ttotal: 28.8ms\tremaining: 691ms\n",
            "2:\tlearn: 0.0253370\ttotal: 38.8ms\tremaining: 607ms\n",
            "3:\tlearn: 0.0223382\ttotal: 45.3ms\tremaining: 521ms\n",
            "4:\tlearn: 0.0199390\ttotal: 51.4ms\tremaining: 463ms\n",
            "5:\tlearn: 0.0180174\ttotal: 57.4ms\tremaining: 421ms\n",
            "6:\tlearn: 0.0165059\ttotal: 63.5ms\tremaining: 390ms\n",
            "7:\tlearn: 0.0152222\ttotal: 69.4ms\tremaining: 364ms\n",
            "8:\tlearn: 0.0145192\ttotal: 75.4ms\tremaining: 343ms\n",
            "9:\tlearn: 0.0138651\ttotal: 81.3ms\tremaining: 325ms\n",
            "10:\tlearn: 0.0134768\ttotal: 87.2ms\tremaining: 309ms\n",
            "11:\tlearn: 0.0133452\ttotal: 93.5ms\tremaining: 296ms\n",
            "12:\tlearn: 0.0132778\ttotal: 99.5ms\tremaining: 283ms\n",
            "13:\tlearn: 0.0132264\ttotal: 106ms\tremaining: 272ms\n",
            "14:\tlearn: 0.0129909\ttotal: 111ms\tremaining: 260ms\n",
            "15:\tlearn: 0.0129048\ttotal: 118ms\tremaining: 250ms\n",
            "16:\tlearn: 0.0127194\ttotal: 124ms\tremaining: 240ms\n",
            "17:\tlearn: 0.0126669\ttotal: 130ms\tremaining: 231ms\n",
            "18:\tlearn: 0.0126417\ttotal: 136ms\tremaining: 222ms\n",
            "19:\tlearn: 0.0126345\ttotal: 143ms\tremaining: 214ms\n",
            "20:\tlearn: 0.0125967\ttotal: 149ms\tremaining: 206ms\n",
            "21:\tlearn: 0.0125494\ttotal: 156ms\tremaining: 199ms\n",
            "22:\tlearn: 0.0125133\ttotal: 163ms\tremaining: 191ms\n",
            "23:\tlearn: 0.0124771\ttotal: 169ms\tremaining: 184ms\n",
            "24:\tlearn: 0.0123598\ttotal: 176ms\tremaining: 176ms\n",
            "25:\tlearn: 0.0123227\ttotal: 183ms\tremaining: 169ms\n",
            "26:\tlearn: 0.0122390\ttotal: 189ms\tremaining: 161ms\n",
            "27:\tlearn: 0.0121205\ttotal: 195ms\tremaining: 154ms\n",
            "28:\tlearn: 0.0120951\ttotal: 202ms\tremaining: 146ms\n",
            "29:\tlearn: 0.0120803\ttotal: 211ms\tremaining: 141ms\n",
            "30:\tlearn: 0.0120448\ttotal: 223ms\tremaining: 137ms\n",
            "31:\tlearn: 0.0119847\ttotal: 229ms\tremaining: 129ms\n",
            "32:\tlearn: 0.0119453\ttotal: 235ms\tremaining: 121ms\n",
            "33:\tlearn: 0.0118920\ttotal: 241ms\tremaining: 114ms\n",
            "34:\tlearn: 0.0118535\ttotal: 248ms\tremaining: 106ms\n",
            "35:\tlearn: 0.0118198\ttotal: 254ms\tremaining: 98.8ms\n",
            "36:\tlearn: 0.0117737\ttotal: 261ms\tremaining: 91.5ms\n",
            "37:\tlearn: 0.0117506\ttotal: 267ms\tremaining: 84.4ms\n",
            "38:\tlearn: 0.0117215\ttotal: 274ms\tremaining: 77.3ms\n",
            "39:\tlearn: 0.0116941\ttotal: 280ms\tremaining: 70ms\n",
            "40:\tlearn: 0.0116671\ttotal: 286ms\tremaining: 62.8ms\n",
            "41:\tlearn: 0.0116390\ttotal: 293ms\tremaining: 55.7ms\n",
            "42:\tlearn: 0.0116239\ttotal: 299ms\tremaining: 48.7ms\n",
            "43:\tlearn: 0.0115865\ttotal: 305ms\tremaining: 41.6ms\n",
            "44:\tlearn: 0.0115709\ttotal: 312ms\tremaining: 34.6ms\n",
            "45:\tlearn: 0.0115559\ttotal: 318ms\tremaining: 27.7ms\n",
            "46:\tlearn: 0.0115254\ttotal: 325ms\tremaining: 20.7ms\n",
            "47:\tlearn: 0.0115046\ttotal: 331ms\tremaining: 13.8ms\n",
            "48:\tlearn: 0.0114864\ttotal: 337ms\tremaining: 6.88ms\n",
            "49:\tlearn: 0.0114662\ttotal: 344ms\tremaining: 0us\n",
            "0:\tlearn: 0.0251208\ttotal: 8.88ms\tremaining: 435ms\n",
            "1:\tlearn: 0.0232537\ttotal: 17.1ms\tremaining: 411ms\n",
            "2:\tlearn: 0.0209981\ttotal: 23.7ms\tremaining: 371ms\n",
            "3:\tlearn: 0.0181312\ttotal: 30ms\tremaining: 345ms\n",
            "4:\tlearn: 0.0161946\ttotal: 36.5ms\tremaining: 329ms\n",
            "5:\tlearn: 0.0150322\ttotal: 42.9ms\tremaining: 314ms\n",
            "6:\tlearn: 0.0142375\ttotal: 49.1ms\tremaining: 301ms\n",
            "7:\tlearn: 0.0134839\ttotal: 55.5ms\tremaining: 291ms\n",
            "8:\tlearn: 0.0134492\ttotal: 61.9ms\tremaining: 282ms\n",
            "9:\tlearn: 0.0128012\ttotal: 67.9ms\tremaining: 272ms\n",
            "10:\tlearn: 0.0124387\ttotal: 74.2ms\tremaining: 263ms\n",
            "11:\tlearn: 0.0124121\ttotal: 80.6ms\tremaining: 255ms\n",
            "12:\tlearn: 0.0122731\ttotal: 86.6ms\tremaining: 247ms\n",
            "13:\tlearn: 0.0122693\ttotal: 93.1ms\tremaining: 239ms\n",
            "14:\tlearn: 0.0122545\ttotal: 99.2ms\tremaining: 232ms\n",
            "15:\tlearn: 0.0122494\ttotal: 105ms\tremaining: 224ms\n",
            "16:\tlearn: 0.0122462\ttotal: 112ms\tremaining: 217ms\n",
            "17:\tlearn: 0.0122402\ttotal: 118ms\tremaining: 210ms\n",
            "18:\tlearn: 0.0122320\ttotal: 125ms\tremaining: 203ms\n",
            "19:\tlearn: 0.0122297\ttotal: 131ms\tremaining: 196ms\n",
            "20:\tlearn: 0.0122123\ttotal: 137ms\tremaining: 190ms\n",
            "21:\tlearn: 0.0121736\ttotal: 144ms\tremaining: 183ms\n",
            "22:\tlearn: 0.0121678\ttotal: 151ms\tremaining: 177ms\n",
            "23:\tlearn: 0.0121609\ttotal: 157ms\tremaining: 170ms\n",
            "24:\tlearn: 0.0119063\ttotal: 164ms\tremaining: 164ms\n",
            "25:\tlearn: 0.0118510\ttotal: 170ms\tremaining: 157ms\n",
            "26:\tlearn: 0.0115870\ttotal: 177ms\tremaining: 151ms\n",
            "27:\tlearn: 0.0115828\ttotal: 184ms\tremaining: 144ms\n",
            "28:\tlearn: 0.0115778\ttotal: 190ms\tremaining: 138ms\n",
            "29:\tlearn: 0.0115753\ttotal: 197ms\tremaining: 131ms\n",
            "30:\tlearn: 0.0115539\ttotal: 209ms\tremaining: 128ms\n",
            "31:\tlearn: 0.0115514\ttotal: 217ms\tremaining: 122ms\n",
            "32:\tlearn: 0.0115500\ttotal: 224ms\tremaining: 115ms\n",
            "33:\tlearn: 0.0115484\ttotal: 230ms\tremaining: 108ms\n",
            "34:\tlearn: 0.0114364\ttotal: 237ms\tremaining: 102ms\n",
            "35:\tlearn: 0.0113914\ttotal: 243ms\tremaining: 94.6ms\n",
            "36:\tlearn: 0.0113480\ttotal: 250ms\tremaining: 87.8ms\n",
            "37:\tlearn: 0.0113447\ttotal: 257ms\tremaining: 81ms\n",
            "38:\tlearn: 0.0113425\ttotal: 263ms\tremaining: 74.2ms\n",
            "39:\tlearn: 0.0113406\ttotal: 270ms\tremaining: 67.4ms\n",
            "40:\tlearn: 0.0113400\ttotal: 276ms\tremaining: 60.6ms\n",
            "41:\tlearn: 0.0113370\ttotal: 283ms\tremaining: 53.9ms\n",
            "42:\tlearn: 0.0112830\ttotal: 289ms\tremaining: 47.1ms\n",
            "43:\tlearn: 0.0112798\ttotal: 296ms\tremaining: 40.4ms\n",
            "44:\tlearn: 0.0112686\ttotal: 302ms\tremaining: 33.6ms\n",
            "45:\tlearn: 0.0112670\ttotal: 309ms\tremaining: 26.9ms\n",
            "46:\tlearn: 0.0112661\ttotal: 316ms\tremaining: 20.2ms\n",
            "47:\tlearn: 0.0112649\ttotal: 323ms\tremaining: 13.4ms\n",
            "48:\tlearn: 0.0112641\ttotal: 330ms\tremaining: 6.72ms\n",
            "49:\tlearn: 0.0112583\ttotal: 336ms\tremaining: 0us\n",
            "0:\tlearn: 0.0364346\ttotal: 8.47ms\tremaining: 415ms\n",
            "1:\tlearn: 0.0307192\ttotal: 17.1ms\tremaining: 411ms\n",
            "2:\tlearn: 0.0259926\ttotal: 24ms\tremaining: 375ms\n",
            "3:\tlearn: 0.0230173\ttotal: 30.5ms\tremaining: 350ms\n",
            "4:\tlearn: 0.0206300\ttotal: 37.3ms\tremaining: 336ms\n",
            "5:\tlearn: 0.0188231\ttotal: 43.5ms\tremaining: 319ms\n",
            "6:\tlearn: 0.0175070\ttotal: 49.9ms\tremaining: 307ms\n",
            "7:\tlearn: 0.0163780\ttotal: 56.2ms\tremaining: 295ms\n",
            "8:\tlearn: 0.0155807\ttotal: 62.4ms\tremaining: 284ms\n",
            "9:\tlearn: 0.0150345\ttotal: 68.4ms\tremaining: 274ms\n",
            "10:\tlearn: 0.0147521\ttotal: 74.5ms\tremaining: 264ms\n",
            "11:\tlearn: 0.0146908\ttotal: 80.8ms\tremaining: 256ms\n",
            "12:\tlearn: 0.0146404\ttotal: 87.2ms\tremaining: 248ms\n",
            "13:\tlearn: 0.0146109\ttotal: 93.7ms\tremaining: 241ms\n",
            "14:\tlearn: 0.0145406\ttotal: 101ms\tremaining: 236ms\n",
            "15:\tlearn: 0.0145216\ttotal: 110ms\tremaining: 233ms\n",
            "16:\tlearn: 0.0142688\ttotal: 117ms\tremaining: 228ms\n",
            "17:\tlearn: 0.0142529\ttotal: 125ms\tremaining: 222ms\n",
            "18:\tlearn: 0.0142251\ttotal: 131ms\tremaining: 214ms\n",
            "19:\tlearn: 0.0140495\ttotal: 137ms\tremaining: 206ms\n",
            "20:\tlearn: 0.0140126\ttotal: 144ms\tremaining: 199ms\n",
            "21:\tlearn: 0.0140012\ttotal: 164ms\tremaining: 208ms\n",
            "22:\tlearn: 0.0138572\ttotal: 170ms\tremaining: 199ms\n",
            "23:\tlearn: 0.0138226\ttotal: 176ms\tremaining: 191ms\n",
            "24:\tlearn: 0.0138019\ttotal: 182ms\tremaining: 182ms\n",
            "25:\tlearn: 0.0136674\ttotal: 187ms\tremaining: 173ms\n",
            "26:\tlearn: 0.0136221\ttotal: 193ms\tremaining: 164ms\n",
            "27:\tlearn: 0.0136020\ttotal: 200ms\tremaining: 157ms\n",
            "28:\tlearn: 0.0134393\ttotal: 209ms\tremaining: 151ms\n",
            "29:\tlearn: 0.0134120\ttotal: 220ms\tremaining: 147ms\n",
            "30:\tlearn: 0.0133777\ttotal: 227ms\tremaining: 139ms\n",
            "31:\tlearn: 0.0133163\ttotal: 233ms\tremaining: 131ms\n",
            "32:\tlearn: 0.0132990\ttotal: 239ms\tremaining: 123ms\n",
            "33:\tlearn: 0.0132734\ttotal: 245ms\tremaining: 115ms\n",
            "34:\tlearn: 0.0132282\ttotal: 251ms\tremaining: 108ms\n",
            "35:\tlearn: 0.0131511\ttotal: 257ms\tremaining: 100ms\n",
            "36:\tlearn: 0.0131345\ttotal: 264ms\tremaining: 92.7ms\n",
            "37:\tlearn: 0.0131190\ttotal: 270ms\tremaining: 85.3ms\n",
            "38:\tlearn: 0.0129994\ttotal: 276ms\tremaining: 77.9ms\n",
            "39:\tlearn: 0.0129859\ttotal: 283ms\tremaining: 70.7ms\n",
            "40:\tlearn: 0.0129104\ttotal: 289ms\tremaining: 63.4ms\n",
            "41:\tlearn: 0.0128660\ttotal: 295ms\tremaining: 56.2ms\n",
            "42:\tlearn: 0.0128477\ttotal: 302ms\tremaining: 49.1ms\n",
            "43:\tlearn: 0.0128262\ttotal: 308ms\tremaining: 42ms\n",
            "44:\tlearn: 0.0128037\ttotal: 314ms\tremaining: 34.9ms\n",
            "45:\tlearn: 0.0127864\ttotal: 321ms\tremaining: 27.9ms\n",
            "46:\tlearn: 0.0127715\ttotal: 327ms\tremaining: 20.8ms\n",
            "47:\tlearn: 0.0127450\ttotal: 333ms\tremaining: 13.9ms\n",
            "48:\tlearn: 0.0127048\ttotal: 340ms\tremaining: 6.93ms\n",
            "49:\tlearn: 0.0126524\ttotal: 347ms\tremaining: 0us\n",
            "{'estimator__depth': 3, 'estimator__iterations': 50, 'estimator__learning_rate': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion:"
      ],
      "metadata": {
        "id": "SBkgDp4b5N8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We figured out that the best parameters are following: moving_average_size 180, max_lag 350, max_depth 3, learning_rate 0.3. It's time to apply them in model building!"
      ],
      "metadata": {
        "id": "5CLl8TA35TcT"
      }
    }
  ]
}